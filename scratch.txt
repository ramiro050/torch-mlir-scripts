Args: ./torch-mlir/build/bin/torch-mlir-opt --torchscript-module-to-torch-backend-pipeline --torch-backend-to-linalg-on-tensors-backend-pipeline --print-ir-after-all /nodclouddata/ramiro/temp.mlir -debug 
Load new dialect in Context builtin
Load new dialect in Context torch
Load new dialect in Context std
Load new dialect in Context arith
Load new dialect in Context linalg
Load new dialect in Context affine
Load new dialect in Context math
Load new dialect in Context memref
Load new dialect in Context tensor
Load new dialect in Context scf
Load new dialect in Context torch_c
// -----// IR Dump After SymbolDCE //----- //
module attributes {torch.debug_module_name = "MyModule"}  {
  func private @__torch__.MyModule.forward(%arg0: !torch.nn.Module<"__torch__.MyModule">, %arg1: !torch.tensor {torch.type_bound = !torch.vtensor<[?,?,?],f32>}, %arg2: !torch.tensor {torch.type_bound = !torch.vtensor<[?,?],f32>}) -> !torch.tensor {
    %int0 = torch.constant.int 0
    %int1 = torch.constant.int 1
    %int2 = torch.constant.int 2
    %int3 = torch.constant.int 3
    %1 = torch.aten.slice.Tensor %arg1, %int0, %int0, %int1, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %2 = torch.aten.slice.Tensor %1, %int1, %int1, %int2, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %3 = torch.aten.slice.Tensor %2, %int2, %int1, %int3, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    return %3 : !torch.tensor
  }
  torch.class_type @__torch__.MyModule  {
    torch.attr private "training" : !torch.bool
    torch.attr private "_is_full_backward_hook" : !torch.optional<!torch.bool>
    torch.method "forward", @__torch__.MyModule.forward
  }
  %true = torch.constant.bool true
  %none = torch.constant.none
  %0 = torch.nn_module  {
    torch.slot "training", %true : !torch.bool
    torch.slot "_is_full_backward_hook", %none : !torch.none
  } : !torch.nn.Module<"__torch__.MyModule">
}



//===-------------------------------------------===//
Processing operation : 'torch.nn_module'(0x68664f0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.nn_module_terminator'(0x6866390) {
  "torch.nn_module_terminator"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.slot'(0x6866460) {
  "torch.slot"(%1) {name = "_is_full_backward_hook"} : (!torch.none) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.slot'(0x68663e0) {
  "torch.slot"(%0) {name = "training"} : (!torch.bool) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.none'(0x6864e50) {
  %1 = "torch.constant.none"() : () -> !torch.none

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.slot'(0x6866460) {
  "torch.slot"(%0) {name = "_is_full_backward_hook"} : (!torch.none) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.none'(0x68887b0) {
  %0 = "torch.constant.none"() : () -> !torch.none

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.bool'(0x6864d10) {
  %1 = "torch.constant.bool"() {value = true} : () -> !torch.bool

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.slot'(0x68663e0) {
  "torch.slot"(%0) {name = "training"} : (!torch.bool) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.bool'(0x6864e50) {
  %0 = "torch.constant.bool"() {value = true} : () -> !torch.bool

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.class_type'(0x6864ca0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.class_type_terminator'(0x6852b60) {
  "torch.class_type_terminator"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.method'(0x6852bb0) {
  "torch.method"() {function = @__torch__.MyModule.forward, name = "forward"} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.attr'(0x6852ae0) {
  "torch.attr"() {isPrivate, name = "_is_full_backward_hook", type = !torch.optional<!torch.bool>} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.attr'(0x683bcc0) {
  "torch.attr"() {isPrivate, name = "training", type = !torch.bool} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.func'(0x6853180) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.return'(0x6859150) {
  "std.return"(%9) : (!torch.tensor) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6852c80) {
  %9 = "torch.aten.slice.Tensor"(%8, %5, %4, %6, %4) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x67f1320) {
  %8 = "torch.aten.slice.Tensor"(%7, %4, %4, %5, %4) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x67f1210) {
  %7 = "torch.aten.slice.Tensor"(%arg1, %3, %3, %4, %4) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x6852a50) {
  %6 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6852c80) {
  %9 = "torch.aten.slice.Tensor"(%8, %6, %5, %3, %5) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x6864d10) {
  %3 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68514a0) {
  %6 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x67f1320) {
  %8 = "torch.aten.slice.Tensor"(%7, %6, %6, %3, %6) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6852c80) {
  %9 = "torch.aten.slice.Tensor"(%8, %3, %6, %4, %6) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x6852a50) {
  %3 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x684ea00) {
  %6 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x67f1210) {
  %7 = "torch.aten.slice.Tensor"(%arg1, %6, %6, %3, %3) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x67f1320) {
  %8 = "torch.aten.slice.Tensor"(%7, %3, %3, %4, %3) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6852c80) {
  %9 = "torch.aten.slice.Tensor"(%8, %4, %3, %5, %3) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68514a0) {
  %3 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x681e350) {
  %6 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x67f1210) {
  %7 = "torch.aten.slice.Tensor"(%arg1, %3, %3, %4, %4) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x684ea00) {
  %3 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.nn_module'(0x68664f0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.nn_module_terminator'(0x6866390) {
  "torch.nn_module_terminator"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.slot'(0x6866460) {
  "torch.slot"(%1) {name = "_is_full_backward_hook"} : (!torch.none) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.slot'(0x68663e0) {
  "torch.slot"(%0) {name = "training"} : (!torch.bool) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.class_type'(0x6864ca0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.class_type_terminator'(0x6852b60) {
  "torch.class_type_terminator"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.method'(0x6852bb0) {
  "torch.method"() {function = @__torch__.MyModule.forward, name = "forward"} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.attr'(0x6852ae0) {
  "torch.attr"() {isPrivate, name = "_is_full_backward_hook", type = !torch.optional<!torch.bool>} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.attr'(0x683bcc0) {
  "torch.attr"() {isPrivate, name = "training", type = !torch.bool} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.func'(0x6853180) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.return'(0x6859150) {
  "std.return"(%9) : (!torch.tensor) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6852c80) {
  %9 = "torch.aten.slice.Tensor"(%8, %5, %4, %6, %4) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x67f1320) {
  %8 = "torch.aten.slice.Tensor"(%7, %4, %4, %5, %4) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x67f1210) {
  %7 = "torch.aten.slice.Tensor"(%arg1, %3, %3, %4, %4) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x6864d10) {
  %6 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x6852a50) {
  %5 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68514a0) {
  %4 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x684ea00) {
  %3 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.none'(0x68887b0) {
  %1 = "torch.constant.none"() : () -> !torch.none

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.bool'(0x6864e50) {
  %0 = "torch.constant.bool"() {value = true} : () -> !torch.bool

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x6867970) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.bool'(0x6864e50) {
  %0 = "torch.constant.bool"() {value = true} : () -> !torch.bool

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.none'(0x68887b0) {
  %1 = "torch.constant.none"() : () -> !torch.none

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'builtin.func'(0x6853180) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x684ea00) {
  %3 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x68514a0) {
  %4 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x6852a50) {
  %5 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x6864d10) {
  %6 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.slice.Tensor'(0x67f1210) {
  %7 = "torch.aten.slice.Tensor"(%arg1, %3, %3, %4, %4) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.slice.Tensor'(0x67f1320) {
  %8 = "torch.aten.slice.Tensor"(%7, %4, %4, %5, %4) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.slice.Tensor'(0x6852c80) {
  %9 = "torch.aten.slice.Tensor"(%8, %5, %4, %6, %4) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.return'(0x6859150) {
  "std.return"(%9) : (!torch.tensor) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.class_type'(0x6864ca0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.attr'(0x683bcc0) {
  "torch.attr"() {isPrivate, name = "training", type = !torch.bool} : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.attr'(0x6852ae0) {
  "torch.attr"() {isPrivate, name = "_is_full_backward_hook", type = !torch.optional<!torch.bool>} : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.method'(0x6852bb0) {
  "torch.method"() {function = @__torch__.MyModule.forward, name = "forward"} : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.class_type_terminator'(0x6852b60) {
  "torch.class_type_terminator"() : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.nn_module'(0x68664f0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.slot'(0x68663e0) {
  "torch.slot"(%0) {name = "training"} : (!torch.bool) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.slot'(0x6866460) {
  "torch.slot"(%1) {name = "_is_full_backward_hook"} : (!torch.none) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.nn_module_terminator'(0x6866390) {
  "torch.nn_module_terminator"() : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After PrepareForGlobalizeObjectGraph //----- //
module attributes {torch.debug_module_name = "MyModule"}  {
  %true = torch.constant.bool true
  %none = torch.constant.none
  func private @__torch__.MyModule.forward(%arg0: !torch.nn.Module<"__torch__.MyModule">, %arg1: !torch.tensor {torch.type_bound = !torch.vtensor<[?,?,?],f32>}, %arg2: !torch.tensor {torch.type_bound = !torch.vtensor<[?,?],f32>}) -> !torch.tensor {
    %int0 = torch.constant.int 0
    %int1 = torch.constant.int 1
    %int2 = torch.constant.int 2
    %int3 = torch.constant.int 3
    %1 = torch.aten.slice.Tensor %arg1, %int0, %int0, %int1, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %2 = torch.aten.slice.Tensor %1, %int1, %int1, %int2, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %3 = torch.aten.slice.Tensor %2, %int2, %int1, %int3, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    return %3 : !torch.tensor
  }
  torch.class_type @__torch__.MyModule  {
    torch.attr private "training" : !torch.bool
    torch.attr private "_is_full_backward_hook" : !torch.optional<!torch.bool>
    torch.method "forward", @__torch__.MyModule.forward
  }
  %0 = torch.nn_module  {
    torch.slot "training", %true : !torch.bool
    torch.slot "_is_full_backward_hook", %none : !torch.none
  } : !torch.nn.Module<"__torch__.MyModule">
}


// -----// IR Dump After GlobalizeObjectGraph //----- //
module attributes {torch.debug_module_name = "MyModule"}  {
  torch.global_slot "private" @training : !torch.bool  {
    %true = torch.constant.bool true
    torch.global_slot.init %true : !torch.bool
  }
  torch.global_slot "private" @_is_full_backward_hook : !torch.optional<!torch.bool>  {
    %none = torch.constant.none
    torch.global_slot.init %none : !torch.none
  }
  func @forward(%arg0: !torch.tensor {torch.type_bound = !torch.vtensor<[?,?,?],f32>}, %arg1: !torch.tensor {torch.type_bound = !torch.vtensor<[?,?],f32>}) -> !torch.tensor {
    %int0 = torch.constant.int 0
    %int1 = torch.constant.int 1
    %int2 = torch.constant.int 2
    %int3 = torch.constant.int 3
    %0 = torch.aten.slice.Tensor %arg0, %int0, %int0, %int1, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %1 = torch.aten.slice.Tensor %0, %int1, %int1, %int2, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %2 = torch.aten.slice.Tensor %1, %int2, %int1, %int3, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    return %2 : !torch.tensor
  }
}


// -----// IR Dump After SymbolDCE //----- //
module attributes {torch.debug_module_name = "MyModule"}  {
  func @forward(%arg0: !torch.tensor {torch.type_bound = !torch.vtensor<[?,?,?],f32>}, %arg1: !torch.tensor {torch.type_bound = !torch.vtensor<[?,?],f32>}) -> !torch.tensor {
    %int0 = torch.constant.int 0
    %int1 = torch.constant.int 1
    %int2 = torch.constant.int 2
    %int3 = torch.constant.int 3
    %0 = torch.aten.slice.Tensor %arg0, %int0, %int0, %int1, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %1 = torch.aten.slice.Tensor %0, %int1, %int1, %int2, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %2 = torch.aten.slice.Tensor %1, %int2, %int1, %int3, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    return %2 : !torch.tensor
  }
}



//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686cd80) {
  %0 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x684ea00) {
  %0 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686b710) {
  %1 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686cd80) {
  %0 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68a99f0) {
  %2 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686b710) {
  %0 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68a9a50) {
  %3 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68a99f0) {
  %0 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6870410) {
  %4 = "torch.aten.slice.Tensor"(%arg0, %3, %3, %2, %2) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6853080) {
  %5 = "torch.aten.slice.Tensor"(%4, %2, %2, %1, %2) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %6 = "torch.aten.slice.Tensor"(%5, %1, %2, %0, %2) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.return'(0x68a9bb0) {
  "std.return"(%6) : (!torch.tensor) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68a99f0) {
  %0 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686b710) {
  %1 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686cd80) {
  %2 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x684ea00) {
  %3 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6870410) {
  %4 = "torch.aten.slice.Tensor"(%arg0, %3, %3, %2, %2) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6853080) {
  %5 = "torch.aten.slice.Tensor"(%4, %2, %2, %1, %2) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %6 = "torch.aten.slice.Tensor"(%5, %1, %2, %0, %2) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.return'(0x68a9bb0) {
  "std.return"(%6) : (!torch.tensor) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer //----- //
func @forward(%arg0: !torch.tensor {torch.type_bound = !torch.vtensor<[?,?,?],f32>}, %arg1: !torch.tensor {torch.type_bound = !torch.vtensor<[?,?],f32>}) -> !torch.tensor {
  %int3 = torch.constant.int 3
  %int2 = torch.constant.int 2
  %int1 = torch.constant.int 1
  %int0 = torch.constant.int 0
  %0 = torch.aten.slice.Tensor %arg0, %int0, %int0, %int1, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
  %1 = torch.aten.slice.Tensor %0, %int1, %int1, %int2, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
  %2 = torch.aten.slice.Tensor %1, %int2, %int1, %int3, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
  return %2 : !torch.tensor
}

// -----// IR Dump After Inliner //----- //
module attributes {torch.debug_module_name = "MyModule"}  {
  func @forward(%arg0: !torch.tensor {torch.type_bound = !torch.vtensor<[?,?,?],f32>}, %arg1: !torch.tensor {torch.type_bound = !torch.vtensor<[?,?],f32>}) -> !torch.tensor {
    %int3 = torch.constant.int 3
    %int2 = torch.constant.int 2
    %int1 = torch.constant.int 1
    %int0 = torch.constant.int 0
    %0 = torch.aten.slice.Tensor %arg0, %int0, %int0, %int1, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %1 = torch.aten.slice.Tensor %0, %int1, %int1, %int2, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %2 = torch.aten.slice.Tensor %1, %int2, %int1, %int3, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    return %2 : !torch.tensor
  }
}



//===-------------------------------------------===//
Legalizing operation : 'builtin.func'(0x686d8b0) {
  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'builtin.func -> ()' {
Trying to match "(anonymous namespace)::AdjustCallingConventionForFunc"
"(anonymous namespace)::AdjustCallingConventionForFunc" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'builtin.func'(0x686d8b0) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
module attributes {torch.debug_module_name = "MyModule"}  {
  func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.tensor {
    %0 = builtin.unrealized_conversion_cast %arg0 : !torch.vtensor<[?,?,?],f32> to !torch.tensor
    %1 = builtin.unrealized_conversion_cast %arg1 : !torch.vtensor<[?,?],f32> to !torch.tensor
    %int3 = torch.constant.int 3
    %int2 = torch.constant.int 2
    %int1 = torch.constant.int 1
    %int0 = torch.constant.int 0
    %2 = torch.aten.slice.Tensor <<UNKNOWN SSA VALUE>>, %int0, %int0, %int1, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %3 = torch.aten.slice.Tensor %2, %int1, %int1, %int2, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %4 = torch.aten.slice.Tensor %3, %int2, %int1, %int3, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    return %4 : !torch.tensor
  }
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x68a99f0) {
  %2 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x686b710) {
  %3 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x686cd80) {
  %4 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x684ea00) {
  %5 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.slice.Tensor'(0x6870410) {
  %6 = "torch.aten.slice.Tensor"(<<UNKNOWN SSA VALUE>>, %5, %5, %4, %4) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.slice.Tensor'(0x6853080) {
  %7 = "torch.aten.slice.Tensor"(%6, %4, %4, %3, %4) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %8 = "torch.aten.slice.Tensor"(%7, %3, %4, %2, %4) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.return'(0x68a9bb0) {
  "std.return"(%8) : (!torch.tensor) -> ()

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'std.return -> ()' {
Trying to match "(anonymous namespace)::AdjustCallingConventionForReturn"
    ** Insert  : 'std.return'(0x68a5110)
    ** Replace : 'std.return'(0x68a9bb0)
"(anonymous namespace)::AdjustCallingConventionForReturn" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'std.return'(0x68a5110) {
      "std.return"(%8) : (!torch.tensor) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.tensor {
  %0 = builtin.unrealized_conversion_cast %arg0 : !torch.vtensor<[?,?,?],f32> to !torch.tensor
  %1 = builtin.unrealized_conversion_cast %arg1 : !torch.vtensor<[?,?],f32> to !torch.tensor
  %int3 = torch.constant.int 3
  %int2 = torch.constant.int 2
  %int1 = torch.constant.int 1
  %int0 = torch.constant.int 0
  %2 = torch.aten.slice.Tensor <<UNKNOWN SSA VALUE>>, %int0, %int0, %int1, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
  %3 = torch.aten.slice.Tensor %2, %int1, %int1, %int2, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
  %4 = torch.aten.slice.Tensor %3, %int2, %int1, %int3, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
  return %4 : !torch.tensor
  return %4 : !torch.tensor
}


} -> SUCCESS
//===-------------------------------------------===//
** Insert  : 'torch.tensor_static_info_cast'(0x68af140)
** Insert  : 'torch.copy.to_tensor'(0x68b3360)
// -----// IR Dump After AdjustCallingConventions //----- //
module attributes {torch.debug_module_name = "MyModule"}  {
  func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.tensor {
    %0 = torch.tensor_static_info_cast %arg0 : !torch.vtensor<[?,?,?],f32> to !torch.vtensor
    %1 = torch.copy.to_tensor %0 : !torch.tensor
    %int3 = torch.constant.int 3
    %int2 = torch.constant.int 2
    %int1 = torch.constant.int 1
    %int0 = torch.constant.int 0
    %2 = torch.aten.slice.Tensor %1, %int0, %int0, %int1, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %3 = torch.aten.slice.Tensor %2, %int1, %int1, %int2, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %4 = torch.aten.slice.Tensor %3, %int2, %int1, %int3, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    return %4 : !torch.tensor
  }
}


// -----// IR Dump After InlineGlobalSlots //----- //
module attributes {torch.debug_module_name = "MyModule"}  {
  func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.tensor {
    %0 = torch.tensor_static_info_cast %arg0 : !torch.vtensor<[?,?,?],f32> to !torch.vtensor
    %1 = torch.copy.to_tensor %0 : !torch.tensor
    %int3 = torch.constant.int 3
    %int2 = torch.constant.int 2
    %int1 = torch.constant.int 1
    %int0 = torch.constant.int 0
    %2 = torch.aten.slice.Tensor %1, %int0, %int0, %int1, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %3 = torch.aten.slice.Tensor %2, %int1, %int1, %int2, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %4 = torch.aten.slice.Tensor %3, %int2, %int1, %int3, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    return %4 : !torch.tensor
  }
}



//===-------------------------------------------===//
Legalizing operation : 'builtin.func'(0x686d8b0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.tensor_static_info_cast'(0x68af140) {
  %0 = "torch.tensor_static_info_cast"(%arg0) : (!torch.vtensor<[?,?,?],f32>) -> !torch.vtensor

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.copy.to_tensor'(0x68b3360) {
  %1 = "torch.copy.to_tensor"(%0) : (!torch.vtensor) -> !torch.tensor

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x68a99f0) {
  %2 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x686b710) {
  %3 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x686cd80) {
  %4 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x684ea00) {
  %5 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.slice.Tensor'(0x6870410) {
  %6 = "torch.aten.slice.Tensor"(%1, %5, %5, %4, %4) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.slice.Tensor'(0x6853080) {
  %7 = "torch.aten.slice.Tensor"(%6, %4, %4, %3, %4) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %8 = "torch.aten.slice.Tensor"(%7, %3, %4, %2, %4) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.return'(0x68a5110) {
  "std.return"(%8) : (!torch.tensor) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After ReduceOpVariants //----- //
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.tensor {
  %0 = torch.tensor_static_info_cast %arg0 : !torch.vtensor<[?,?,?],f32> to !torch.vtensor
  %1 = torch.copy.to_tensor %0 : !torch.tensor
  %int3 = torch.constant.int 3
  %int2 = torch.constant.int 2
  %int1 = torch.constant.int 1
  %int0 = torch.constant.int 0
  %2 = torch.aten.slice.Tensor %1, %int0, %int0, %int1, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
  %3 = torch.aten.slice.Tensor %2, %int1, %int1, %int2, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
  %4 = torch.aten.slice.Tensor %3, %int2, %int1, %int3, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
  return %4 : !torch.tensor
}


//===-------------------------------------------===//
Processing operation : 'torch.tensor_static_info_cast'(0x68af140) {
  %0 = "torch.tensor_static_info_cast"(%arg0) : (!torch.vtensor<[?,?,?],f32>) -> !torch.vtensor


  * Pattern  : 'torch.tensor_static_info_cast -> ()' {
Trying to match ""
"" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.copy.to_tensor'(0x68b3360) {
  %1 = "torch.copy.to_tensor"(%0) : (!torch.vtensor) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68a99f0) {
  %2 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68be3f0) {
  %0 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686b710) {
  %3 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68a99f0) {
  %0 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686cd80) {
  %4 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686b710) {
  %0 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x684ea00) {
  %5 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686cd80) {
  %0 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6870410) {
  %6 = "torch.aten.slice.Tensor"(%5, %0, %0, %1, %1) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6853080) {
  %7 = "torch.aten.slice.Tensor"(%6, %1, %1, %2, %1) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %8 = "torch.aten.slice.Tensor"(%7, %2, %1, %3, %1) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.return'(0x68a5110) {
  "std.return"(%8) : (!torch.tensor) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686cd80) {
  %0 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686b710) {
  %1 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68a99f0) {
  %2 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68be3f0) {
  %3 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.tensor_static_info_cast'(0x68af140) {
  %4 = "torch.tensor_static_info_cast"(%arg0) : (!torch.vtensor<[?,?,?],f32>) -> !torch.vtensor


  * Pattern  : 'torch.tensor_static_info_cast -> ()' {
Trying to match ""
"" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.copy.to_tensor'(0x68b3360) {
  %5 = "torch.copy.to_tensor"(%4) : (!torch.vtensor) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6870410) {
  %6 = "torch.aten.slice.Tensor"(%5, %0, %0, %1, %1) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6853080) {
  %7 = "torch.aten.slice.Tensor"(%6, %1, %1, %2, %1) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %8 = "torch.aten.slice.Tensor"(%7, %2, %1, %3, %1) : (!torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.return'(0x68a5110) {
  "std.return"(%8) : (!torch.tensor) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer //----- //
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.tensor {
  %int0 = torch.constant.int 0
  %int1 = torch.constant.int 1
  %int2 = torch.constant.int 2
  %int3 = torch.constant.int 3
  %0 = torch.tensor_static_info_cast %arg0 : !torch.vtensor<[?,?,?],f32> to !torch.vtensor
  %1 = torch.copy.to_tensor %0 : !torch.tensor
  %2 = torch.aten.slice.Tensor %1, %int0, %int0, %int1, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
  %3 = torch.aten.slice.Tensor %2, %int1, %int1, %int2, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
  %4 = torch.aten.slice.Tensor %3, %int2, %int1, %int3, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
  return %4 : !torch.tensor
}

// -----// IR Dump After SymbolDCE //----- //
module attributes {torch.debug_module_name = "MyModule"}  {
  func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.tensor {
    %int0 = torch.constant.int 0
    %int1 = torch.constant.int 1
    %int2 = torch.constant.int 2
    %int3 = torch.constant.int 3
    %0 = torch.tensor_static_info_cast %arg0 : !torch.vtensor<[?,?,?],f32> to !torch.vtensor
    %1 = torch.copy.to_tensor %0 : !torch.tensor
    %2 = torch.aten.slice.Tensor %1, %int0, %int0, %int1, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %3 = torch.aten.slice.Tensor %2, %int1, %int1, %int2, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    %4 = torch.aten.slice.Tensor %3, %int2, %int1, %int3, %int1 : !torch.tensor, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor
    return %4 : !torch.tensor
  }
}


// -----// IR Dump After RefineTypes //----- //
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.tensor {
  %int0 = torch.constant.int 0
  %int1 = torch.constant.int 1
  %int2 = torch.constant.int 2
  %int3 = torch.constant.int 3
  %0 = torch.tensor_static_info_cast %arg0 : !torch.vtensor<[?,?,?],f32> to !torch.vtensor<[?,?,?],f32>
  %1 = torch.copy.to_tensor %0 : !torch.tensor<[?,?,?],f32>
  %2 = torch.aten.slice.Tensor %1, %int0, %int0, %int1, %int1 : !torch.tensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor<[1,?,?],f32>
  %3 = torch.aten.slice.Tensor %2, %int1, %int1, %int2, %int1 : !torch.tensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor<[1,1,?],f32>
  %4 = torch.aten.slice.Tensor %3, %int2, %int1, %int3, %int1 : !torch.tensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor<[1,1,2],f32>
  %5 = torch.tensor_static_info_cast %4 : !torch.tensor<[1,1,2],f32> to !torch.tensor
  return %5 : !torch.tensor
}

// -----// IR Dump After RefinePublicReturn //----- //
module attributes {torch.debug_module_name = "MyModule"}  {
  func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
    %int0 = torch.constant.int 0
    %int1 = torch.constant.int 1
    %int2 = torch.constant.int 2
    %int3 = torch.constant.int 3
    %0 = torch.tensor_static_info_cast %arg0 : !torch.vtensor<[?,?,?],f32> to !torch.vtensor<[?,?,?],f32>
    %1 = torch.copy.to_tensor %0 : !torch.tensor<[?,?,?],f32>
    %2 = torch.aten.slice.Tensor %1, %int0, %int0, %int1, %int1 : !torch.tensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor<[1,?,?],f32>
    %3 = torch.aten.slice.Tensor %2, %int1, %int1, %int2, %int1 : !torch.tensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor<[1,1,?],f32>
    %4 = torch.aten.slice.Tensor %3, %int2, %int1, %int3, %int1 : !torch.tensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor<[1,1,2],f32>
    %5 = torch.tensor_static_info_cast %4 : !torch.tensor<[1,1,2],f32> to !torch.tensor
    %6 = torch.copy.to_vtensor %4 : !torch.vtensor<[1,1,2],f32>
    return %6 : !torch.vtensor<[1,1,2],f32>
  }
}



//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686cd80) {
  %0 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68d8350) {
  %0 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686b710) {
  %1 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686cd80) {
  %0 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68a99f0) {
  %2 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686b710) {
  %0 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68be3f0) {
  %3 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68a99f0) {
  %0 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.tensor_static_info_cast'(0x68af140) {
  %4 = "torch.tensor_static_info_cast"(%arg0) : (!torch.vtensor<[?,?,?],f32>) -> !torch.vtensor<[?,?,?],f32>

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.copy.to_tensor'(0x68b3360) {
  %4 = "torch.copy.to_tensor"(%arg0) : (!torch.vtensor<[?,?,?],f32>) -> !torch.tensor<[?,?,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6870410) {
  %5 = "torch.aten.slice.Tensor"(%4, %3, %3, %2, %2) : (!torch.tensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor<[1,?,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6853080) {
  %6 = "torch.aten.slice.Tensor"(%5, %2, %2, %1, %2) : (!torch.tensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor<[1,1,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %7 = "torch.aten.slice.Tensor"(%6, %1, %2, %0, %2) : (!torch.tensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor<[1,1,2],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.tensor_static_info_cast'(0x68aa720) {
  %8 = "torch.tensor_static_info_cast"(%7) : (!torch.tensor<[1,1,2],f32>) -> !torch.tensor

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.copy.to_vtensor'(0x688a8e0) {
  %8 = "torch.copy.to_vtensor"(%7) : (!torch.tensor<[1,1,2],f32>) -> !torch.vtensor<[1,1,2],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.return'(0x68a5110) {
  "std.return"(%8) : (!torch.vtensor<[1,1,2],f32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68a99f0) {
  %0 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686b710) {
  %1 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686cd80) {
  %2 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68d8350) {
  %3 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.copy.to_tensor'(0x68b3360) {
  %4 = "torch.copy.to_tensor"(%arg0) : (!torch.vtensor<[?,?,?],f32>) -> !torch.tensor<[?,?,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6870410) {
  %5 = "torch.aten.slice.Tensor"(%4, %3, %3, %2, %2) : (!torch.tensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor<[1,?,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6853080) {
  %6 = "torch.aten.slice.Tensor"(%5, %2, %2, %1, %2) : (!torch.tensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor<[1,1,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %7 = "torch.aten.slice.Tensor"(%6, %1, %2, %0, %2) : (!torch.tensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor<[1,1,2],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.copy.to_vtensor'(0x688a8e0) {
  %8 = "torch.copy.to_vtensor"(%7) : (!torch.tensor<[1,1,2],f32>) -> !torch.vtensor<[1,1,2],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.return'(0x68a5110) {
  "std.return"(%8) : (!torch.vtensor<[1,1,2],f32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer //----- //
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %int3 = torch.constant.int 3
  %int2 = torch.constant.int 2
  %int1 = torch.constant.int 1
  %int0 = torch.constant.int 0
  %0 = torch.copy.to_tensor %arg0 : !torch.tensor<[?,?,?],f32>
  %1 = torch.aten.slice.Tensor %0, %int0, %int0, %int1, %int1 : !torch.tensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor<[1,?,?],f32>
  %2 = torch.aten.slice.Tensor %1, %int1, %int1, %int2, %int1 : !torch.tensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor<[1,1,?],f32>
  %3 = torch.aten.slice.Tensor %2, %int2, %int1, %int3, %int1 : !torch.tensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor<[1,1,2],f32>
  %4 = torch.copy.to_vtensor %3 : !torch.vtensor<[1,1,2],f32>
  return %4 : !torch.vtensor<[1,1,2],f32>
}


//===-------------------------------------------===//
Processing operation : 'std.return'(0x68a5110) {
  "std.return"(%8) : (!torch.vtensor<[1,1,2],f32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.copy.to_vtensor'(0x688a8e0) {
  %8 = "torch.copy.to_vtensor"(%7) : (!torch.tensor<[1,1,2],f32>) -> !torch.vtensor<[1,1,2],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %7 = "torch.aten.slice.Tensor"(%6, %1, %2, %0, %2) : (!torch.tensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor<[1,1,2],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6853080) {
  %6 = "torch.aten.slice.Tensor"(%5, %2, %2, %1, %2) : (!torch.tensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor<[1,1,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6870410) {
  %5 = "torch.aten.slice.Tensor"(%4, %3, %3, %2, %2) : (!torch.tensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.tensor<[1,?,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.copy.to_tensor'(0x68b3360) {
  %4 = "torch.copy.to_tensor"(%arg0) : (!torch.vtensor<[?,?,?],f32>) -> !torch.tensor<[?,?,?],f32>


  * Pattern (anonymous namespace)::AbstractlyInterpretCopyToNonValueTensorOpUsersWithinABlock : 'torch.copy.to_tensor -> ()' {
Trying to match "(anonymous namespace)::AbstractlyInterpretCopyToNonValueTensorOpUsersWithinABlock"
"(anonymous namespace)::AbstractlyInterpretCopyToNonValueTensorOpUsersWithinABlock" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::RewriteViewLikeSubgraph : 'torch.copy.to_tensor -> ()' {
Trying to match "(anonymous namespace)::RewriteViewLikeSubgraph"
    ** Replace : 'torch.copy.to_vtensor'(0x688a8e0)
"(anonymous namespace)::RewriteViewLikeSubgraph" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %int3 = torch.constant.int 3
  %int2 = torch.constant.int 2
  %int1 = torch.constant.int 1
  %int0 = torch.constant.int 0
  %0 = torch.copy.to_tensor %arg0 : !torch.tensor<[?,?,?],f32>
  %1 = torch.aten.slice.Tensor %arg0, %int0, %int0, %int1, %int1 : !torch.vtensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,?],f32>
  %2 = torch.aten.slice.Tensor %1, %int1, %int1, %int2, %int1 : !torch.vtensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,?],f32>
  %3 = torch.aten.slice.Tensor %2, %int2, %int1, %int3, %int1 : !torch.vtensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,2],f32>
  return %3 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.return'(0x68a5110) {
  "std.return"(%7) : (!torch.vtensor<[1,1,2],f32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68d8350) {
  %3 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6870410) {
  %5 = "torch.aten.slice.Tensor"(%arg0, %0, %0, %3, %3) : (!torch.vtensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,?,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68d8530) {
  %0 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686cd80) {
  %3 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %7 = "torch.aten.slice.Tensor"(%6, %3, %0, %2, %0) : (!torch.vtensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,1,2],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6853080) {
  %6 = "torch.aten.slice.Tensor"(%5, %0, %0, %3, %0) : (!torch.vtensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,1,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6870410) {
  %5 = "torch.aten.slice.Tensor"(%arg0, %1, %1, %0, %0) : (!torch.vtensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,?,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68d8350) {
  %0 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686b710) {
  %3 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %7 = "torch.aten.slice.Tensor"(%6, %0, %1, %3, %1) : (!torch.vtensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,1,2],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6853080) {
  %6 = "torch.aten.slice.Tensor"(%5, %1, %1, %0, %1) : (!torch.vtensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,1,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686cd80) {
  %0 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68a99f0) {
  %3 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %7 = "torch.aten.slice.Tensor"(%6, %1, %2, %0, %2) : (!torch.vtensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,1,2],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686b710) {
  %0 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//
** Erase   : 'torch.copy.to_tensor'(0x68b3360)

//===-------------------------------------------===//
Processing operation : 'std.return'(0x68a5110) {
  "std.return"(%6) : (!torch.vtensor<[1,1,2],f32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %6 = "torch.aten.slice.Tensor"(%5, %1, %2, %0, %2) : (!torch.vtensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,1,2],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6853080) {
  %5 = "torch.aten.slice.Tensor"(%4, %2, %2, %1, %2) : (!torch.vtensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,1,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6870410) {
  %4 = "torch.aten.slice.Tensor"(%arg0, %3, %3, %2, %2) : (!torch.vtensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,?,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68d8530) {
  %3 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68d8350) {
  %2 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686cd80) {
  %1 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686b710) {
  %0 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After MaximizeValueSemantics //----- //
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %int3 = torch.constant.int 3
  %int2 = torch.constant.int 2
  %int1 = torch.constant.int 1
  %int0 = torch.constant.int 0
  %0 = torch.aten.slice.Tensor %arg0, %int0, %int0, %int1, %int1 : !torch.vtensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,?],f32>
  %1 = torch.aten.slice.Tensor %0, %int1, %int1, %int2, %int1 : !torch.vtensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,?],f32>
  %2 = torch.aten.slice.Tensor %1, %int2, %int1, %int3, %int1 : !torch.vtensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,2],f32>
  return %2 : !torch.vtensor<[1,1,2],f32>
}


//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686b710) {
  %0 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68a99f0) {
  %0 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686cd80) {
  %1 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686b710) {
  %0 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68d8350) {
  %2 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686cd80) {
  %0 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68d8530) {
  %3 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68d8350) {
  %0 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6870410) {
  %4 = "torch.aten.slice.Tensor"(%arg0, %0, %0, %1, %1) : (!torch.vtensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,?,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6853080) {
  %5 = "torch.aten.slice.Tensor"(%4, %1, %1, %2, %1) : (!torch.vtensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,1,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %6 = "torch.aten.slice.Tensor"(%5, %2, %1, %3, %1) : (!torch.vtensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,1,2],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.return'(0x68a5110) {
  "std.return"(%6) : (!torch.vtensor<[1,1,2],f32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68d8350) {
  %0 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686cd80) {
  %1 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x686b710) {
  %2 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x68a99f0) {
  %3 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6870410) {
  %4 = "torch.aten.slice.Tensor"(%arg0, %0, %0, %1, %1) : (!torch.vtensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,?,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x6853080) {
  %5 = "torch.aten.slice.Tensor"(%4, %1, %1, %2, %1) : (!torch.vtensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,1,?],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %6 = "torch.aten.slice.Tensor"(%5, %2, %1, %3, %1) : (!torch.vtensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,1,2],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.return'(0x68a5110) {
  "std.return"(%6) : (!torch.vtensor<[1,1,2],f32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer //----- //
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %int0 = torch.constant.int 0
  %int1 = torch.constant.int 1
  %int2 = torch.constant.int 2
  %int3 = torch.constant.int 3
  %0 = torch.aten.slice.Tensor %arg0, %int0, %int0, %int1, %int1 : !torch.vtensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,?],f32>
  %1 = torch.aten.slice.Tensor %0, %int1, %int1, %int2, %int1 : !torch.vtensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,?],f32>
  %2 = torch.aten.slice.Tensor %1, %int2, %int1, %int3, %int1 : !torch.vtensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,2],f32>
  return %2 : !torch.vtensor<[1,1,2],f32>
}


//===-------------------------------------------===//
Legalizing operation : 'builtin.func'(0x686d8b0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x68d8350) {
  %0 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x686cd80) {
  %1 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x686b710) {
  %2 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x68a99f0) {
  %3 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.slice.Tensor'(0x6870410) {
  %4 = "torch.aten.slice.Tensor"(%arg0, %0, %0, %1, %1) : (!torch.vtensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,?,?],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.slice.Tensor'(0x6853080) {
  %5 = "torch.aten.slice.Tensor"(%4, %1, %1, %2, %1) : (!torch.vtensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,1,?],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %6 = "torch.aten.slice.Tensor"(%5, %2, %1, %3, %1) : (!torch.vtensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,1,2],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.return'(0x68a5110) {
  "std.return"(%6) : (!torch.vtensor<[1,1,2],f32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//
// -----// IR Dump After DecomposeComplexOps //----- //
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %int0 = torch.constant.int 0
  %int1 = torch.constant.int 1
  %int2 = torch.constant.int 2
  %int3 = torch.constant.int 3
  %0 = torch.aten.slice.Tensor %arg0, %int0, %int0, %int1, %int1 : !torch.vtensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,?],f32>
  %1 = torch.aten.slice.Tensor %0, %int1, %int1, %int2, %int1 : !torch.vtensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,?],f32>
  %2 = torch.aten.slice.Tensor %1, %int2, %int1, %int3, %int1 : !torch.vtensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,2],f32>
  return %2 : !torch.vtensor<[1,1,2],f32>
}

// -----// IR Dump After VerifyInvariantsBeforeBackendLowering //----- //
module attributes {torch.debug_module_name = "MyModule"}  {
  func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
    %int0 = torch.constant.int 0
    %int1 = torch.constant.int 1
    %int2 = torch.constant.int 2
    %int3 = torch.constant.int 3
    %0 = torch.aten.slice.Tensor %arg0, %int0, %int0, %int1, %int1 : !torch.vtensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,?],f32>
    %1 = torch.aten.slice.Tensor %0, %int1, %int1, %int2, %int1 : !torch.vtensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,?],f32>
    %2 = torch.aten.slice.Tensor %1, %int2, %int1, %int3, %int1 : !torch.vtensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,2],f32>
    return %2 : !torch.vtensor<[1,1,2],f32>
  }
}



//===-------------------------------------------===//
Legalizing operation : 'builtin.func'(0x686d8b0) {
  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'builtin.func -> ()' {
Trying to match "(anonymous namespace)::ConvertElementwiseOp"
    ** Failure : not a supported elementwise op
"(anonymous namespace)::ConvertElementwiseOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'builtin.func -> ()' {
Trying to match "(anonymous namespace)::ConvertReductionOp"
    ** Failure : not a supported reduce op
"(anonymous namespace)::ConvertReductionOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'builtin.func -> ()' {
Trying to match "(anonymous namespace)::ConvertAtenSliceLikeOp"
    ** Failure : not a supported slice op
"(anonymous namespace)::ConvertAtenSliceLikeOp" result 0
  } -> FAILURE : pattern failed to match
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x68d8350) {
  %0 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertElementwiseOp"
    ** Failure : not a supported elementwise op
"(anonymous namespace)::ConvertElementwiseOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertReductionOp"
    ** Failure : not a supported reduce op
"(anonymous namespace)::ConvertReductionOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertAtenSliceLikeOp"
    ** Failure : not a supported slice op
"(anonymous namespace)::ConvertAtenSliceLikeOp" result 0
  } -> FAILURE : pattern failed to match
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x686cd80) {
  %1 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertElementwiseOp"
    ** Failure : not a supported elementwise op
"(anonymous namespace)::ConvertElementwiseOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertReductionOp"
    ** Failure : not a supported reduce op
"(anonymous namespace)::ConvertReductionOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertAtenSliceLikeOp"
    ** Failure : not a supported slice op
"(anonymous namespace)::ConvertAtenSliceLikeOp" result 0
  } -> FAILURE : pattern failed to match
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x686b710) {
  %2 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertElementwiseOp"
    ** Failure : not a supported elementwise op
"(anonymous namespace)::ConvertElementwiseOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertReductionOp"
    ** Failure : not a supported reduce op
"(anonymous namespace)::ConvertReductionOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertAtenSliceLikeOp"
    ** Failure : not a supported slice op
"(anonymous namespace)::ConvertAtenSliceLikeOp" result 0
  } -> FAILURE : pattern failed to match
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x68a99f0) {
  %3 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertElementwiseOp"
    ** Failure : not a supported elementwise op
"(anonymous namespace)::ConvertElementwiseOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertReductionOp"
    ** Failure : not a supported reduce op
"(anonymous namespace)::ConvertReductionOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertAtenSliceLikeOp"
    ** Failure : not a supported slice op
"(anonymous namespace)::ConvertAtenSliceLikeOp" result 0
  } -> FAILURE : pattern failed to match
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.slice.Tensor'(0x6870410) {
  %4 = "torch.aten.slice.Tensor"(%arg0, %0, %0, %1, %1) : (!torch.vtensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,?,?],f32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.aten.slice.Tensor -> ()' {
Trying to match "(anonymous namespace)::ConvertElementwiseOp"
    ** Failure : not a supported elementwise op
"(anonymous namespace)::ConvertElementwiseOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.aten.slice.Tensor -> ()' {
Trying to match "(anonymous namespace)::ConvertReductionOp"
    ** Failure : not a supported reduce op
"(anonymous namespace)::ConvertReductionOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.aten.slice.Tensor -> ()' {
Trying to match "(anonymous namespace)::ConvertAtenSliceLikeOp"
    ** Insert  : 'arith.constant'(0x68ebc70)
    ** Insert  : 'tensor.dim'(0x68f1f10)
    ** Insert  : 'arith.constant'(0x68f49a0)
    ** Insert  : 'tensor.dim'(0x68f4a00)
    ** Insert  : 'arith.constant'(0x68f5fa0)
    ** Insert  : 'tensor.dim'(0x68f6000)
    ** Insert  : 'arith.constant'(0x68f60b0)
    ** Insert  : 'arith.constant'(0x68f6180)
    ** Insert  : 'arith.constant'(0x68f61e0)
    ** Insert  : 'arith.constant'(0x68f6240)
    ** Insert  : 'arith.addi'(0x68f62a0)
    ** Insert  : 'arith.cmpi'(0x68f6350)
    ** Insert  : 'std.select'(0x68f6400)
    ** Insert  : 'arith.addi'(0x68f64d0)
    ** Insert  : 'arith.cmpi'(0x68f6660)
    ** Insert  : 'std.select'(0x68f6710)
    ** Insert  : 'arith.subi'(0x68f67e0)
    ** Insert  : 'arith.constant'(0x68f6890)
    ** Insert  : 'arith.addi'(0x68f68f0)
    ** Insert  : 'arith.subi'(0x68f69a0)
    ** Insert  : 'arith.ceildivsi'(0x68f6a50)
    ** Insert  : 'linalg.init_tensor'(0x68f6b00)
    ** Insert  : 'arith.muli'(0x68f6bd0)
    ** Insert  : 'tensor.insert_slice'(0x68fb490)
    ** Insert  : 'tensor.cast'(0x68af140)
    ** Replace : 'torch.aten.slice.Tensor'(0x6870410)
"(anonymous namespace)::ConvertAtenSliceLikeOp" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68ebc70) {
      %7 = "arith.constant"() {value = 0 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x68f1f10) {
      %8 = "tensor.dim"(%0, %7) : (tensor<?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68f49a0) {
      %9 = "arith.constant"() {value = 1 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x68f4a00) {
      %10 = "tensor.dim"(%0, %9) : (tensor<?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68f5fa0) {
      %11 = "arith.constant"() {value = 2 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x68f6000) {
      %12 = "tensor.dim"(%0, %11) : (tensor<?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68f60b0) {
      %13 = "arith.constant"() {value = 0 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68f6180) {
      %14 = "arith.constant"() {value = 1 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68f61e0) {
      %15 = "arith.constant"() {value = 0 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68f6240) {
      %16 = "arith.constant"() {value = 1 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.addi'(0x68f62a0) {
      %17 = "arith.addi"(%15, %8) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.cmpi'(0x68f6350) {
      %18 = "arith.cmpi"(%15, %13) {predicate = 2 : i64} : (index, index) -> i1

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'std.select'(0x68f6400) {
      %19 = "std.select"(%18, %17, %15) : (i1, index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.addi'(0x68f64d0) {
      %20 = "arith.addi"(%16, %8) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.cmpi'(0x68f6660) {
      %21 = "arith.cmpi"(%16, %13) {predicate = 2 : i64} : (index, index) -> i1

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'std.select'(0x68f6710) {
      %22 = "std.select"(%21, %20, %16) : (i1, index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.subi'(0x68f67e0) {
      %23 = "arith.subi"(%22, %19) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68f6890) {
      %24 = "arith.constant"() {value = 1 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.addi'(0x68f68f0) {
      %25 = "arith.addi"(%23, %24) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.subi'(0x68f69a0) {
      %26 = "arith.subi"(%25, %14) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.ceildivsi'(0x68f6a50) {
      %27 = "arith.ceildivsi"(%26, %24) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.init_tensor'(0x68f6b00) {
      %28 = "linalg.init_tensor"(%27, %10, %12) {static_sizes = [-1, -1, -1]} : (index, index, index) -> tensor<?x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.muli'(0x68f6bd0) {
      %29 = "arith.muli"(%14, %24) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.insert_slice'(0x68fb490) {
      %30 = "tensor.insert_slice"(%0, %28, %13, %13, %13, %27, %10, %12, %29, %14, %14) {operand_segment_sizes = dense<[1, 1, 3, 3, 3]> : vector<5xi32>, static_offsets = [-9223372036854775808, -9223372036854775808, -9223372036854775808], static_sizes = [-1, -1, -1], static_strides = [-9223372036854775808, -9223372036854775808, -9223372036854775808]} : (tensor<?x?x?xf32>, tensor<?x?x?xf32>, index, index, index, index, index, index, index, index, index) -> tensor<?x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.cast'(0x68af140) {
      %31 = "tensor.cast"(%30) : (tensor<?x?x?xf32>) -> tensor<1x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %0 = builtin.unrealized_conversion_cast %arg0 : !torch.vtensor<[?,?,?],f32> to tensor<?x?x?xf32>
  %int0 = torch.constant.int 0
  %1 = builtin.unrealized_conversion_cast %int0 : !torch.int to i64
  %int1 = torch.constant.int 1
  %2 = builtin.unrealized_conversion_cast %int1 : !torch.int to i64
  %int2 = torch.constant.int 2
  %int3 = torch.constant.int 3
  %c0 = arith.constant 0 : index
  %3 = tensor.dim %0, %c0 : tensor<?x?x?xf32>
  %c1 = arith.constant 1 : index
  %4 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %c2 = arith.constant 2 : index
  %5 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c1_3 = arith.constant 1 : index
  %6 = arith.addi %c0_2, %3 : index
  %7 = arith.cmpi slt, %c0_2, %c0_0 : index
  %8 = select %7, %6, %c0_2 : index
  %9 = arith.addi %c1_3, %3 : index
  %10 = arith.cmpi slt, %c1_3, %c0_0 : index
  %11 = select %10, %9, %c1_3 : index
  %12 = arith.subi %11, %8 : index
  %c1_4 = arith.constant 1 : index
  %13 = arith.addi %12, %c1_4 : index
  %14 = arith.subi %13, %c1_1 : index
  %15 = arith.ceildivsi %14, %c1_4 : index
  %16 = linalg.init_tensor [%15, %4, %5] : tensor<?x?x?xf32>
  %17 = arith.muli %c1_1, %c1_4 : index
  %18 = tensor.insert_slice %0 into %16[%c0_0, %c0_0, %c0_0] [%15, %4, %5] [%17, %c1_1, %c1_1] : tensor<?x?x?xf32> into tensor<?x?x?xf32>
  %19 = tensor.cast %18 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %20 = torch.aten.slice.Tensor %arg0, %int0, %int0, %int1, %int1 : !torch.vtensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,?],f32>
  %21 = torch.aten.slice.Tensor %20, %int1, %int1, %int2, %int1 : !torch.vtensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,?],f32>
  %22 = torch.aten.slice.Tensor %21, %int2, %int1, %int3, %int1 : !torch.vtensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,2],f32>
  return %22 : !torch.vtensor<[1,1,2],f32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.slice.Tensor'(0x6853080) {
  %33 = "torch.aten.slice.Tensor"(%32, %3, %3, %5, %3) : (!torch.vtensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,1,?],f32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.aten.slice.Tensor -> ()' {
Trying to match "(anonymous namespace)::ConvertElementwiseOp"
    ** Failure : not a supported elementwise op
"(anonymous namespace)::ConvertElementwiseOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.aten.slice.Tensor -> ()' {
Trying to match "(anonymous namespace)::ConvertReductionOp"
    ** Failure : not a supported reduce op
"(anonymous namespace)::ConvertReductionOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.aten.slice.Tensor -> ()' {
Trying to match "(anonymous namespace)::ConvertAtenSliceLikeOp"
    ** Insert  : 'arith.constant'(0x68fba70)
    ** Insert  : 'tensor.dim'(0x68fbfb0)
    ** Insert  : 'arith.constant'(0x68ef190)
    ** Insert  : 'tensor.dim'(0x68f6580)
    ** Insert  : 'arith.constant'(0x68fc220)
    ** Insert  : 'tensor.dim'(0x68fc280)
    ** Insert  : 'arith.constant'(0x68fc330)
    ** Insert  : 'arith.constant'(0x68fc390)
    ** Insert  : 'arith.constant'(0x68fc3f0)
    ** Insert  : 'arith.constant'(0x68fc450)
    ** Insert  : 'arith.addi'(0x68fc4b0)
    ** Insert  : 'arith.cmpi'(0x68fc560)
    ** Insert  : 'std.select'(0x68fc610)
    ** Insert  : 'arith.addi'(0x68fc6e0)
    ** Insert  : 'arith.cmpi'(0x68fc790)
    ** Insert  : 'std.select'(0x68fc840)
    ** Insert  : 'arith.subi'(0x68fc910)
    ** Insert  : 'arith.constant'(0x68fc9c0)
    ** Insert  : 'arith.addi'(0x68fca20)
    ** Insert  : 'arith.subi'(0x68fe310)
    ** Insert  : 'arith.ceildivsi'(0x68fe3c0)
    ** Insert  : 'linalg.init_tensor'(0x68fe470)
    ** Insert  : 'arith.muli'(0x68fe540)
    ** Insert  : 'tensor.insert_slice'(0x68fe690)
    ** Insert  : 'tensor.cast'(0x68fe860)
    ** Replace : 'torch.aten.slice.Tensor'(0x6853080)
"(anonymous namespace)::ConvertAtenSliceLikeOp" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68fba70) {
      %34 = "arith.constant"() {value = 0 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x68fbfb0) {
      %35 = "tensor.dim"(%32, %34) : (tensor<1x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68ef190) {
      %36 = "arith.constant"() {value = 1 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x68f6580) {
      %37 = "tensor.dim"(%32, %36) : (tensor<1x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68fc220) {
      %38 = "arith.constant"() {value = 2 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x68fc280) {
      %39 = "tensor.dim"(%32, %38) : (tensor<1x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68fc330) {
      %40 = "arith.constant"() {value = 0 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68fc390) {
      %41 = "arith.constant"() {value = 1 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68fc3f0) {
      %42 = "arith.constant"() {value = 1 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68fc450) {
      %43 = "arith.constant"() {value = 2 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.addi'(0x68fc4b0) {
      %44 = "arith.addi"(%42, %37) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.cmpi'(0x68fc560) {
      %45 = "arith.cmpi"(%42, %40) {predicate = 2 : i64} : (index, index) -> i1

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'std.select'(0x68fc610) {
      %46 = "std.select"(%45, %44, %42) : (i1, index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.addi'(0x68fc6e0) {
      %47 = "arith.addi"(%43, %37) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.cmpi'(0x68fc790) {
      %48 = "arith.cmpi"(%43, %40) {predicate = 2 : i64} : (index, index) -> i1

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'std.select'(0x68fc840) {
      %49 = "std.select"(%48, %47, %43) : (i1, index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.subi'(0x68fc910) {
      %50 = "arith.subi"(%49, %46) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68fc9c0) {
      %51 = "arith.constant"() {value = 1 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.addi'(0x68fca20) {
      %52 = "arith.addi"(%50, %51) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.subi'(0x68fe310) {
      %53 = "arith.subi"(%52, %41) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.ceildivsi'(0x68fe3c0) {
      %54 = "arith.ceildivsi"(%53, %51) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.init_tensor'(0x68fe470) {
      %55 = "linalg.init_tensor"(%35, %54, %39) {static_sizes = [-1, -1, -1]} : (index, index, index) -> tensor<?x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.muli'(0x68fe540) {
      %56 = "arith.muli"(%41, %51) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.insert_slice'(0x68fe690) {
      %57 = "tensor.insert_slice"(%32, %55, %40, %40, %40, %35, %54, %39, %41, %56, %41) {operand_segment_sizes = dense<[1, 1, 3, 3, 3]> : vector<5xi32>, static_offsets = [-9223372036854775808, -9223372036854775808, -9223372036854775808], static_sizes = [-1, -1, -1], static_strides = [-9223372036854775808, -9223372036854775808, -9223372036854775808]} : (tensor<1x?x?xf32>, tensor<?x?x?xf32>, index, index, index, index, index, index, index, index, index) -> tensor<?x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.cast'(0x68fe860) {
      %58 = "tensor.cast"(%57) : (tensor<?x?x?xf32>) -> tensor<1x1x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %0 = builtin.unrealized_conversion_cast %arg0 : !torch.vtensor<[?,?,?],f32> to tensor<?x?x?xf32>
  %int0 = torch.constant.int 0
  %1 = builtin.unrealized_conversion_cast %int0 : !torch.int to i64
  %int1 = torch.constant.int 1
  %2 = builtin.unrealized_conversion_cast %int1 : !torch.int to i64
  %int2 = torch.constant.int 2
  %3 = builtin.unrealized_conversion_cast %int2 : !torch.int to i64
  %int3 = torch.constant.int 3
  %c0 = arith.constant 0 : index
  %4 = tensor.dim %0, %c0 : tensor<?x?x?xf32>
  %c1 = arith.constant 1 : index
  %5 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %c2 = arith.constant 2 : index
  %6 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c1_3 = arith.constant 1 : index
  %7 = arith.addi %c0_2, %4 : index
  %8 = arith.cmpi slt, %c0_2, %c0_0 : index
  %9 = select %8, %7, %c0_2 : index
  %10 = arith.addi %c1_3, %4 : index
  %11 = arith.cmpi slt, %c1_3, %c0_0 : index
  %12 = select %11, %10, %c1_3 : index
  %13 = arith.subi %12, %9 : index
  %c1_4 = arith.constant 1 : index
  %14 = arith.addi %13, %c1_4 : index
  %15 = arith.subi %14, %c1_1 : index
  %16 = arith.ceildivsi %15, %c1_4 : index
  %17 = linalg.init_tensor [%16, %5, %6] : tensor<?x?x?xf32>
  %18 = arith.muli %c1_1, %c1_4 : index
  %19 = tensor.insert_slice %0 into %17[%c0_0, %c0_0, %c0_0] [%16, %5, %6] [%18, %c1_1, %c1_1] : tensor<?x?x?xf32> into tensor<?x?x?xf32>
  %20 = tensor.cast %19 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %21 = torch.aten.slice.Tensor %arg0, %int0, %int0, %int1, %int1 : !torch.vtensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,?],f32>
  %c0_5 = arith.constant 0 : index
  %22 = tensor.dim %20, %c0_5 : tensor<1x?x?xf32>
  %c1_6 = arith.constant 1 : index
  %23 = tensor.dim %20, %c1_6 : tensor<1x?x?xf32>
  %c2_7 = arith.constant 2 : index
  %24 = tensor.dim %20, %c2_7 : tensor<1x?x?xf32>
  %c0_8 = arith.constant 0 : index
  %c1_9 = arith.constant 1 : index
  %c1_10 = arith.constant 1 : index
  %c2_11 = arith.constant 2 : index
  %25 = arith.addi %c1_10, %23 : index
  %26 = arith.cmpi slt, %c1_10, %c0_8 : index
  %27 = select %26, %25, %c1_10 : index
  %28 = arith.addi %c2_11, %23 : index
  %29 = arith.cmpi slt, %c2_11, %c0_8 : index
  %30 = select %29, %28, %c2_11 : index
  %31 = arith.subi %30, %27 : index
  %c1_12 = arith.constant 1 : index
  %32 = arith.addi %31, %c1_12 : index
  %33 = arith.subi %32, %c1_9 : index
  %34 = arith.ceildivsi %33, %c1_12 : index
  %35 = linalg.init_tensor [%22, %34, %24] : tensor<?x?x?xf32>
  %36 = arith.muli %c1_9, %c1_12 : index
  %37 = tensor.insert_slice %20 into %35[%c0_8, %c0_8, %c0_8] [%22, %34, %24] [%c1_9, %36, %c1_9] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %38 = tensor.cast %37 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %39 = torch.aten.slice.Tensor %21, %int1, %int1, %int2, %int1 : !torch.vtensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,?],f32>
  %40 = torch.aten.slice.Tensor %39, %int2, %int1, %int3, %int1 : !torch.vtensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,2],f32>
  return %40 : !torch.vtensor<[1,1,2],f32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.slice.Tensor'(0x68a9ab0) {
  %60 = "torch.aten.slice.Tensor"(%59, %5, %3, %7, %3) : (!torch.vtensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.vtensor<[1,1,2],f32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.aten.slice.Tensor -> ()' {
Trying to match "(anonymous namespace)::ConvertElementwiseOp"
    ** Failure : not a supported elementwise op
"(anonymous namespace)::ConvertElementwiseOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.aten.slice.Tensor -> ()' {
Trying to match "(anonymous namespace)::ConvertReductionOp"
    ** Failure : not a supported reduce op
"(anonymous namespace)::ConvertReductionOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.aten.slice.Tensor -> ()' {
Trying to match "(anonymous namespace)::ConvertAtenSliceLikeOp"
    ** Insert  : 'arith.constant'(0x68fe8f0)
    ** Insert  : 'tensor.dim'(0x68fe950)
    ** Insert  : 'arith.constant'(0x68fea00)
    ** Insert  : 'tensor.dim'(0x68fea60)
    ** Insert  : 'arith.constant'(0x68feb10)
    ** Insert  : 'tensor.dim'(0x68feb70)
    ** Insert  : 'arith.constant'(0x6901950)
    ** Insert  : 'arith.constant'(0x69019b0)
    ** Insert  : 'arith.constant'(0x6901a10)
    ** Insert  : 'arith.constant'(0x6901a70)
    ** Insert  : 'arith.addi'(0x6901ad0)
    ** Insert  : 'arith.cmpi'(0x6901b80)
    ** Insert  : 'std.select'(0x6901c30)
    ** Insert  : 'arith.addi'(0x6901d00)
    ** Insert  : 'arith.cmpi'(0x6901db0)
    ** Insert  : 'std.select'(0x6901e60)
    ** Insert  : 'arith.subi'(0x6901f30)
    ** Insert  : 'arith.constant'(0x6901fe0)
    ** Insert  : 'arith.addi'(0x6902040)
    ** Insert  : 'arith.subi'(0x69001e0)
    ** Insert  : 'arith.ceildivsi'(0x6900290)
    ** Insert  : 'linalg.init_tensor'(0x6900340)
    ** Insert  : 'arith.muli'(0x6900410)
    ** Insert  : 'tensor.insert_slice'(0x6900560)
    ** Insert  : 'tensor.cast'(0x6900730)
    ** Replace : 'torch.aten.slice.Tensor'(0x68a9ab0)
"(anonymous namespace)::ConvertAtenSliceLikeOp" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68fe8f0) {
      %61 = "arith.constant"() {value = 0 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x68fe950) {
      %62 = "tensor.dim"(%59, %61) : (tensor<1x1x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68fea00) {
      %63 = "arith.constant"() {value = 1 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x68fea60) {
      %64 = "tensor.dim"(%59, %63) : (tensor<1x1x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68feb10) {
      %65 = "arith.constant"() {value = 2 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x68feb70) {
      %66 = "tensor.dim"(%59, %65) : (tensor<1x1x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x6901950) {
      %67 = "arith.constant"() {value = 0 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x69019b0) {
      %68 = "arith.constant"() {value = 1 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x6901a10) {
      %69 = "arith.constant"() {value = 1 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x6901a70) {
      %70 = "arith.constant"() {value = 3 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.addi'(0x6901ad0) {
      %71 = "arith.addi"(%69, %66) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.cmpi'(0x6901b80) {
      %72 = "arith.cmpi"(%69, %67) {predicate = 2 : i64} : (index, index) -> i1

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'std.select'(0x6901c30) {
      %73 = "std.select"(%72, %71, %69) : (i1, index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.addi'(0x6901d00) {
      %74 = "arith.addi"(%70, %66) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.cmpi'(0x6901db0) {
      %75 = "arith.cmpi"(%70, %67) {predicate = 2 : i64} : (index, index) -> i1

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'std.select'(0x6901e60) {
      %76 = "std.select"(%75, %74, %70) : (i1, index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.subi'(0x6901f30) {
      %77 = "arith.subi"(%76, %73) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x6901fe0) {
      %78 = "arith.constant"() {value = 1 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.addi'(0x6902040) {
      %79 = "arith.addi"(%77, %78) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.subi'(0x69001e0) {
      %80 = "arith.subi"(%79, %68) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.ceildivsi'(0x6900290) {
      %81 = "arith.ceildivsi"(%80, %78) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.init_tensor'(0x6900340) {
      %82 = "linalg.init_tensor"(%62, %64, %81) {static_sizes = [-1, -1, -1]} : (index, index, index) -> tensor<?x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.muli'(0x6900410) {
      %83 = "arith.muli"(%68, %78) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.insert_slice'(0x6900560) {
      %84 = "tensor.insert_slice"(%59, %82, %67, %67, %67, %62, %64, %81, %68, %68, %83) {operand_segment_sizes = dense<[1, 1, 3, 3, 3]> : vector<5xi32>, static_offsets = [-9223372036854775808, -9223372036854775808, -9223372036854775808], static_sizes = [-1, -1, -1], static_strides = [-9223372036854775808, -9223372036854775808, -9223372036854775808]} : (tensor<1x1x?xf32>, tensor<?x?x?xf32>, index, index, index, index, index, index, index, index, index) -> tensor<?x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.cast'(0x6900730) {
      %85 = "tensor.cast"(%84) : (tensor<?x?x?xf32>) -> tensor<1x1x2xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %0 = builtin.unrealized_conversion_cast %arg0 : !torch.vtensor<[?,?,?],f32> to tensor<?x?x?xf32>
  %int0 = torch.constant.int 0
  %1 = builtin.unrealized_conversion_cast %int0 : !torch.int to i64
  %int1 = torch.constant.int 1
  %2 = builtin.unrealized_conversion_cast %int1 : !torch.int to i64
  %int2 = torch.constant.int 2
  %3 = builtin.unrealized_conversion_cast %int2 : !torch.int to i64
  %int3 = torch.constant.int 3
  %4 = builtin.unrealized_conversion_cast %int3 : !torch.int to i64
  %c0 = arith.constant 0 : index
  %5 = tensor.dim %0, %c0 : tensor<?x?x?xf32>
  %c1 = arith.constant 1 : index
  %6 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %c2 = arith.constant 2 : index
  %7 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c1_3 = arith.constant 1 : index
  %8 = arith.addi %c0_2, %5 : index
  %9 = arith.cmpi slt, %c0_2, %c0_0 : index
  %10 = select %9, %8, %c0_2 : index
  %11 = arith.addi %c1_3, %5 : index
  %12 = arith.cmpi slt, %c1_3, %c0_0 : index
  %13 = select %12, %11, %c1_3 : index
  %14 = arith.subi %13, %10 : index
  %c1_4 = arith.constant 1 : index
  %15 = arith.addi %14, %c1_4 : index
  %16 = arith.subi %15, %c1_1 : index
  %17 = arith.ceildivsi %16, %c1_4 : index
  %18 = linalg.init_tensor [%17, %6, %7] : tensor<?x?x?xf32>
  %19 = arith.muli %c1_1, %c1_4 : index
  %20 = tensor.insert_slice %0 into %18[%c0_0, %c0_0, %c0_0] [%17, %6, %7] [%19, %c1_1, %c1_1] : tensor<?x?x?xf32> into tensor<?x?x?xf32>
  %21 = tensor.cast %20 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %22 = torch.aten.slice.Tensor %arg0, %int0, %int0, %int1, %int1 : !torch.vtensor<[?,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,?],f32>
  %c0_5 = arith.constant 0 : index
  %23 = tensor.dim %21, %c0_5 : tensor<1x?x?xf32>
  %c1_6 = arith.constant 1 : index
  %24 = tensor.dim %21, %c1_6 : tensor<1x?x?xf32>
  %c2_7 = arith.constant 2 : index
  %25 = tensor.dim %21, %c2_7 : tensor<1x?x?xf32>
  %c0_8 = arith.constant 0 : index
  %c1_9 = arith.constant 1 : index
  %c1_10 = arith.constant 1 : index
  %c2_11 = arith.constant 2 : index
  %26 = arith.addi %c1_10, %24 : index
  %27 = arith.cmpi slt, %c1_10, %c0_8 : index
  %28 = select %27, %26, %c1_10 : index
  %29 = arith.addi %c2_11, %24 : index
  %30 = arith.cmpi slt, %c2_11, %c0_8 : index
  %31 = select %30, %29, %c2_11 : index
  %32 = arith.subi %31, %28 : index
  %c1_12 = arith.constant 1 : index
  %33 = arith.addi %32, %c1_12 : index
  %34 = arith.subi %33, %c1_9 : index
  %35 = arith.ceildivsi %34, %c1_12 : index
  %36 = linalg.init_tensor [%23, %35, %25] : tensor<?x?x?xf32>
  %37 = arith.muli %c1_9, %c1_12 : index
  %38 = tensor.insert_slice %21 into %36[%c0_8, %c0_8, %c0_8] [%23, %35, %25] [%c1_9, %37, %c1_9] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %39 = tensor.cast %38 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %40 = torch.aten.slice.Tensor %22, %int1, %int1, %int2, %int1 : !torch.vtensor<[1,?,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,?],f32>
  %c0_13 = arith.constant 0 : index
  %41 = tensor.dim %39, %c0_13 : tensor<1x1x?xf32>
  %c1_14 = arith.constant 1 : index
  %42 = tensor.dim %39, %c1_14 : tensor<1x1x?xf32>
  %c2_15 = arith.constant 2 : index
  %43 = tensor.dim %39, %c2_15 : tensor<1x1x?xf32>
  %c0_16 = arith.constant 0 : index
  %c1_17 = arith.constant 1 : index
  %c1_18 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %44 = arith.addi %c1_18, %43 : index
  %45 = arith.cmpi slt, %c1_18, %c0_16 : index
  %46 = select %45, %44, %c1_18 : index
  %47 = arith.addi %c3, %43 : index
  %48 = arith.cmpi slt, %c3, %c0_16 : index
  %49 = select %48, %47, %c3 : index
  %50 = arith.subi %49, %46 : index
  %c1_19 = arith.constant 1 : index
  %51 = arith.addi %50, %c1_19 : index
  %52 = arith.subi %51, %c1_17 : index
  %53 = arith.ceildivsi %52, %c1_19 : index
  %54 = linalg.init_tensor [%41, %42, %53] : tensor<?x?x?xf32>
  %55 = arith.muli %c1_17, %c1_19 : index
  %56 = tensor.insert_slice %39 into %54[%c0_16, %c0_16, %c0_16] [%41, %42, %53] [%c1_17, %c1_17, %55] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %57 = tensor.cast %56 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %58 = torch.aten.slice.Tensor %40, %int2, %int1, %int3, %int1 : !torch.vtensor<[1,1,?],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,2],f32>
  return %58 : !torch.vtensor<[1,1,2],f32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.return'(0x68a5110) {
  "std.return"(%86) : (!torch.vtensor<[1,1,2],f32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
** Insert  : 'torch_c.to_builtin_tensor'(0x6900820)
** Insert  : 'torch_c.from_builtin_tensor'(0x69008b0)
// -----// IR Dump After ConvertTorchToLinalg //----- //
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %int0 = torch.constant.int 0
  %int1 = torch.constant.int 1
  %int2 = torch.constant.int 2
  %int3 = torch.constant.int 3
  %c0 = arith.constant 0 : index
  %1 = tensor.dim %0, %c0 : tensor<?x?x?xf32>
  %c1 = arith.constant 1 : index
  %2 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %c2 = arith.constant 2 : index
  %3 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c1_3 = arith.constant 1 : index
  %4 = arith.addi %c0_2, %1 : index
  %5 = arith.cmpi slt, %c0_2, %c0_0 : index
  %6 = select %5, %4, %c0_2 : index
  %7 = arith.addi %c1_3, %1 : index
  %8 = arith.cmpi slt, %c1_3, %c0_0 : index
  %9 = select %8, %7, %c1_3 : index
  %10 = arith.subi %9, %6 : index
  %c1_4 = arith.constant 1 : index
  %11 = arith.addi %10, %c1_4 : index
  %12 = arith.subi %11, %c1_1 : index
  %13 = arith.ceildivsi %12, %c1_4 : index
  %14 = linalg.init_tensor [%13, %2, %3] : tensor<?x?x?xf32>
  %15 = arith.muli %c1_1, %c1_4 : index
  %16 = tensor.insert_slice %0 into %14[%c0_0, %c0_0, %c0_0] [%13, %2, %3] [%15, %c1_1, %c1_1] : tensor<?x?x?xf32> into tensor<?x?x?xf32>
  %17 = tensor.cast %16 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %c0_5 = arith.constant 0 : index
  %18 = tensor.dim %17, %c0_5 : tensor<1x?x?xf32>
  %c1_6 = arith.constant 1 : index
  %19 = tensor.dim %17, %c1_6 : tensor<1x?x?xf32>
  %c2_7 = arith.constant 2 : index
  %20 = tensor.dim %17, %c2_7 : tensor<1x?x?xf32>
  %c0_8 = arith.constant 0 : index
  %c1_9 = arith.constant 1 : index
  %c1_10 = arith.constant 1 : index
  %c2_11 = arith.constant 2 : index
  %21 = arith.addi %c1_10, %19 : index
  %22 = arith.cmpi slt, %c1_10, %c0_8 : index
  %23 = select %22, %21, %c1_10 : index
  %24 = arith.addi %c2_11, %19 : index
  %25 = arith.cmpi slt, %c2_11, %c0_8 : index
  %26 = select %25, %24, %c2_11 : index
  %27 = arith.subi %26, %23 : index
  %c1_12 = arith.constant 1 : index
  %28 = arith.addi %27, %c1_12 : index
  %29 = arith.subi %28, %c1_9 : index
  %30 = arith.ceildivsi %29, %c1_12 : index
  %31 = linalg.init_tensor [%18, %30, %20] : tensor<?x?x?xf32>
  %32 = arith.muli %c1_9, %c1_12 : index
  %33 = tensor.insert_slice %17 into %31[%c0_8, %c0_8, %c0_8] [%18, %30, %20] [%c1_9, %32, %c1_9] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %34 = tensor.cast %33 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_13 = arith.constant 0 : index
  %35 = tensor.dim %34, %c0_13 : tensor<1x1x?xf32>
  %c1_14 = arith.constant 1 : index
  %36 = tensor.dim %34, %c1_14 : tensor<1x1x?xf32>
  %c2_15 = arith.constant 2 : index
  %37 = tensor.dim %34, %c2_15 : tensor<1x1x?xf32>
  %c0_16 = arith.constant 0 : index
  %c1_17 = arith.constant 1 : index
  %c1_18 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %38 = arith.addi %c1_18, %37 : index
  %39 = arith.cmpi slt, %c1_18, %c0_16 : index
  %40 = select %39, %38, %c1_18 : index
  %41 = arith.addi %c3, %37 : index
  %42 = arith.cmpi slt, %c3, %c0_16 : index
  %43 = select %42, %41, %c3 : index
  %44 = arith.subi %43, %40 : index
  %c1_19 = arith.constant 1 : index
  %45 = arith.addi %44, %c1_19 : index
  %46 = arith.subi %45, %c1_17 : index
  %47 = arith.ceildivsi %46, %c1_19 : index
  %48 = linalg.init_tensor [%35, %36, %47] : tensor<?x?x?xf32>
  %49 = arith.muli %c1_17, %c1_19 : index
  %50 = tensor.insert_slice %34 into %48[%c0_16, %c0_16, %c0_16] [%35, %36, %47] [%c1_17, %c1_17, %49] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %51 = tensor.cast %50 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %52 = torch_c.from_builtin_tensor %51 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %52 : !torch.vtensor<[1,1,2],f32>
}


//===-------------------------------------------===//
Legalizing operation : 'builtin.func'(0x686d8b0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x6900820) {
  %0 = "torch_c.to_builtin_tensor"(%arg0) : (!torch.vtensor<[?,?,?],f32>) -> tensor<?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x68d8350) {
  %1 = "torch.constant.int"() {value = 0 : i64} : () -> !torch.int

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertTorchConstantOp<mlir::torch::Torch::ConstantIntOp>"
    ** Insert  : 'arith.constant'(0x69007c0)
    ** Replace : 'torch.constant.int'(0x68d8350)
"(anonymous namespace)::ConvertTorchConstantOp<mlir::torch::Torch::ConstantIntOp>" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x69007c0) {
      %1 = "arith.constant"() {value = 0 : i64} : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %c0_i64 = arith.constant 0 : i64
  %int0 = torch.constant.int 0
  %int1 = torch.constant.int 1
  %int2 = torch.constant.int 2
  %int3 = torch.constant.int 3
  %c0 = arith.constant 0 : index
  %1 = tensor.dim %0, %c0 : tensor<?x?x?xf32>
  %c1 = arith.constant 1 : index
  %2 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %c2 = arith.constant 2 : index
  %3 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c1_3 = arith.constant 1 : index
  %4 = arith.addi %c0_2, %1 : index
  %5 = arith.cmpi slt, %c0_2, %c0_0 : index
  %6 = select %5, %4, %c0_2 : index
  %7 = arith.addi %c1_3, %1 : index
  %8 = arith.cmpi slt, %c1_3, %c0_0 : index
  %9 = select %8, %7, %c1_3 : index
  %10 = arith.subi %9, %6 : index
  %c1_4 = arith.constant 1 : index
  %11 = arith.addi %10, %c1_4 : index
  %12 = arith.subi %11, %c1_1 : index
  %13 = arith.ceildivsi %12, %c1_4 : index
  %14 = linalg.init_tensor [%13, %2, %3] : tensor<?x?x?xf32>
  %15 = arith.muli %c1_1, %c1_4 : index
  %16 = tensor.insert_slice %0 into %14[%c0_0, %c0_0, %c0_0] [%13, %2, %3] [%15, %c1_1, %c1_1] : tensor<?x?x?xf32> into tensor<?x?x?xf32>
  %17 = tensor.cast %16 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %c0_5 = arith.constant 0 : index
  %18 = tensor.dim %17, %c0_5 : tensor<1x?x?xf32>
  %c1_6 = arith.constant 1 : index
  %19 = tensor.dim %17, %c1_6 : tensor<1x?x?xf32>
  %c2_7 = arith.constant 2 : index
  %20 = tensor.dim %17, %c2_7 : tensor<1x?x?xf32>
  %c0_8 = arith.constant 0 : index
  %c1_9 = arith.constant 1 : index
  %c1_10 = arith.constant 1 : index
  %c2_11 = arith.constant 2 : index
  %21 = arith.addi %c1_10, %19 : index
  %22 = arith.cmpi slt, %c1_10, %c0_8 : index
  %23 = select %22, %21, %c1_10 : index
  %24 = arith.addi %c2_11, %19 : index
  %25 = arith.cmpi slt, %c2_11, %c0_8 : index
  %26 = select %25, %24, %c2_11 : index
  %27 = arith.subi %26, %23 : index
  %c1_12 = arith.constant 1 : index
  %28 = arith.addi %27, %c1_12 : index
  %29 = arith.subi %28, %c1_9 : index
  %30 = arith.ceildivsi %29, %c1_12 : index
  %31 = linalg.init_tensor [%18, %30, %20] : tensor<?x?x?xf32>
  %32 = arith.muli %c1_9, %c1_12 : index
  %33 = tensor.insert_slice %17 into %31[%c0_8, %c0_8, %c0_8] [%18, %30, %20] [%c1_9, %32, %c1_9] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %34 = tensor.cast %33 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_13 = arith.constant 0 : index
  %35 = tensor.dim %34, %c0_13 : tensor<1x1x?xf32>
  %c1_14 = arith.constant 1 : index
  %36 = tensor.dim %34, %c1_14 : tensor<1x1x?xf32>
  %c2_15 = arith.constant 2 : index
  %37 = tensor.dim %34, %c2_15 : tensor<1x1x?xf32>
  %c0_16 = arith.constant 0 : index
  %c1_17 = arith.constant 1 : index
  %c1_18 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %38 = arith.addi %c1_18, %37 : index
  %39 = arith.cmpi slt, %c1_18, %c0_16 : index
  %40 = select %39, %38, %c1_18 : index
  %41 = arith.addi %c3, %37 : index
  %42 = arith.cmpi slt, %c3, %c0_16 : index
  %43 = select %42, %41, %c3 : index
  %44 = arith.subi %43, %40 : index
  %c1_19 = arith.constant 1 : index
  %45 = arith.addi %44, %c1_19 : index
  %46 = arith.subi %45, %c1_17 : index
  %47 = arith.ceildivsi %46, %c1_19 : index
  %48 = linalg.init_tensor [%35, %36, %47] : tensor<?x?x?xf32>
  %49 = arith.muli %c1_17, %c1_19 : index
  %50 = tensor.insert_slice %34 into %48[%c0_16, %c0_16, %c0_16] [%35, %36, %47] [%c1_17, %c1_17, %49] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %51 = tensor.cast %50 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %52 = torch_c.from_builtin_tensor %51 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %52 : !torch.vtensor<[1,1,2],f32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x686cd80) {
  %3 = "torch.constant.int"() {value = 1 : i64} : () -> !torch.int

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertTorchConstantOp<mlir::torch::Torch::ConstantIntOp>"
    ** Insert  : 'arith.constant'(0x68edf60)
    ** Replace : 'torch.constant.int'(0x686cd80)
"(anonymous namespace)::ConvertTorchConstantOp<mlir::torch::Torch::ConstantIntOp>" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68edf60) {
      %3 = "arith.constant"() {value = 1 : i64} : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %c0_i64 = arith.constant 0 : i64
  %int0 = torch.constant.int 0
  %c1_i64 = arith.constant 1 : i64
  %int1 = torch.constant.int 1
  %int2 = torch.constant.int 2
  %int3 = torch.constant.int 3
  %c0 = arith.constant 0 : index
  %1 = tensor.dim %0, %c0 : tensor<?x?x?xf32>
  %c1 = arith.constant 1 : index
  %2 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %c2 = arith.constant 2 : index
  %3 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c1_3 = arith.constant 1 : index
  %4 = arith.addi %c0_2, %1 : index
  %5 = arith.cmpi slt, %c0_2, %c0_0 : index
  %6 = select %5, %4, %c0_2 : index
  %7 = arith.addi %c1_3, %1 : index
  %8 = arith.cmpi slt, %c1_3, %c0_0 : index
  %9 = select %8, %7, %c1_3 : index
  %10 = arith.subi %9, %6 : index
  %c1_4 = arith.constant 1 : index
  %11 = arith.addi %10, %c1_4 : index
  %12 = arith.subi %11, %c1_1 : index
  %13 = arith.ceildivsi %12, %c1_4 : index
  %14 = linalg.init_tensor [%13, %2, %3] : tensor<?x?x?xf32>
  %15 = arith.muli %c1_1, %c1_4 : index
  %16 = tensor.insert_slice %0 into %14[%c0_0, %c0_0, %c0_0] [%13, %2, %3] [%15, %c1_1, %c1_1] : tensor<?x?x?xf32> into tensor<?x?x?xf32>
  %17 = tensor.cast %16 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %c0_5 = arith.constant 0 : index
  %18 = tensor.dim %17, %c0_5 : tensor<1x?x?xf32>
  %c1_6 = arith.constant 1 : index
  %19 = tensor.dim %17, %c1_6 : tensor<1x?x?xf32>
  %c2_7 = arith.constant 2 : index
  %20 = tensor.dim %17, %c2_7 : tensor<1x?x?xf32>
  %c0_8 = arith.constant 0 : index
  %c1_9 = arith.constant 1 : index
  %c1_10 = arith.constant 1 : index
  %c2_11 = arith.constant 2 : index
  %21 = arith.addi %c1_10, %19 : index
  %22 = arith.cmpi slt, %c1_10, %c0_8 : index
  %23 = select %22, %21, %c1_10 : index
  %24 = arith.addi %c2_11, %19 : index
  %25 = arith.cmpi slt, %c2_11, %c0_8 : index
  %26 = select %25, %24, %c2_11 : index
  %27 = arith.subi %26, %23 : index
  %c1_12 = arith.constant 1 : index
  %28 = arith.addi %27, %c1_12 : index
  %29 = arith.subi %28, %c1_9 : index
  %30 = arith.ceildivsi %29, %c1_12 : index
  %31 = linalg.init_tensor [%18, %30, %20] : tensor<?x?x?xf32>
  %32 = arith.muli %c1_9, %c1_12 : index
  %33 = tensor.insert_slice %17 into %31[%c0_8, %c0_8, %c0_8] [%18, %30, %20] [%c1_9, %32, %c1_9] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %34 = tensor.cast %33 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_13 = arith.constant 0 : index
  %35 = tensor.dim %34, %c0_13 : tensor<1x1x?xf32>
  %c1_14 = arith.constant 1 : index
  %36 = tensor.dim %34, %c1_14 : tensor<1x1x?xf32>
  %c2_15 = arith.constant 2 : index
  %37 = tensor.dim %34, %c2_15 : tensor<1x1x?xf32>
  %c0_16 = arith.constant 0 : index
  %c1_17 = arith.constant 1 : index
  %c1_18 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %38 = arith.addi %c1_18, %37 : index
  %39 = arith.cmpi slt, %c1_18, %c0_16 : index
  %40 = select %39, %38, %c1_18 : index
  %41 = arith.addi %c3, %37 : index
  %42 = arith.cmpi slt, %c3, %c0_16 : index
  %43 = select %42, %41, %c3 : index
  %44 = arith.subi %43, %40 : index
  %c1_19 = arith.constant 1 : index
  %45 = arith.addi %44, %c1_19 : index
  %46 = arith.subi %45, %c1_17 : index
  %47 = arith.ceildivsi %46, %c1_19 : index
  %48 = linalg.init_tensor [%35, %36, %47] : tensor<?x?x?xf32>
  %49 = arith.muli %c1_17, %c1_19 : index
  %50 = tensor.insert_slice %34 into %48[%c0_16, %c0_16, %c0_16] [%35, %36, %47] [%c1_17, %c1_17, %49] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %51 = tensor.cast %50 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %52 = torch_c.from_builtin_tensor %51 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %52 : !torch.vtensor<[1,1,2],f32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x686b710) {
  %5 = "torch.constant.int"() {value = 2 : i64} : () -> !torch.int

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertTorchConstantOp<mlir::torch::Torch::ConstantIntOp>"
    ** Insert  : 'arith.constant'(0x68ee000)
    ** Replace : 'torch.constant.int'(0x686b710)
"(anonymous namespace)::ConvertTorchConstantOp<mlir::torch::Torch::ConstantIntOp>" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68ee000) {
      %5 = "arith.constant"() {value = 2 : i64} : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %c0_i64 = arith.constant 0 : i64
  %int0 = torch.constant.int 0
  %c1_i64 = arith.constant 1 : i64
  %int1 = torch.constant.int 1
  %c2_i64 = arith.constant 2 : i64
  %int2 = torch.constant.int 2
  %int3 = torch.constant.int 3
  %c0 = arith.constant 0 : index
  %1 = tensor.dim %0, %c0 : tensor<?x?x?xf32>
  %c1 = arith.constant 1 : index
  %2 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %c2 = arith.constant 2 : index
  %3 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c1_3 = arith.constant 1 : index
  %4 = arith.addi %c0_2, %1 : index
  %5 = arith.cmpi slt, %c0_2, %c0_0 : index
  %6 = select %5, %4, %c0_2 : index
  %7 = arith.addi %c1_3, %1 : index
  %8 = arith.cmpi slt, %c1_3, %c0_0 : index
  %9 = select %8, %7, %c1_3 : index
  %10 = arith.subi %9, %6 : index
  %c1_4 = arith.constant 1 : index
  %11 = arith.addi %10, %c1_4 : index
  %12 = arith.subi %11, %c1_1 : index
  %13 = arith.ceildivsi %12, %c1_4 : index
  %14 = linalg.init_tensor [%13, %2, %3] : tensor<?x?x?xf32>
  %15 = arith.muli %c1_1, %c1_4 : index
  %16 = tensor.insert_slice %0 into %14[%c0_0, %c0_0, %c0_0] [%13, %2, %3] [%15, %c1_1, %c1_1] : tensor<?x?x?xf32> into tensor<?x?x?xf32>
  %17 = tensor.cast %16 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %c0_5 = arith.constant 0 : index
  %18 = tensor.dim %17, %c0_5 : tensor<1x?x?xf32>
  %c1_6 = arith.constant 1 : index
  %19 = tensor.dim %17, %c1_6 : tensor<1x?x?xf32>
  %c2_7 = arith.constant 2 : index
  %20 = tensor.dim %17, %c2_7 : tensor<1x?x?xf32>
  %c0_8 = arith.constant 0 : index
  %c1_9 = arith.constant 1 : index
  %c1_10 = arith.constant 1 : index
  %c2_11 = arith.constant 2 : index
  %21 = arith.addi %c1_10, %19 : index
  %22 = arith.cmpi slt, %c1_10, %c0_8 : index
  %23 = select %22, %21, %c1_10 : index
  %24 = arith.addi %c2_11, %19 : index
  %25 = arith.cmpi slt, %c2_11, %c0_8 : index
  %26 = select %25, %24, %c2_11 : index
  %27 = arith.subi %26, %23 : index
  %c1_12 = arith.constant 1 : index
  %28 = arith.addi %27, %c1_12 : index
  %29 = arith.subi %28, %c1_9 : index
  %30 = arith.ceildivsi %29, %c1_12 : index
  %31 = linalg.init_tensor [%18, %30, %20] : tensor<?x?x?xf32>
  %32 = arith.muli %c1_9, %c1_12 : index
  %33 = tensor.insert_slice %17 into %31[%c0_8, %c0_8, %c0_8] [%18, %30, %20] [%c1_9, %32, %c1_9] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %34 = tensor.cast %33 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_13 = arith.constant 0 : index
  %35 = tensor.dim %34, %c0_13 : tensor<1x1x?xf32>
  %c1_14 = arith.constant 1 : index
  %36 = tensor.dim %34, %c1_14 : tensor<1x1x?xf32>
  %c2_15 = arith.constant 2 : index
  %37 = tensor.dim %34, %c2_15 : tensor<1x1x?xf32>
  %c0_16 = arith.constant 0 : index
  %c1_17 = arith.constant 1 : index
  %c1_18 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %38 = arith.addi %c1_18, %37 : index
  %39 = arith.cmpi slt, %c1_18, %c0_16 : index
  %40 = select %39, %38, %c1_18 : index
  %41 = arith.addi %c3, %37 : index
  %42 = arith.cmpi slt, %c3, %c0_16 : index
  %43 = select %42, %41, %c3 : index
  %44 = arith.subi %43, %40 : index
  %c1_19 = arith.constant 1 : index
  %45 = arith.addi %44, %c1_19 : index
  %46 = arith.subi %45, %c1_17 : index
  %47 = arith.ceildivsi %46, %c1_19 : index
  %48 = linalg.init_tensor [%35, %36, %47] : tensor<?x?x?xf32>
  %49 = arith.muli %c1_17, %c1_19 : index
  %50 = tensor.insert_slice %34 into %48[%c0_16, %c0_16, %c0_16] [%35, %36, %47] [%c1_17, %c1_17, %49] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %51 = tensor.cast %50 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %52 = torch_c.from_builtin_tensor %51 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %52 : !torch.vtensor<[1,1,2],f32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x68a99f0) {
  %7 = "torch.constant.int"() {value = 3 : i64} : () -> !torch.int

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertTorchConstantOp<mlir::torch::Torch::ConstantIntOp>"
    ** Insert  : 'arith.constant'(0x68ec460)
    ** Replace : 'torch.constant.int'(0x68a99f0)
"(anonymous namespace)::ConvertTorchConstantOp<mlir::torch::Torch::ConstantIntOp>" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68ec460) {
      %7 = "arith.constant"() {value = 3 : i64} : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %c0_i64 = arith.constant 0 : i64
  %int0 = torch.constant.int 0
  %c1_i64 = arith.constant 1 : i64
  %int1 = torch.constant.int 1
  %c2_i64 = arith.constant 2 : i64
  %int2 = torch.constant.int 2
  %c3_i64 = arith.constant 3 : i64
  %int3 = torch.constant.int 3
  %c0 = arith.constant 0 : index
  %1 = tensor.dim %0, %c0 : tensor<?x?x?xf32>
  %c1 = arith.constant 1 : index
  %2 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %c2 = arith.constant 2 : index
  %3 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c1_3 = arith.constant 1 : index
  %4 = arith.addi %c0_2, %1 : index
  %5 = arith.cmpi slt, %c0_2, %c0_0 : index
  %6 = select %5, %4, %c0_2 : index
  %7 = arith.addi %c1_3, %1 : index
  %8 = arith.cmpi slt, %c1_3, %c0_0 : index
  %9 = select %8, %7, %c1_3 : index
  %10 = arith.subi %9, %6 : index
  %c1_4 = arith.constant 1 : index
  %11 = arith.addi %10, %c1_4 : index
  %12 = arith.subi %11, %c1_1 : index
  %13 = arith.ceildivsi %12, %c1_4 : index
  %14 = linalg.init_tensor [%13, %2, %3] : tensor<?x?x?xf32>
  %15 = arith.muli %c1_1, %c1_4 : index
  %16 = tensor.insert_slice %0 into %14[%c0_0, %c0_0, %c0_0] [%13, %2, %3] [%15, %c1_1, %c1_1] : tensor<?x?x?xf32> into tensor<?x?x?xf32>
  %17 = tensor.cast %16 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %c0_5 = arith.constant 0 : index
  %18 = tensor.dim %17, %c0_5 : tensor<1x?x?xf32>
  %c1_6 = arith.constant 1 : index
  %19 = tensor.dim %17, %c1_6 : tensor<1x?x?xf32>
  %c2_7 = arith.constant 2 : index
  %20 = tensor.dim %17, %c2_7 : tensor<1x?x?xf32>
  %c0_8 = arith.constant 0 : index
  %c1_9 = arith.constant 1 : index
  %c1_10 = arith.constant 1 : index
  %c2_11 = arith.constant 2 : index
  %21 = arith.addi %c1_10, %19 : index
  %22 = arith.cmpi slt, %c1_10, %c0_8 : index
  %23 = select %22, %21, %c1_10 : index
  %24 = arith.addi %c2_11, %19 : index
  %25 = arith.cmpi slt, %c2_11, %c0_8 : index
  %26 = select %25, %24, %c2_11 : index
  %27 = arith.subi %26, %23 : index
  %c1_12 = arith.constant 1 : index
  %28 = arith.addi %27, %c1_12 : index
  %29 = arith.subi %28, %c1_9 : index
  %30 = arith.ceildivsi %29, %c1_12 : index
  %31 = linalg.init_tensor [%18, %30, %20] : tensor<?x?x?xf32>
  %32 = arith.muli %c1_9, %c1_12 : index
  %33 = tensor.insert_slice %17 into %31[%c0_8, %c0_8, %c0_8] [%18, %30, %20] [%c1_9, %32, %c1_9] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %34 = tensor.cast %33 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_13 = arith.constant 0 : index
  %35 = tensor.dim %34, %c0_13 : tensor<1x1x?xf32>
  %c1_14 = arith.constant 1 : index
  %36 = tensor.dim %34, %c1_14 : tensor<1x1x?xf32>
  %c2_15 = arith.constant 2 : index
  %37 = tensor.dim %34, %c2_15 : tensor<1x1x?xf32>
  %c0_16 = arith.constant 0 : index
  %c1_17 = arith.constant 1 : index
  %c1_18 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %38 = arith.addi %c1_18, %37 : index
  %39 = arith.cmpi slt, %c1_18, %c0_16 : index
  %40 = select %39, %38, %c1_18 : index
  %41 = arith.addi %c3, %37 : index
  %42 = arith.cmpi slt, %c3, %c0_16 : index
  %43 = select %42, %41, %c3 : index
  %44 = arith.subi %43, %40 : index
  %c1_19 = arith.constant 1 : index
  %45 = arith.addi %44, %c1_19 : index
  %46 = arith.subi %45, %c1_17 : index
  %47 = arith.ceildivsi %46, %c1_19 : index
  %48 = linalg.init_tensor [%35, %36, %47] : tensor<?x?x?xf32>
  %49 = arith.muli %c1_17, %c1_19 : index
  %50 = tensor.insert_slice %34 into %48[%c0_16, %c0_16, %c0_16] [%35, %36, %47] [%c1_17, %c1_17, %49] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %51 = tensor.cast %50 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %52 = torch_c.from_builtin_tensor %51 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %52 : !torch.vtensor<[1,1,2],f32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68ebc70) {
  %9 = "arith.constant"() {value = 0 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f1f10) {
  %10 = "tensor.dim"(%0, %9) : (tensor<?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f49a0) {
  %11 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f4a00) {
  %12 = "tensor.dim"(%0, %11) : (tensor<?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f5fa0) {
  %13 = "arith.constant"() {value = 2 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f6000) {
  %14 = "tensor.dim"(%0, %13) : (tensor<?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f60b0) {
  %15 = "arith.constant"() {value = 0 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f6180) {
  %16 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f61e0) {
  %17 = "arith.constant"() {value = 0 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f6240) {
  %18 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68f62a0) {
  %19 = "arith.addi"(%17, %10) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x68f6350) {
  %20 = "arith.cmpi"(%17, %15) {predicate = 2 : i64} : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x68f6400) {
  %21 = "std.select"(%20, %19, %17) : (i1, index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68f64d0) {
  %22 = "arith.addi"(%18, %10) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x68f6660) {
  %23 = "arith.cmpi"(%18, %15) {predicate = 2 : i64} : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x68f6710) {
  %24 = "std.select"(%23, %22, %18) : (i1, index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x68f67e0) {
  %25 = "arith.subi"(%24, %21) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f6890) {
  %26 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68f68f0) {
  %27 = "arith.addi"(%25, %26) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x68f69a0) {
  %28 = "arith.subi"(%27, %16) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.ceildivsi'(0x68f6a50) {
  %29 = "arith.ceildivsi"(%28, %26) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.init_tensor'(0x68f6b00) {
  %30 = "linalg.init_tensor"(%29, %12, %14) {static_sizes = [-1, -1, -1]} : (index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.muli'(0x68f6bd0) {
  %31 = "arith.muli"(%16, %26) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.insert_slice'(0x68fb490) {
  %32 = "tensor.insert_slice"(%0, %30, %15, %15, %15, %29, %12, %14, %31, %16, %16) {operand_segment_sizes = dense<[1, 1, 3, 3, 3]> : vector<5xi32>, static_offsets = [-9223372036854775808, -9223372036854775808, -9223372036854775808], static_sizes = [-1, -1, -1], static_strides = [-9223372036854775808, -9223372036854775808, -9223372036854775808]} : (tensor<?x?x?xf32>, tensor<?x?x?xf32>, index, index, index, index, index, index, index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x68af140) {
  %33 = "tensor.cast"(%32) : (tensor<?x?x?xf32>) -> tensor<1x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fba70) {
  %34 = "arith.constant"() {value = 0 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68fbfb0) {
  %35 = "tensor.dim"(%33, %34) : (tensor<1x?x?xf32>, index) -> index

  * Fold {
    ** Insert  : 'arith.constant'(0x68ebdf0)
    ** Replace : 'tensor.dim'(0x68fbfb0)

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68ebdf0) {
      %35 = "arith.constant"() {value = 1 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS
} -> SUCCESS : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68ef190) {
  %37 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f6580) {
  %38 = "tensor.dim"(%33, %37) : (tensor<1x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc220) {
  %39 = "arith.constant"() {value = 2 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68fc280) {
  %40 = "tensor.dim"(%33, %39) : (tensor<1x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc330) {
  %41 = "arith.constant"() {value = 0 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc390) {
  %42 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc3f0) {
  %43 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc450) {
  %44 = "arith.constant"() {value = 2 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68fc4b0) {
  %45 = "arith.addi"(%43, %38) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x68fc560) {
  %46 = "arith.cmpi"(%43, %41) {predicate = 2 : i64} : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x68fc610) {
  %47 = "std.select"(%46, %45, %43) : (i1, index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68fc6e0) {
  %48 = "arith.addi"(%44, %38) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x68fc790) {
  %49 = "arith.cmpi"(%44, %41) {predicate = 2 : i64} : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x68fc840) {
  %50 = "std.select"(%49, %48, %44) : (i1, index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x68fc910) {
  %51 = "arith.subi"(%50, %47) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc9c0) {
  %52 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68fca20) {
  %53 = "arith.addi"(%51, %52) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x68fe310) {
  %54 = "arith.subi"(%53, %42) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.ceildivsi'(0x68fe3c0) {
  %55 = "arith.ceildivsi"(%54, %52) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.init_tensor'(0x68fe470) {
  %56 = "linalg.init_tensor"(%36, %55, %40) {static_sizes = [-1, -1, -1]} : (index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.muli'(0x68fe540) {
  %57 = "arith.muli"(%42, %52) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.insert_slice'(0x68fe690) {
  %58 = "tensor.insert_slice"(%33, %56, %41, %41, %41, %36, %55, %40, %42, %57, %42) {operand_segment_sizes = dense<[1, 1, 3, 3, 3]> : vector<5xi32>, static_offsets = [-9223372036854775808, -9223372036854775808, -9223372036854775808], static_sizes = [-1, -1, -1], static_strides = [-9223372036854775808, -9223372036854775808, -9223372036854775808]} : (tensor<1x?x?xf32>, tensor<?x?x?xf32>, index, index, index, index, index, index, index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x68fe860) {
  %59 = "tensor.cast"(%58) : (tensor<?x?x?xf32>) -> tensor<1x1x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fe8f0) {
  %60 = "arith.constant"() {value = 0 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68fe950) {
  %61 = "tensor.dim"(%59, %60) : (tensor<1x1x?xf32>, index) -> index

  * Fold {
    ** Insert  : 'arith.constant'(0x68eea90)
    ** Replace : 'tensor.dim'(0x68fe950)

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68eea90) {
      %61 = "arith.constant"() {value = 1 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS
} -> SUCCESS : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fea00) {
  %63 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68fea60) {
  %64 = "tensor.dim"(%59, %63) : (tensor<1x1x?xf32>, index) -> index

  * Fold {
    ** Insert  : 'arith.constant'(0x68eeaf0)
    ** Replace : 'tensor.dim'(0x68fea60)

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68eeaf0) {
      %64 = "arith.constant"() {value = 1 : index} : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS
} -> SUCCESS : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68feb10) {
  %66 = "arith.constant"() {value = 2 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68feb70) {
  %67 = "tensor.dim"(%59, %66) : (tensor<1x1x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x6901950) {
  %68 = "arith.constant"() {value = 0 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x69019b0) {
  %69 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x6901a10) {
  %70 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x6901a70) {
  %71 = "arith.constant"() {value = 3 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x6901ad0) {
  %72 = "arith.addi"(%70, %67) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x6901b80) {
  %73 = "arith.cmpi"(%70, %68) {predicate = 2 : i64} : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x6901c30) {
  %74 = "std.select"(%73, %72, %70) : (i1, index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x6901d00) {
  %75 = "arith.addi"(%71, %67) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x6901db0) {
  %76 = "arith.cmpi"(%71, %68) {predicate = 2 : i64} : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x6901e60) {
  %77 = "std.select"(%76, %75, %71) : (i1, index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x6901f30) {
  %78 = "arith.subi"(%77, %74) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x6901fe0) {
  %79 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x6902040) {
  %80 = "arith.addi"(%78, %79) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x69001e0) {
  %81 = "arith.subi"(%80, %69) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.ceildivsi'(0x6900290) {
  %82 = "arith.ceildivsi"(%81, %79) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.init_tensor'(0x6900340) {
  %83 = "linalg.init_tensor"(%62, %65, %82) {static_sizes = [-1, -1, -1]} : (index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.muli'(0x6900410) {
  %84 = "arith.muli"(%69, %79) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.insert_slice'(0x6900560) {
  %85 = "tensor.insert_slice"(%59, %83, %68, %68, %68, %62, %65, %82, %69, %69, %84) {operand_segment_sizes = dense<[1, 1, 3, 3, 3]> : vector<5xi32>, static_offsets = [-9223372036854775808, -9223372036854775808, -9223372036854775808], static_sizes = [-1, -1, -1], static_strides = [-9223372036854775808, -9223372036854775808, -9223372036854775808]} : (tensor<1x1x?xf32>, tensor<?x?x?xf32>, index, index, index, index, index, index, index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x6900730) {
  %86 = "tensor.cast"(%85) : (tensor<?x?x?xf32>) -> tensor<1x1x2xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x69008b0) {
  %87 = "torch_c.from_builtin_tensor"(%86) : (tensor<1x1x2xf32>) -> !torch.vtensor<[1,1,2],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.return'(0x68a5110) {
  "std.return"(%87) : (!torch.vtensor<[1,1,2],f32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After ConvertTorchToStd //----- //
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %c0_i64 = arith.constant 0 : i64
  %c1_i64 = arith.constant 1 : i64
  %c2_i64 = arith.constant 2 : i64
  %c3_i64 = arith.constant 3 : i64
  %c0 = arith.constant 0 : index
  %1 = tensor.dim %0, %c0 : tensor<?x?x?xf32>
  %c1 = arith.constant 1 : index
  %2 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %c2 = arith.constant 2 : index
  %3 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c1_3 = arith.constant 1 : index
  %4 = arith.addi %c0_2, %1 : index
  %5 = arith.cmpi slt, %c0_2, %c0_0 : index
  %6 = select %5, %4, %c0_2 : index
  %7 = arith.addi %c1_3, %1 : index
  %8 = arith.cmpi slt, %c1_3, %c0_0 : index
  %9 = select %8, %7, %c1_3 : index
  %10 = arith.subi %9, %6 : index
  %c1_4 = arith.constant 1 : index
  %11 = arith.addi %10, %c1_4 : index
  %12 = arith.subi %11, %c1_1 : index
  %13 = arith.ceildivsi %12, %c1_4 : index
  %14 = linalg.init_tensor [%13, %2, %3] : tensor<?x?x?xf32>
  %15 = arith.muli %c1_1, %c1_4 : index
  %16 = tensor.insert_slice %0 into %14[%c0_0, %c0_0, %c0_0] [%13, %2, %3] [%15, %c1_1, %c1_1] : tensor<?x?x?xf32> into tensor<?x?x?xf32>
  %17 = tensor.cast %16 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %c0_5 = arith.constant 0 : index
  %c1_6 = arith.constant 1 : index
  %c1_7 = arith.constant 1 : index
  %18 = tensor.dim %17, %c1_7 : tensor<1x?x?xf32>
  %c2_8 = arith.constant 2 : index
  %19 = tensor.dim %17, %c2_8 : tensor<1x?x?xf32>
  %c0_9 = arith.constant 0 : index
  %c1_10 = arith.constant 1 : index
  %c1_11 = arith.constant 1 : index
  %c2_12 = arith.constant 2 : index
  %20 = arith.addi %c1_11, %18 : index
  %21 = arith.cmpi slt, %c1_11, %c0_9 : index
  %22 = select %21, %20, %c1_11 : index
  %23 = arith.addi %c2_12, %18 : index
  %24 = arith.cmpi slt, %c2_12, %c0_9 : index
  %25 = select %24, %23, %c2_12 : index
  %26 = arith.subi %25, %22 : index
  %c1_13 = arith.constant 1 : index
  %27 = arith.addi %26, %c1_13 : index
  %28 = arith.subi %27, %c1_10 : index
  %29 = arith.ceildivsi %28, %c1_13 : index
  %30 = linalg.init_tensor [%c1_6, %29, %19] : tensor<?x?x?xf32>
  %31 = arith.muli %c1_10, %c1_13 : index
  %32 = tensor.insert_slice %17 into %30[%c0_9, %c0_9, %c0_9] [%c1_6, %29, %19] [%c1_10, %31, %c1_10] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %33 = tensor.cast %32 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_14 = arith.constant 0 : index
  %c1_15 = arith.constant 1 : index
  %c1_16 = arith.constant 1 : index
  %c1_17 = arith.constant 1 : index
  %c2_18 = arith.constant 2 : index
  %34 = tensor.dim %33, %c2_18 : tensor<1x1x?xf32>
  %c0_19 = arith.constant 0 : index
  %c1_20 = arith.constant 1 : index
  %c1_21 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %35 = arith.addi %c1_21, %34 : index
  %36 = arith.cmpi slt, %c1_21, %c0_19 : index
  %37 = select %36, %35, %c1_21 : index
  %38 = arith.addi %c3, %34 : index
  %39 = arith.cmpi slt, %c3, %c0_19 : index
  %40 = select %39, %38, %c3 : index
  %41 = arith.subi %40, %37 : index
  %c1_22 = arith.constant 1 : index
  %42 = arith.addi %41, %c1_22 : index
  %43 = arith.subi %42, %c1_20 : index
  %44 = arith.ceildivsi %43, %c1_22 : index
  %45 = linalg.init_tensor [%c1_15, %c1_17, %44] : tensor<?x?x?xf32>
  %46 = arith.muli %c1_20, %c1_22 : index
  %47 = tensor.insert_slice %33 into %45[%c0_19, %c0_19, %c0_19] [%c1_15, %c1_17, %44] [%c1_20, %c1_20, %46] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %48 = tensor.cast %47 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %49 = torch_c.from_builtin_tensor %48 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %49 : !torch.vtensor<[1,1,2],f32>
}


//===-------------------------------------------===//
Legalizing operation : 'builtin.func'(0x686d8b0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x6900820) {
  %0 = "torch_c.to_builtin_tensor"(%arg0) : (!torch.vtensor<[?,?,?],f32>) -> tensor<?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x69007c0) {
  %1 = "arith.constant"() {value = 0 : i64} : () -> i64

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68edf60) {
  %2 = "arith.constant"() {value = 1 : i64} : () -> i64

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68ee000) {
  %3 = "arith.constant"() {value = 2 : i64} : () -> i64

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68ec460) {
  %4 = "arith.constant"() {value = 3 : i64} : () -> i64

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68ebc70) {
  %5 = "arith.constant"() {value = 0 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f1f10) {
  %6 = "tensor.dim"(%0, %5) : (tensor<?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f49a0) {
  %7 = "arith.constant"() {value = 1 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f4a00) {
  %8 = "tensor.dim"(%0, %7) : (tensor<?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f5fa0) {
  %9 = "arith.constant"() {value = 2 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f6000) {
  %10 = "tensor.dim"(%0, %9) : (tensor<?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f60b0) {
  %11 = "arith.constant"() {value = 0 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f6180) {
  %12 = "arith.constant"() {value = 1 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f61e0) {
  %13 = "arith.constant"() {value = 0 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f6240) {
  %14 = "arith.constant"() {value = 1 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68f62a0) {
  %15 = "arith.addi"(%13, %6) : (index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x68f6350) {
  %16 = "arith.cmpi"(%13, %11) {predicate = 2 : i64} : (index, index) -> i1

  * Fold {
    ** Insert  : 'arith.constant'(0x68d8350)
    ** Replace : 'arith.cmpi'(0x68f6350)

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68d8350) {
      %16 = "arith.constant"() {value = false} : () -> i1

      * Fold {
      } -> FAILURE : unable to fold
    } -> FAILURE : no matched legalization pattern
    //===-------------------------------------------===//
  } -> FAILURE : generated constant 'arith.constant' was illegal
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x68f6400) {
  %17 = "std.select"(%16, %15, %13) : (i1, index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68f64d0) {
  %18 = "arith.addi"(%14, %6) : (index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x68f6660) {
  %19 = "arith.cmpi"(%14, %11) {predicate = 2 : i64} : (index, index) -> i1

  * Fold {
    ** Insert  : 'arith.constant'(0x68d8350)
    ** Replace : 'arith.cmpi'(0x68f6660)

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68d8350) {
      %19 = "arith.constant"() {value = false} : () -> i1

      * Fold {
      } -> FAILURE : unable to fold
    } -> FAILURE : no matched legalization pattern
    //===-------------------------------------------===//
  } -> FAILURE : generated constant 'arith.constant' was illegal
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x68f6710) {
  %20 = "std.select"(%19, %18, %14) : (i1, index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x68f67e0) {
  %21 = "arith.subi"(%20, %17) : (index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f6890) {
  %22 = "arith.constant"() {value = 1 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68f68f0) {
  %23 = "arith.addi"(%21, %22) : (index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x68f69a0) {
  %24 = "arith.subi"(%23, %12) : (index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.ceildivsi'(0x68f6a50) {
  %25 = "arith.ceildivsi"(%24, %22) : (index, index) -> index

  * Fold {
    ** Replace : 'arith.ceildivsi'(0x68f6a50)
  } -> SUCCESS
} -> SUCCESS : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.init_tensor'(0x68f6b00) {
  %26 = "linalg.init_tensor"(%25, %8, %10) {static_sizes = [-1, -1, -1]} : (index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.muli'(0x68f6bd0) {
  %27 = "arith.muli"(%12, %22) : (index, index) -> index

  * Fold {
    ** Replace : 'arith.muli'(0x68f6bd0)
  } -> SUCCESS
} -> SUCCESS : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.insert_slice'(0x68fb490) {
  %28 = "tensor.insert_slice"(%0, %26, %11, %11, %11, %25, %8, %10, %27, %12, %12) {operand_segment_sizes = dense<[1, 1, 3, 3, 3]> : vector<5xi32>, static_offsets = [-9223372036854775808, -9223372036854775808, -9223372036854775808], static_sizes = [-1, -1, -1], static_strides = [-9223372036854775808, -9223372036854775808, -9223372036854775808]} : (tensor<?x?x?xf32>, tensor<?x?x?xf32>, index, index, index, index, index, index, index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x68af140) {
  %29 = "tensor.cast"(%28) : (tensor<?x?x?xf32>) -> tensor<1x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fba70) {
  %30 = "arith.constant"() {value = 0 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68ebdf0) {
  %31 = "arith.constant"() {value = 1 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68ef190) {
  %32 = "arith.constant"() {value = 1 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f6580) {
  %33 = "tensor.dim"(%29, %32) : (tensor<1x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc220) {
  %34 = "arith.constant"() {value = 2 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68fc280) {
  %35 = "tensor.dim"(%29, %34) : (tensor<1x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc330) {
  %36 = "arith.constant"() {value = 0 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc390) {
  %37 = "arith.constant"() {value = 1 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc3f0) {
  %38 = "arith.constant"() {value = 1 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc450) {
  %39 = "arith.constant"() {value = 2 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68fc4b0) {
  %40 = "arith.addi"(%38, %33) : (index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x68fc560) {
  %41 = "arith.cmpi"(%38, %36) {predicate = 2 : i64} : (index, index) -> i1

  * Fold {
    ** Insert  : 'arith.constant'(0x68d8350)
    ** Replace : 'arith.cmpi'(0x68fc560)

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68d8350) {
      %41 = "arith.constant"() {value = false} : () -> i1

      * Fold {
      } -> FAILURE : unable to fold
    } -> FAILURE : no matched legalization pattern
    //===-------------------------------------------===//
  } -> FAILURE : generated constant 'arith.constant' was illegal
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x68fc610) {
  %42 = "std.select"(%41, %40, %38) : (i1, index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68fc6e0) {
  %43 = "arith.addi"(%39, %33) : (index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x68fc790) {
  %44 = "arith.cmpi"(%39, %36) {predicate = 2 : i64} : (index, index) -> i1

  * Fold {
    ** Insert  : 'arith.constant'(0x68d8350)
    ** Replace : 'arith.cmpi'(0x68fc790)

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68d8350) {
      %44 = "arith.constant"() {value = false} : () -> i1

      * Fold {
      } -> FAILURE : unable to fold
    } -> FAILURE : no matched legalization pattern
    //===-------------------------------------------===//
  } -> FAILURE : generated constant 'arith.constant' was illegal
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x68fc840) {
  %45 = "std.select"(%44, %43, %39) : (i1, index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x68fc910) {
  %46 = "arith.subi"(%45, %42) : (index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc9c0) {
  %47 = "arith.constant"() {value = 1 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68fca20) {
  %48 = "arith.addi"(%46, %47) : (index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x68fe310) {
  %49 = "arith.subi"(%48, %37) : (index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.ceildivsi'(0x68fe3c0) {
  %50 = "arith.ceildivsi"(%49, %47) : (index, index) -> index

  * Fold {
    ** Replace : 'arith.ceildivsi'(0x68fe3c0)
  } -> SUCCESS
} -> SUCCESS : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.init_tensor'(0x68fe470) {
  %51 = "linalg.init_tensor"(%31, %50, %35) {static_sizes = [-1, -1, -1]} : (index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.muli'(0x68fe540) {
  %52 = "arith.muli"(%37, %47) : (index, index) -> index

  * Fold {
    ** Replace : 'arith.muli'(0x68fe540)
  } -> SUCCESS
} -> SUCCESS : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.insert_slice'(0x68fe690) {
  %53 = "tensor.insert_slice"(%29, %51, %36, %36, %36, %31, %50, %35, %37, %52, %37) {operand_segment_sizes = dense<[1, 1, 3, 3, 3]> : vector<5xi32>, static_offsets = [-9223372036854775808, -9223372036854775808, -9223372036854775808], static_sizes = [-1, -1, -1], static_strides = [-9223372036854775808, -9223372036854775808, -9223372036854775808]} : (tensor<1x?x?xf32>, tensor<?x?x?xf32>, index, index, index, index, index, index, index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x68fe860) {
  %54 = "tensor.cast"(%53) : (tensor<?x?x?xf32>) -> tensor<1x1x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fe8f0) {
  %55 = "arith.constant"() {value = 0 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68eea90) {
  %56 = "arith.constant"() {value = 1 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fea00) {
  %57 = "arith.constant"() {value = 1 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68eeaf0) {
  %58 = "arith.constant"() {value = 1 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68feb10) {
  %59 = "arith.constant"() {value = 2 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68feb70) {
  %60 = "tensor.dim"(%54, %59) : (tensor<1x1x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x6901950) {
  %61 = "arith.constant"() {value = 0 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x69019b0) {
  %62 = "arith.constant"() {value = 1 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x6901a10) {
  %63 = "arith.constant"() {value = 1 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x6901a70) {
  %64 = "arith.constant"() {value = 3 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x6901ad0) {
  %65 = "arith.addi"(%63, %60) : (index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x6901b80) {
  %66 = "arith.cmpi"(%63, %61) {predicate = 2 : i64} : (index, index) -> i1

  * Fold {
    ** Insert  : 'arith.constant'(0x68d8350)
    ** Replace : 'arith.cmpi'(0x6901b80)

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68d8350) {
      %66 = "arith.constant"() {value = false} : () -> i1

      * Fold {
      } -> FAILURE : unable to fold
    } -> FAILURE : no matched legalization pattern
    //===-------------------------------------------===//
  } -> FAILURE : generated constant 'arith.constant' was illegal
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x6901c30) {
  %67 = "std.select"(%66, %65, %63) : (i1, index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x6901d00) {
  %68 = "arith.addi"(%64, %60) : (index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x6901db0) {
  %69 = "arith.cmpi"(%64, %61) {predicate = 2 : i64} : (index, index) -> i1

  * Fold {
    ** Insert  : 'arith.constant'(0x68d8350)
    ** Replace : 'arith.cmpi'(0x6901db0)

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x68d8350) {
      %69 = "arith.constant"() {value = false} : () -> i1

      * Fold {
      } -> FAILURE : unable to fold
    } -> FAILURE : no matched legalization pattern
    //===-------------------------------------------===//
  } -> FAILURE : generated constant 'arith.constant' was illegal
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x6901e60) {
  %70 = "std.select"(%69, %68, %64) : (i1, index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x6901f30) {
  %71 = "arith.subi"(%70, %67) : (index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x6901fe0) {
  %72 = "arith.constant"() {value = 1 : index} : () -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x6902040) {
  %73 = "arith.addi"(%71, %72) : (index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x69001e0) {
  %74 = "arith.subi"(%73, %62) : (index, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.ceildivsi'(0x6900290) {
  %75 = "arith.ceildivsi"(%74, %72) : (index, index) -> index

  * Fold {
    ** Replace : 'arith.ceildivsi'(0x6900290)
  } -> SUCCESS
} -> SUCCESS : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.init_tensor'(0x6900340) {
  %76 = "linalg.init_tensor"(%56, %58, %75) {static_sizes = [-1, -1, -1]} : (index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.muli'(0x6900410) {
  %77 = "arith.muli"(%62, %72) : (index, index) -> index

  * Fold {
    ** Replace : 'arith.muli'(0x6900410)
  } -> SUCCESS
} -> SUCCESS : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.insert_slice'(0x6900560) {
  %78 = "tensor.insert_slice"(%54, %76, %61, %61, %61, %56, %58, %75, %62, %62, %77) {operand_segment_sizes = dense<[1, 1, 3, 3, 3]> : vector<5xi32>, static_offsets = [-9223372036854775808, -9223372036854775808, -9223372036854775808], static_sizes = [-1, -1, -1], static_strides = [-9223372036854775808, -9223372036854775808, -9223372036854775808]} : (tensor<1x1x?xf32>, tensor<?x?x?xf32>, index, index, index, index, index, index, index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x6900730) {
  %79 = "tensor.cast"(%78) : (tensor<?x?x?xf32>) -> tensor<1x1x2xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x69008b0) {
  %80 = "torch_c.from_builtin_tensor"(%79) : (tensor<1x1x2xf32>) -> !torch.vtensor<[1,1,2],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.return'(0x68a5110) {
  "std.return"(%80) : (!torch.vtensor<[1,1,2],f32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//
// -----// IR Dump After ConvertTorchToSCF //----- //
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %c0_i64 = arith.constant 0 : i64
  %c1_i64 = arith.constant 1 : i64
  %c2_i64 = arith.constant 2 : i64
  %c3_i64 = arith.constant 3 : i64
  %c0 = arith.constant 0 : index
  %1 = tensor.dim %0, %c0 : tensor<?x?x?xf32>
  %c1 = arith.constant 1 : index
  %2 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %c2 = arith.constant 2 : index
  %3 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c1_3 = arith.constant 1 : index
  %4 = arith.addi %c0_2, %1 : index
  %5 = arith.cmpi slt, %c0_2, %c0_0 : index
  %6 = select %5, %4, %c0_2 : index
  %7 = arith.addi %c1_3, %1 : index
  %8 = arith.cmpi slt, %c1_3, %c0_0 : index
  %9 = select %8, %7, %c1_3 : index
  %10 = arith.subi %9, %6 : index
  %c1_4 = arith.constant 1 : index
  %11 = arith.addi %10, %c1_4 : index
  %12 = arith.subi %11, %c1_1 : index
  %13 = linalg.init_tensor [%12, %2, %3] : tensor<?x?x?xf32>
  %14 = tensor.insert_slice %0 into %13[%c0_0, %c0_0, %c0_0] [%12, %2, %3] [%c1_1, %c1_1, %c1_1] : tensor<?x?x?xf32> into tensor<?x?x?xf32>
  %15 = tensor.cast %14 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %c0_5 = arith.constant 0 : index
  %c1_6 = arith.constant 1 : index
  %c1_7 = arith.constant 1 : index
  %16 = tensor.dim %15, %c1_7 : tensor<1x?x?xf32>
  %c2_8 = arith.constant 2 : index
  %17 = tensor.dim %15, %c2_8 : tensor<1x?x?xf32>
  %c0_9 = arith.constant 0 : index
  %c1_10 = arith.constant 1 : index
  %c1_11 = arith.constant 1 : index
  %c2_12 = arith.constant 2 : index
  %18 = arith.addi %c1_11, %16 : index
  %19 = arith.cmpi slt, %c1_11, %c0_9 : index
  %20 = select %19, %18, %c1_11 : index
  %21 = arith.addi %c2_12, %16 : index
  %22 = arith.cmpi slt, %c2_12, %c0_9 : index
  %23 = select %22, %21, %c2_12 : index
  %24 = arith.subi %23, %20 : index
  %c1_13 = arith.constant 1 : index
  %25 = arith.addi %24, %c1_13 : index
  %26 = arith.subi %25, %c1_10 : index
  %27 = linalg.init_tensor [%c1_6, %26, %17] : tensor<?x?x?xf32>
  %28 = tensor.insert_slice %15 into %27[%c0_9, %c0_9, %c0_9] [%c1_6, %26, %17] [%c1_10, %c1_10, %c1_10] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %29 = tensor.cast %28 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_14 = arith.constant 0 : index
  %c1_15 = arith.constant 1 : index
  %c1_16 = arith.constant 1 : index
  %c1_17 = arith.constant 1 : index
  %c2_18 = arith.constant 2 : index
  %30 = tensor.dim %29, %c2_18 : tensor<1x1x?xf32>
  %c0_19 = arith.constant 0 : index
  %c1_20 = arith.constant 1 : index
  %c1_21 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %31 = arith.addi %c1_21, %30 : index
  %32 = arith.cmpi slt, %c1_21, %c0_19 : index
  %33 = select %32, %31, %c1_21 : index
  %34 = arith.addi %c3, %30 : index
  %35 = arith.cmpi slt, %c3, %c0_19 : index
  %36 = select %35, %34, %c3 : index
  %37 = arith.subi %36, %33 : index
  %c1_22 = arith.constant 1 : index
  %38 = arith.addi %37, %c1_22 : index
  %39 = arith.subi %38, %c1_20 : index
  %40 = linalg.init_tensor [%c1_15, %c1_17, %39] : tensor<?x?x?xf32>
  %41 = tensor.insert_slice %29 into %40[%c0_19, %c0_19, %c0_19] [%c1_15, %c1_17, %39] [%c1_20, %c1_20, %c1_20] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %42 = tensor.cast %41 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %43 = torch_c.from_builtin_tensor %42 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %43 : !torch.vtensor<[1,1,2],f32>
}


//===-------------------------------------------===//
Legalizing operation : 'builtin.func'(0x686d8b0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x6900820) {
  %0 = "torch_c.to_builtin_tensor"(%arg0) : (!torch.vtensor<[?,?,?],f32>) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x69007c0) {
  %1 = "arith.constant"() {value = 0 : i64} : () -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68edf60) {
  %2 = "arith.constant"() {value = 1 : i64} : () -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68ee000) {
  %3 = "arith.constant"() {value = 2 : i64} : () -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68ec460) {
  %4 = "arith.constant"() {value = 3 : i64} : () -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68ebc70) {
  %5 = "arith.constant"() {value = 0 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f1f10) {
  %6 = "tensor.dim"(%0, %5) : (tensor<?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f49a0) {
  %7 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f4a00) {
  %8 = "tensor.dim"(%0, %7) : (tensor<?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f5fa0) {
  %9 = "arith.constant"() {value = 2 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f6000) {
  %10 = "tensor.dim"(%0, %9) : (tensor<?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f60b0) {
  %11 = "arith.constant"() {value = 0 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f6180) {
  %12 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f61e0) {
  %13 = "arith.constant"() {value = 0 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f6240) {
  %14 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68f62a0) {
  %15 = "arith.addi"(%13, %6) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x68f6350) {
  %16 = "arith.cmpi"(%13, %11) {predicate = 2 : i64} : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x68f6400) {
  %17 = "std.select"(%16, %15, %13) : (i1, index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68f64d0) {
  %18 = "arith.addi"(%14, %6) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x68f6660) {
  %19 = "arith.cmpi"(%14, %11) {predicate = 2 : i64} : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x68f6710) {
  %20 = "std.select"(%19, %18, %14) : (i1, index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x68f67e0) {
  %21 = "arith.subi"(%20, %17) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68f6890) {
  %22 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68f68f0) {
  %23 = "arith.addi"(%21, %22) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x68f69a0) {
  %24 = "arith.subi"(%23, %12) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.init_tensor'(0x68f6b00) {
  %25 = "linalg.init_tensor"(%24, %8, %10) {static_sizes = [-1, -1, -1]} : (index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.insert_slice'(0x68fb490) {
  %26 = "tensor.insert_slice"(%0, %25, %11, %11, %11, %24, %8, %10, %12, %12, %12) {operand_segment_sizes = dense<[1, 1, 3, 3, 3]> : vector<5xi32>, static_offsets = [-9223372036854775808, -9223372036854775808, -9223372036854775808], static_sizes = [-1, -1, -1], static_strides = [-9223372036854775808, -9223372036854775808, -9223372036854775808]} : (tensor<?x?x?xf32>, tensor<?x?x?xf32>, index, index, index, index, index, index, index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x68af140) {
  %27 = "tensor.cast"(%26) : (tensor<?x?x?xf32>) -> tensor<1x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fba70) {
  %28 = "arith.constant"() {value = 0 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68ebdf0) {
  %29 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68ef190) {
  %30 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f6580) {
  %31 = "tensor.dim"(%27, %30) : (tensor<1x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc220) {
  %32 = "arith.constant"() {value = 2 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68fc280) {
  %33 = "tensor.dim"(%27, %32) : (tensor<1x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc330) {
  %34 = "arith.constant"() {value = 0 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc390) {
  %35 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc3f0) {
  %36 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc450) {
  %37 = "arith.constant"() {value = 2 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68fc4b0) {
  %38 = "arith.addi"(%36, %31) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x68fc560) {
  %39 = "arith.cmpi"(%36, %34) {predicate = 2 : i64} : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x68fc610) {
  %40 = "std.select"(%39, %38, %36) : (i1, index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68fc6e0) {
  %41 = "arith.addi"(%37, %31) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x68fc790) {
  %42 = "arith.cmpi"(%37, %34) {predicate = 2 : i64} : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x68fc840) {
  %43 = "std.select"(%42, %41, %37) : (i1, index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x68fc910) {
  %44 = "arith.subi"(%43, %40) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fc9c0) {
  %45 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x68fca20) {
  %46 = "arith.addi"(%44, %45) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x68fe310) {
  %47 = "arith.subi"(%46, %35) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.init_tensor'(0x68fe470) {
  %48 = "linalg.init_tensor"(%29, %47, %33) {static_sizes = [-1, -1, -1]} : (index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.insert_slice'(0x68fe690) {
  %49 = "tensor.insert_slice"(%27, %48, %34, %34, %34, %29, %47, %33, %35, %35, %35) {operand_segment_sizes = dense<[1, 1, 3, 3, 3]> : vector<5xi32>, static_offsets = [-9223372036854775808, -9223372036854775808, -9223372036854775808], static_sizes = [-1, -1, -1], static_strides = [-9223372036854775808, -9223372036854775808, -9223372036854775808]} : (tensor<1x?x?xf32>, tensor<?x?x?xf32>, index, index, index, index, index, index, index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x68fe860) {
  %50 = "tensor.cast"(%49) : (tensor<?x?x?xf32>) -> tensor<1x1x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fe8f0) {
  %51 = "arith.constant"() {value = 0 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68eea90) {
  %52 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68fea00) {
  %53 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68eeaf0) {
  %54 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68feb10) {
  %55 = "arith.constant"() {value = 2 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68feb70) {
  %56 = "tensor.dim"(%50, %55) : (tensor<1x1x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x6901950) {
  %57 = "arith.constant"() {value = 0 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x69019b0) {
  %58 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x6901a10) {
  %59 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x6901a70) {
  %60 = "arith.constant"() {value = 3 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x6901ad0) {
  %61 = "arith.addi"(%59, %56) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x6901b80) {
  %62 = "arith.cmpi"(%59, %57) {predicate = 2 : i64} : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x6901c30) {
  %63 = "std.select"(%62, %61, %59) : (i1, index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x6901d00) {
  %64 = "arith.addi"(%60, %56) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x6901db0) {
  %65 = "arith.cmpi"(%60, %57) {predicate = 2 : i64} : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.select'(0x6901e60) {
  %66 = "std.select"(%65, %64, %60) : (i1, index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x6901f30) {
  %67 = "arith.subi"(%66, %63) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x6901fe0) {
  %68 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x6902040) {
  %69 = "arith.addi"(%67, %68) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subi'(0x69001e0) {
  %70 = "arith.subi"(%69, %58) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.init_tensor'(0x6900340) {
  %71 = "linalg.init_tensor"(%52, %54, %70) {static_sizes = [-1, -1, -1]} : (index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.insert_slice'(0x6900560) {
  %72 = "tensor.insert_slice"(%50, %71, %57, %57, %57, %52, %54, %70, %58, %58, %58) {operand_segment_sizes = dense<[1, 1, 3, 3, 3]> : vector<5xi32>, static_offsets = [-9223372036854775808, -9223372036854775808, -9223372036854775808], static_sizes = [-1, -1, -1], static_strides = [-9223372036854775808, -9223372036854775808, -9223372036854775808]} : (tensor<1x1x?xf32>, tensor<?x?x?xf32>, index, index, index, index, index, index, index, index, index) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x6900730) {
  %73 = "tensor.cast"(%72) : (tensor<?x?x?xf32>) -> tensor<1x1x2xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x69008b0) {
  %74 = "torch_c.from_builtin_tensor"(%73) : (tensor<1x1x2xf32>) -> !torch.vtensor<[1,1,2],f32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.return'(0x68a5110) {
  "std.return"(%74) : (!torch.vtensor<[1,1,2],f32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After StdExpandOps //----- //
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %c0_i64 = arith.constant 0 : i64
  %c1_i64 = arith.constant 1 : i64
  %c2_i64 = arith.constant 2 : i64
  %c3_i64 = arith.constant 3 : i64
  %c0 = arith.constant 0 : index
  %1 = tensor.dim %0, %c0 : tensor<?x?x?xf32>
  %c1 = arith.constant 1 : index
  %2 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %c2 = arith.constant 2 : index
  %3 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c1_3 = arith.constant 1 : index
  %4 = arith.addi %c0_2, %1 : index
  %5 = arith.cmpi slt, %c0_2, %c0_0 : index
  %6 = select %5, %4, %c0_2 : index
  %7 = arith.addi %c1_3, %1 : index
  %8 = arith.cmpi slt, %c1_3, %c0_0 : index
  %9 = select %8, %7, %c1_3 : index
  %10 = arith.subi %9, %6 : index
  %c1_4 = arith.constant 1 : index
  %11 = arith.addi %10, %c1_4 : index
  %12 = arith.subi %11, %c1_1 : index
  %13 = linalg.init_tensor [%12, %2, %3] : tensor<?x?x?xf32>
  %14 = tensor.insert_slice %0 into %13[%c0_0, %c0_0, %c0_0] [%12, %2, %3] [%c1_1, %c1_1, %c1_1] : tensor<?x?x?xf32> into tensor<?x?x?xf32>
  %15 = tensor.cast %14 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %c0_5 = arith.constant 0 : index
  %c1_6 = arith.constant 1 : index
  %c1_7 = arith.constant 1 : index
  %16 = tensor.dim %15, %c1_7 : tensor<1x?x?xf32>
  %c2_8 = arith.constant 2 : index
  %17 = tensor.dim %15, %c2_8 : tensor<1x?x?xf32>
  %c0_9 = arith.constant 0 : index
  %c1_10 = arith.constant 1 : index
  %c1_11 = arith.constant 1 : index
  %c2_12 = arith.constant 2 : index
  %18 = arith.addi %c1_11, %16 : index
  %19 = arith.cmpi slt, %c1_11, %c0_9 : index
  %20 = select %19, %18, %c1_11 : index
  %21 = arith.addi %c2_12, %16 : index
  %22 = arith.cmpi slt, %c2_12, %c0_9 : index
  %23 = select %22, %21, %c2_12 : index
  %24 = arith.subi %23, %20 : index
  %c1_13 = arith.constant 1 : index
  %25 = arith.addi %24, %c1_13 : index
  %26 = arith.subi %25, %c1_10 : index
  %27 = linalg.init_tensor [%c1_6, %26, %17] : tensor<?x?x?xf32>
  %28 = tensor.insert_slice %15 into %27[%c0_9, %c0_9, %c0_9] [%c1_6, %26, %17] [%c1_10, %c1_10, %c1_10] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %29 = tensor.cast %28 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_14 = arith.constant 0 : index
  %c1_15 = arith.constant 1 : index
  %c1_16 = arith.constant 1 : index
  %c1_17 = arith.constant 1 : index
  %c2_18 = arith.constant 2 : index
  %30 = tensor.dim %29, %c2_18 : tensor<1x1x?xf32>
  %c0_19 = arith.constant 0 : index
  %c1_20 = arith.constant 1 : index
  %c1_21 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %31 = arith.addi %c1_21, %30 : index
  %32 = arith.cmpi slt, %c1_21, %c0_19 : index
  %33 = select %32, %31, %c1_21 : index
  %34 = arith.addi %c3, %30 : index
  %35 = arith.cmpi slt, %c3, %c0_19 : index
  %36 = select %35, %34, %c3 : index
  %37 = arith.subi %36, %33 : index
  %c1_22 = arith.constant 1 : index
  %38 = arith.addi %37, %c1_22 : index
  %39 = arith.subi %38, %c1_20 : index
  %40 = linalg.init_tensor [%c1_15, %c1_17, %39] : tensor<?x?x?xf32>
  %41 = tensor.insert_slice %29 into %40[%c0_19, %c0_19, %c0_19] [%c1_15, %c1_17, %39] [%c1_20, %c1_20, %c1_20] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %42 = tensor.cast %41 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %43 = torch_c.from_builtin_tensor %42 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %43 : !torch.vtensor<[1,1,2],f32>
}


//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor'(0x6900820) {
  %0 = "torch_c.to_builtin_tensor"(%arg0) : (!torch.vtensor<[?,?,?],f32>) -> tensor<?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x69007c0) {
  %1 = "arith.constant"() {value = 0 : i64} : () -> i64

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68edf60) {
  %1 = "arith.constant"() {value = 1 : i64} : () -> i64

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ee000) {
  %1 = "arith.constant"() {value = 2 : i64} : () -> i64

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ec460) {
  %1 = "arith.constant"() {value = 3 : i64} : () -> i64

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ebc70) {
  %1 = "arith.constant"() {value = 0 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68edf60) {
  %0 = "arith.constant"() {value = 0 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68f1f10) {
  %2 = "tensor.dim"(%1, %0) : (tensor<?x?x?xf32>, index) -> index


  * Pattern (anonymous namespace)::FoldInitTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::FoldInitTensorWithDimOp"
"(anonymous namespace)::FoldInitTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTensorLoadFolder : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTensorLoadFolder"
"(anonymous namespace)::DimOfTensorLoadFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfCastOp"
"(anonymous namespace)::DimOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f49a0) {
  %3 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ebc70) {
  %0 = "arith.constant"() {value = 1 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68f4a00) {
  %4 = "tensor.dim"(%2, %0) : (tensor<?x?x?xf32>, index) -> index


  * Pattern (anonymous namespace)::FoldInitTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::FoldInitTensorWithDimOp"
"(anonymous namespace)::FoldInitTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTensorLoadFolder : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTensorLoadFolder"
"(anonymous namespace)::DimOfTensorLoadFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfCastOp"
"(anonymous namespace)::DimOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f5fa0) {
  %5 = "arith.constant"() {value = 2 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f49a0) {
  %0 = "arith.constant"() {value = 2 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68f6000) {
  %6 = "tensor.dim"(%3, %0) : (tensor<?x?x?xf32>, index) -> index


  * Pattern (anonymous namespace)::FoldInitTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::FoldInitTensorWithDimOp"
"(anonymous namespace)::FoldInitTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTensorLoadFolder : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTensorLoadFolder"
"(anonymous namespace)::DimOfTensorLoadFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfCastOp"
"(anonymous namespace)::DimOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f60b0) {
  %7 = "arith.constant"() {value = 0 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68edf60) {
  %0 = "arith.constant"() {value = 0 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f6180) {
  %7 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ebc70) {
  %0 = "arith.constant"() {value = 1 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f61e0) {
  %7 = "arith.constant"() {value = 0 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68edf60) {
  %0 = "arith.constant"() {value = 0 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f6240) {
  %7 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ebc70) {
  %0 = "arith.constant"() {value = 1 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x68f62a0) {
  %7 = "arith.addi"(%1, %4) : (index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x68f6350) {
  %7 = "arith.cmpi"(%1, %1) {predicate = 2 : i64} : (index, index) -> i1

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f5fa0) {
  %0 = "arith.constant"() {value = false} : () -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.select'(0x68f6400) {
  %8 = "std.select"(%0, %5, %2) : (i1, index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f5fa0) {
  %0 = "arith.constant"() {value = false} : () -> i1

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x68f64d0) {
  %7 = "arith.addi"(%0, %4) : (index, index) -> index


  * Pattern (anonymous namespace)::AddIAddConstant : 'arith.addi -> (arith.addi, arith.constant)' {
Trying to match "(anonymous namespace)::AddIAddConstant"
    ** Failure : castedOp1 is not ::mlir::arith::AddIOp type
"(anonymous namespace)::AddIAddConstant" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddISubConstantRHS : 'arith.addi -> (arith.addi, arith.constant)' {
Trying to match "(anonymous namespace)::AddISubConstantRHS"
    ** Failure : castedOp1 is not ::mlir::arith::SubIOp type
"(anonymous namespace)::AddISubConstantRHS" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddISubConstantLHS : 'arith.addi -> (arith.constant, arith.subi)' {
Trying to match "(anonymous namespace)::AddISubConstantLHS"
    ** Failure : castedOp1 is not ::mlir::arith::SubIOp type
"(anonymous namespace)::AddISubConstantLHS" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x68f6660) {
  %8 = "arith.cmpi"(%0, %1) {predicate = 2 : i64} : (index, index) -> i1

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f5fa0) {
  %0 = "arith.constant"() {value = false} : () -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.select'(0x68f6710) {
  %9 = "std.select"(%0, %8, %1) : (i1, index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x68f64d0) {
  %8 = "arith.addi"(%5, %1) : (index, index) -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68f1f10) {
  %5 = "tensor.dim"(%4, %2) : (tensor<?x?x?xf32>, index) -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f5fa0) {
  %0 = "arith.constant"() {value = false} : () -> i1

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.subi'(0x68f67e0) {
  %6 = "arith.subi"(%0, %1) : (index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f6890) {
  %6 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x68f68f0) {
  %6 = "arith.addi"(%0, %0) : (index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f49a0) {
  %0 = "arith.constant"() {value = 2 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.subi'(0x68f69a0) {
  %6 = "arith.subi"(%0, %1) : (index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ebc70) {
  %0 = "arith.constant"() {value = 1 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.init_tensor'(0x68f6b00) {
  %6 = "linalg.init_tensor"(%0, %4, %5) {static_sizes = [-1, -1, -1]} : (index, index, index) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::ReplaceStaticShapeDims : 'linalg.init_tensor -> ()' {
Trying to match "(anonymous namespace)::ReplaceStaticShapeDims"
    ** Insert  : 'linalg.init_tensor'(0x68fe3c0)
    ** Insert  : 'tensor.cast'(0x68ff140)
    ** Replace : 'linalg.init_tensor'(0x68f6b00)
"(anonymous namespace)::ReplaceStaticShapeDims" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %3 : tensor<1x?x?xf32> to tensor<?x?x?xf32>
  %5 = tensor.insert_slice %0 into %4[%c0, %c0, %c0] [%c1, %1, %2] [%c1, %c1, %c1] : tensor<?x?x?xf32> into tensor<?x?x?xf32>
  %6 = tensor.cast %5 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c1_2 = arith.constant 1 : index
  %7 = tensor.dim %6, %c1_2 : tensor<1x?x?xf32>
  %c2_3 = arith.constant 2 : index
  %8 = tensor.dim %6, %c2_3 : tensor<1x?x?xf32>
  %c0_4 = arith.constant 0 : index
  %c1_5 = arith.constant 1 : index
  %c1_6 = arith.constant 1 : index
  %c2_7 = arith.constant 2 : index
  %9 = arith.addi %c1_6, %7 : index
  %10 = arith.cmpi slt, %c1_6, %c0_4 : index
  %11 = select %10, %9, %c1_6 : index
  %12 = arith.addi %c2_7, %7 : index
  %13 = arith.cmpi slt, %c2_7, %c0_4 : index
  %14 = select %13, %12, %c2_7 : index
  %15 = arith.subi %14, %11 : index
  %c1_8 = arith.constant 1 : index
  %16 = arith.addi %15, %c1_8 : index
  %17 = arith.subi %16, %c1_5 : index
  %18 = linalg.init_tensor [%c1_1, %17, %8] : tensor<?x?x?xf32>
  %19 = tensor.insert_slice %6 into %18[%c0_4, %c0_4, %c0_4] [%c1_1, %17, %8] [%c1_5, %c1_5, %c1_5] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %20 = tensor.cast %19 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_9 = arith.constant 0 : index
  %c1_10 = arith.constant 1 : index
  %c1_11 = arith.constant 1 : index
  %c1_12 = arith.constant 1 : index
  %c2_13 = arith.constant 2 : index
  %21 = tensor.dim %20, %c2_13 : tensor<1x1x?xf32>
  %c0_14 = arith.constant 0 : index
  %c1_15 = arith.constant 1 : index
  %c1_16 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %22 = arith.addi %c1_16, %21 : index
  %23 = arith.cmpi slt, %c1_16, %c0_14 : index
  %24 = select %23, %22, %c1_16 : index
  %25 = arith.addi %c3, %21 : index
  %26 = arith.cmpi slt, %c3, %c0_14 : index
  %27 = select %26, %25, %c3 : index
  %28 = arith.subi %27, %24 : index
  %c1_17 = arith.constant 1 : index
  %29 = arith.addi %28, %c1_17 : index
  %30 = arith.subi %29, %c1_15 : index
  %31 = linalg.init_tensor [%c1_10, %c1_12, %30] : tensor<?x?x?xf32>
  %32 = tensor.insert_slice %20 into %31[%c0_14, %c0_14, %c0_14] [%c1_10, %c1_12, %30] [%c1_15, %c1_15, %c1_15] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %33 = tensor.cast %32 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %34 = torch_c.from_builtin_tensor %33 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %34 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68ff140) {
  %7 = "tensor.cast"(%6) : (tensor<1x?x?xf32>) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.init_tensor'(0x68fe3c0) {
  %6 = "linalg.init_tensor"(%4, %5) {static_sizes = [1, -1, -1]} : (index, index) -> tensor<1x?x?xf32>


  * Pattern (anonymous namespace)::ReplaceStaticShapeDims : 'linalg.init_tensor -> ()' {
Trying to match "(anonymous namespace)::ReplaceStaticShapeDims"
"(anonymous namespace)::ReplaceStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x68fb490) {
  %8 = "tensor.insert_slice"(%3, %7, %2, %2, %2, %0, %4, %5, %0, %0, %0) {operand_segment_sizes = dense<[1, 1, 3, 3, 3]> : vector<5xi32>, static_offsets = [-9223372036854775808, -9223372036854775808, -9223372036854775808], static_sizes = [-1, -1, -1], static_strides = [-9223372036854775808, -9223372036854775808, -9223372036854775808]} : (tensor<?x?x?xf32>, tensor<?x?x?xf32>, index, index, index, index, index, index, index, index, index) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::InsertSliceOpConstantArgumentFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpConstantArgumentFolder"
    ** Insert  : 'tensor.insert_slice'(0x6873070)
    ** Replace : 'tensor.insert_slice'(0x68fb490)
"(anonymous namespace)::InsertSliceOpConstantArgumentFolder" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %3 : tensor<1x?x?xf32> to tensor<?x?x?xf32>
  %5 = tensor.insert_slice %0 into %4[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<?x?x?xf32> into tensor<?x?x?xf32>
  %6 = tensor.cast %5 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c1_2 = arith.constant 1 : index
  %7 = tensor.dim %6, %c1_2 : tensor<1x?x?xf32>
  %c2_3 = arith.constant 2 : index
  %8 = tensor.dim %6, %c2_3 : tensor<1x?x?xf32>
  %c0_4 = arith.constant 0 : index
  %c1_5 = arith.constant 1 : index
  %c1_6 = arith.constant 1 : index
  %c2_7 = arith.constant 2 : index
  %9 = arith.addi %c1_6, %7 : index
  %10 = arith.cmpi slt, %c1_6, %c0_4 : index
  %11 = select %10, %9, %c1_6 : index
  %12 = arith.addi %c2_7, %7 : index
  %13 = arith.cmpi slt, %c2_7, %c0_4 : index
  %14 = select %13, %12, %c2_7 : index
  %15 = arith.subi %14, %11 : index
  %c1_8 = arith.constant 1 : index
  %16 = arith.addi %15, %c1_8 : index
  %17 = arith.subi %16, %c1_5 : index
  %18 = linalg.init_tensor [%c1_1, %17, %8] : tensor<?x?x?xf32>
  %19 = tensor.insert_slice %6 into %18[%c0_4, %c0_4, %c0_4] [%c1_1, %17, %8] [%c1_5, %c1_5, %c1_5] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %20 = tensor.cast %19 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_9 = arith.constant 0 : index
  %c1_10 = arith.constant 1 : index
  %c1_11 = arith.constant 1 : index
  %c1_12 = arith.constant 1 : index
  %c2_13 = arith.constant 2 : index
  %21 = tensor.dim %20, %c2_13 : tensor<1x1x?xf32>
  %c0_14 = arith.constant 0 : index
  %c1_15 = arith.constant 1 : index
  %c1_16 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %22 = arith.addi %c1_16, %21 : index
  %23 = arith.cmpi slt, %c1_16, %c0_14 : index
  %24 = select %23, %22, %c1_16 : index
  %25 = arith.addi %c3, %21 : index
  %26 = arith.cmpi slt, %c3, %c0_14 : index
  %27 = select %26, %25, %c3 : index
  %28 = arith.subi %27, %24 : index
  %c1_17 = arith.constant 1 : index
  %29 = arith.addi %28, %c1_17 : index
  %30 = arith.subi %29, %c1_15 : index
  %31 = linalg.init_tensor [%c1_10, %c1_12, %30] : tensor<?x?x?xf32>
  %32 = tensor.insert_slice %20 into %31[%c0_14, %c0_14, %c0_14] [%c1_10, %c1_12, %30] [%c1_15, %c1_15, %c1_15] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %33 = tensor.cast %32 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %34 = torch_c.from_builtin_tensor %33 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %34 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x6873070) {
  %8 = "tensor.insert_slice"(%3, %7, %4, %5) {operand_segment_sizes = dense<[1, 1, 0, 2, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, -1, -1], static_strides = [1, 1, 1]} : (tensor<?x?x?xf32>, tensor<?x?x?xf32>, index, index) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::InsertSliceOpConstantArgumentFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpConstantArgumentFolder"
"(anonymous namespace)::InsertSliceOpConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpCastFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpCastFolder"
    ** Insert  : 'tensor.insert_slice'(0x6870cb0)
    ** Insert  : 'tensor.cast'(0x68aa720)
    ** Replace : 'tensor.insert_slice'(0x6873070)
"(anonymous namespace)::InsertSliceOpCastFolder" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %3 : tensor<1x?x?xf32> to tensor<?x?x?xf32>
  %5 = tensor.insert_slice %0 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<?x?x?xf32> into tensor<1x?x?xf32>
  %6 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<?x?x?xf32>
  %7 = tensor.cast %6 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c1_2 = arith.constant 1 : index
  %8 = tensor.dim %7, %c1_2 : tensor<1x?x?xf32>
  %c2_3 = arith.constant 2 : index
  %9 = tensor.dim %7, %c2_3 : tensor<1x?x?xf32>
  %c0_4 = arith.constant 0 : index
  %c1_5 = arith.constant 1 : index
  %c1_6 = arith.constant 1 : index
  %c2_7 = arith.constant 2 : index
  %10 = arith.addi %c1_6, %8 : index
  %11 = arith.cmpi slt, %c1_6, %c0_4 : index
  %12 = select %11, %10, %c1_6 : index
  %13 = arith.addi %c2_7, %8 : index
  %14 = arith.cmpi slt, %c2_7, %c0_4 : index
  %15 = select %14, %13, %c2_7 : index
  %16 = arith.subi %15, %12 : index
  %c1_8 = arith.constant 1 : index
  %17 = arith.addi %16, %c1_8 : index
  %18 = arith.subi %17, %c1_5 : index
  %19 = linalg.init_tensor [%c1_1, %18, %9] : tensor<?x?x?xf32>
  %20 = tensor.insert_slice %7 into %19[%c0_4, %c0_4, %c0_4] [%c1_1, %18, %9] [%c1_5, %c1_5, %c1_5] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %21 = tensor.cast %20 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_9 = arith.constant 0 : index
  %c1_10 = arith.constant 1 : index
  %c1_11 = arith.constant 1 : index
  %c1_12 = arith.constant 1 : index
  %c2_13 = arith.constant 2 : index
  %22 = tensor.dim %21, %c2_13 : tensor<1x1x?xf32>
  %c0_14 = arith.constant 0 : index
  %c1_15 = arith.constant 1 : index
  %c1_16 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %23 = arith.addi %c1_16, %22 : index
  %24 = arith.cmpi slt, %c1_16, %c0_14 : index
  %25 = select %24, %23, %c1_16 : index
  %26 = arith.addi %c3, %22 : index
  %27 = arith.cmpi slt, %c3, %c0_14 : index
  %28 = select %27, %26, %c3 : index
  %29 = arith.subi %28, %25 : index
  %c1_17 = arith.constant 1 : index
  %30 = arith.addi %29, %c1_17 : index
  %31 = arith.subi %30, %c1_15 : index
  %32 = linalg.init_tensor [%c1_10, %c1_12, %31] : tensor<?x?x?xf32>
  %33 = tensor.insert_slice %21 into %32[%c0_14, %c0_14, %c0_14] [%c1_10, %c1_12, %31] [%c1_15, %c1_15, %c1_15] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %34 = tensor.cast %33 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %35 = torch_c.from_builtin_tensor %34 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %35 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68ff140) {
  %7 = "tensor.cast"(%6) : (tensor<1x?x?xf32>) -> tensor<?x?x?xf32>

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68aa720) {
  %8 = "tensor.cast"(%7) : (tensor<1x?x?xf32>) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x6870cb0) {
  %7 = "tensor.insert_slice"(%3, %6, %4, %5) {operand_segment_sizes = dense<[1, 1, 0, 2, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, -1, -1], static_strides = [1, 1, 1]} : (tensor<?x?x?xf32>, tensor<1x?x?xf32>, index, index) -> tensor<1x?x?xf32>


  * Pattern (anonymous namespace)::InsertSliceOpConstantArgumentFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpConstantArgumentFolder"
"(anonymous namespace)::InsertSliceOpConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpCastFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpCastFolder"
"(anonymous namespace)::InsertSliceOpCastFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpSourceCastInserter : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpSourceCastInserter"
    ** Insert  : 'tensor.cast'(0x68ff140)
    ** Insert  : 'tensor.insert_slice'(0x6873070)
    ** Replace : 'tensor.insert_slice'(0x6870cb0)
"(anonymous namespace)::InsertSliceOpSourceCastInserter" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %6 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<?x?x?xf32>
  %7 = tensor.cast %6 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c1_2 = arith.constant 1 : index
  %8 = tensor.dim %7, %c1_2 : tensor<1x?x?xf32>
  %c2_3 = arith.constant 2 : index
  %9 = tensor.dim %7, %c2_3 : tensor<1x?x?xf32>
  %c0_4 = arith.constant 0 : index
  %c1_5 = arith.constant 1 : index
  %c1_6 = arith.constant 1 : index
  %c2_7 = arith.constant 2 : index
  %10 = arith.addi %c1_6, %8 : index
  %11 = arith.cmpi slt, %c1_6, %c0_4 : index
  %12 = select %11, %10, %c1_6 : index
  %13 = arith.addi %c2_7, %8 : index
  %14 = arith.cmpi slt, %c2_7, %c0_4 : index
  %15 = select %14, %13, %c2_7 : index
  %16 = arith.subi %15, %12 : index
  %c1_8 = arith.constant 1 : index
  %17 = arith.addi %16, %c1_8 : index
  %18 = arith.subi %17, %c1_5 : index
  %19 = linalg.init_tensor [%c1_1, %18, %9] : tensor<?x?x?xf32>
  %20 = tensor.insert_slice %7 into %19[%c0_4, %c0_4, %c0_4] [%c1_1, %18, %9] [%c1_5, %c1_5, %c1_5] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %21 = tensor.cast %20 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_9 = arith.constant 0 : index
  %c1_10 = arith.constant 1 : index
  %c1_11 = arith.constant 1 : index
  %c1_12 = arith.constant 1 : index
  %c2_13 = arith.constant 2 : index
  %22 = tensor.dim %21, %c2_13 : tensor<1x1x?xf32>
  %c0_14 = arith.constant 0 : index
  %c1_15 = arith.constant 1 : index
  %c1_16 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %23 = arith.addi %c1_16, %22 : index
  %24 = arith.cmpi slt, %c1_16, %c0_14 : index
  %25 = select %24, %23, %c1_16 : index
  %26 = arith.addi %c3, %22 : index
  %27 = arith.cmpi slt, %c3, %c0_14 : index
  %28 = select %27, %26, %c3 : index
  %29 = arith.subi %28, %25 : index
  %c1_17 = arith.constant 1 : index
  %30 = arith.addi %29, %c1_17 : index
  %31 = arith.subi %30, %c1_15 : index
  %32 = linalg.init_tensor [%c1_10, %c1_12, %31] : tensor<?x?x?xf32>
  %33 = tensor.insert_slice %21 into %32[%c0_14, %c0_14, %c0_14] [%c1_10, %c1_12, %31] [%c1_15, %c1_15, %c1_15] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %34 = tensor.cast %33 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %35 = torch_c.from_builtin_tensor %34 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %35 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68aa720) {
  %9 = "tensor.cast"(%8) : (tensor<1x?x?xf32>) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x6873070) {
  %8 = "tensor.insert_slice"(%7, %6, %4, %5) {operand_segment_sizes = dense<[1, 1, 0, 2, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, -1, -1], static_strides = [1, 1, 1]} : (tensor<1x?x?xf32>, tensor<1x?x?xf32>, index, index) -> tensor<1x?x?xf32>


  * Pattern (anonymous namespace)::InsertSliceOpConstantArgumentFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpConstantArgumentFolder"
"(anonymous namespace)::InsertSliceOpConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpCastFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpCastFolder"
"(anonymous namespace)::InsertSliceOpCastFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpSourceCastInserter : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpSourceCastInserter"
"(anonymous namespace)::InsertSliceOpSourceCastInserter" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68ff140) {
  %7 = "tensor.cast"(%3) : (tensor<?x?x?xf32>) -> tensor<1x?x?xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68af140) {
  %10 = "tensor.cast"(%9) : (tensor<?x?x?xf32>) -> tensor<1x?x?xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
    ** Insert  : 'tensor.cast'(0x688a8e0)
    ** Replace : 'tensor.cast'(0x68af140)
"(anonymous namespace)::ChainedTensorCast" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %6 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<?x?x?xf32>
  %7 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<1x?x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c1_2 = arith.constant 1 : index
  %8 = tensor.dim %7, %c1_2 : tensor<1x?x?xf32>
  %c2_3 = arith.constant 2 : index
  %9 = tensor.dim %7, %c2_3 : tensor<1x?x?xf32>
  %c0_4 = arith.constant 0 : index
  %c1_5 = arith.constant 1 : index
  %c1_6 = arith.constant 1 : index
  %c2_7 = arith.constant 2 : index
  %10 = arith.addi %c1_6, %8 : index
  %11 = arith.cmpi slt, %c1_6, %c0_4 : index
  %12 = select %11, %10, %c1_6 : index
  %13 = arith.addi %c2_7, %8 : index
  %14 = arith.cmpi slt, %c2_7, %c0_4 : index
  %15 = select %14, %13, %c2_7 : index
  %16 = arith.subi %15, %12 : index
  %c1_8 = arith.constant 1 : index
  %17 = arith.addi %16, %c1_8 : index
  %18 = arith.subi %17, %c1_5 : index
  %19 = linalg.init_tensor [%c1_1, %18, %9] : tensor<?x?x?xf32>
  %20 = tensor.insert_slice %7 into %19[%c0_4, %c0_4, %c0_4] [%c1_1, %18, %9] [%c1_5, %c1_5, %c1_5] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %21 = tensor.cast %20 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_9 = arith.constant 0 : index
  %c1_10 = arith.constant 1 : index
  %c1_11 = arith.constant 1 : index
  %c1_12 = arith.constant 1 : index
  %c2_13 = arith.constant 2 : index
  %22 = tensor.dim %21, %c2_13 : tensor<1x1x?xf32>
  %c0_14 = arith.constant 0 : index
  %c1_15 = arith.constant 1 : index
  %c1_16 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %23 = arith.addi %c1_16, %22 : index
  %24 = arith.cmpi slt, %c1_16, %c0_14 : index
  %25 = select %24, %23, %c1_16 : index
  %26 = arith.addi %c3, %22 : index
  %27 = arith.cmpi slt, %c3, %c0_14 : index
  %28 = select %27, %26, %c3 : index
  %29 = arith.subi %28, %25 : index
  %c1_17 = arith.constant 1 : index
  %30 = arith.addi %29, %c1_17 : index
  %31 = arith.subi %30, %c1_15 : index
  %32 = linalg.init_tensor [%c1_10, %c1_12, %31] : tensor<?x?x?xf32>
  %33 = tensor.insert_slice %21 into %32[%c0_14, %c0_14, %c0_14] [%c1_10, %c1_12, %31] [%c1_15, %c1_15, %c1_15] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %34 = tensor.cast %33 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %35 = torch_c.from_builtin_tensor %34 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %35 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68aa720) {
  %9 = "tensor.cast"(%8) : (tensor<1x?x?xf32>) -> tensor<?x?x?xf32>

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x688a8e0) {
  %9 = "tensor.cast"(%8) : (tensor<1x?x?xf32>) -> tensor<1x?x?xf32>

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x6873070) {
  %8 = "tensor.insert_slice"(%7, %6, %4, %5) {operand_segment_sizes = dense<[1, 1, 0, 2, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, -1, -1], static_strides = [1, 1, 1]} : (tensor<1x?x?xf32>, tensor<1x?x?xf32>, index, index) -> tensor<1x?x?xf32>


  * Pattern (anonymous namespace)::InsertSliceOpConstantArgumentFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpConstantArgumentFolder"
"(anonymous namespace)::InsertSliceOpConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpCastFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpCastFolder"
"(anonymous namespace)::InsertSliceOpCastFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpSourceCastInserter : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpSourceCastInserter"
"(anonymous namespace)::InsertSliceOpSourceCastInserter" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68fba70) {
  %9 = "arith.constant"() {value = 0 : index} : () -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ebdf0) {
  %9 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ef190) {
  %9 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68f6580) {
  %9 = "tensor.dim"(%8, %0) : (tensor<1x?x?xf32>, index) -> index


  * Pattern (anonymous namespace)::FoldInitTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::FoldInitTensorWithDimOp"
"(anonymous namespace)::FoldInitTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTensorLoadFolder : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTensorLoadFolder"
"(anonymous namespace)::DimOfTensorLoadFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfCastOp"
"(anonymous namespace)::DimOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68fc220) {
  %10 = "arith.constant"() {value = 2 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f49a0) {
  %0 = "arith.constant"() {value = 2 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68fc280) {
  %10 = "tensor.dim"(%8, %0) : (tensor<1x?x?xf32>, index) -> index


  * Pattern (anonymous namespace)::FoldInitTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::FoldInitTensorWithDimOp"
"(anonymous namespace)::FoldInitTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTensorLoadFolder : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTensorLoadFolder"
"(anonymous namespace)::DimOfTensorLoadFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfCastOp"
"(anonymous namespace)::DimOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68fc330) {
  %11 = "arith.constant"() {value = 0 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68edf60) {
  %0 = "arith.constant"() {value = 0 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68fc390) {
  %11 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ebc70) {
  %0 = "arith.constant"() {value = 1 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68fc3f0) {
  %11 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68fc450) {
  %11 = "arith.constant"() {value = 2 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f49a0) {
  %0 = "arith.constant"() {value = 2 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x68fc4b0) {
  %11 = "arith.addi"(%1, %9) : (index, index) -> index


  * Pattern (anonymous namespace)::AddIAddConstant : 'arith.addi -> (arith.addi, arith.constant)' {
Trying to match "(anonymous namespace)::AddIAddConstant"
    ** Failure : castedOp1 is not ::mlir::arith::AddIOp type
"(anonymous namespace)::AddIAddConstant" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddISubConstantRHS : 'arith.addi -> (arith.addi, arith.constant)' {
Trying to match "(anonymous namespace)::AddISubConstantRHS"
    ** Failure : castedOp1 is not ::mlir::arith::SubIOp type
"(anonymous namespace)::AddISubConstantRHS" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddISubConstantLHS : 'arith.addi -> (arith.constant, arith.subi)' {
Trying to match "(anonymous namespace)::AddISubConstantLHS"
    ** Failure : castedOp1 is not ::mlir::arith::SubIOp type
"(anonymous namespace)::AddISubConstantLHS" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x68fc560) {
  %12 = "arith.cmpi"(%1, %2) {predicate = 2 : i64} : (index, index) -> i1

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f5fa0) {
  %0 = "arith.constant"() {value = false} : () -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.select'(0x68fc610) {
  %13 = "std.select"(%0, %12, %2) : (i1, index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x68fc4b0) {
  %12 = "arith.addi"(%10, %2) : (index, index) -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f5fa0) {
  %0 = "arith.constant"() {value = false} : () -> i1

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x68fc6e0) {
  %11 = "arith.addi"(%0, %9) : (index, index) -> index


  * Pattern (anonymous namespace)::AddIAddConstant : 'arith.addi -> (arith.addi, arith.constant)' {
Trying to match "(anonymous namespace)::AddIAddConstant"
    ** Failure : castedOp1 is not ::mlir::arith::AddIOp type
"(anonymous namespace)::AddIAddConstant" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddISubConstantRHS : 'arith.addi -> (arith.addi, arith.constant)' {
Trying to match "(anonymous namespace)::AddISubConstantRHS"
    ** Failure : castedOp1 is not ::mlir::arith::SubIOp type
"(anonymous namespace)::AddISubConstantRHS" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddISubConstantLHS : 'arith.addi -> (arith.constant, arith.subi)' {
Trying to match "(anonymous namespace)::AddISubConstantLHS"
    ** Failure : castedOp1 is not ::mlir::arith::SubIOp type
"(anonymous namespace)::AddISubConstantLHS" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x68fc790) {
  %12 = "arith.cmpi"(%0, %2) {predicate = 2 : i64} : (index, index) -> i1

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f5fa0) {
  %0 = "arith.constant"() {value = false} : () -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.select'(0x68fc840) {
  %13 = "std.select"(%0, %12, %1) : (i1, index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x68fc6e0) {
  %12 = "arith.addi"(%10, %1) : (index, index) -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68f6580) {
  %10 = "tensor.dim"(%9, %2) : (tensor<1x?x?xf32>, index) -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f5fa0) {
  %0 = "arith.constant"() {value = false} : () -> i1

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.subi'(0x68fc910) {
  %10 = "arith.subi"(%0, %1) : (index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ebc70) {
  %0 = "arith.constant"() {value = 1 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68fc9c0) {
  %10 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x68fca20) {
  %10 = "arith.addi"(%0, %0) : (index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f49a0) {
  %0 = "arith.constant"() {value = 2 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.subi'(0x68fe310) {
  %10 = "arith.subi"(%0, %1) : (index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ebc70) {
  %0 = "arith.constant"() {value = 1 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.init_tensor'(0x68fe470) {
  %10 = "linalg.init_tensor"(%0, %0, %9) {static_sizes = [-1, -1, -1]} : (index, index, index) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::ReplaceStaticShapeDims : 'linalg.init_tensor -> ()' {
Trying to match "(anonymous namespace)::ReplaceStaticShapeDims"
    ** Insert  : 'linalg.init_tensor'(0x688a8e0)
    ** Insert  : 'tensor.cast'(0x68aa720)
    ** Replace : 'linalg.init_tensor'(0x68fe470)
"(anonymous namespace)::ReplaceStaticShapeDims" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %6 = tensor.dim %5, %c2 : tensor<1x?x?xf32>
  %7 = linalg.init_tensor [1, 1, %6] : tensor<1x1x?xf32>
  %8 = tensor.cast %7 : tensor<1x1x?xf32> to tensor<?x?x?xf32>
  %9 = tensor.insert_slice %5 into %8[%c0, %c0, %c0] [%c1, %c1, %6] [%c1, %c1, %c1] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %10 = tensor.cast %9 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c1_2 = arith.constant 1 : index
  %c1_3 = arith.constant 1 : index
  %c2_4 = arith.constant 2 : index
  %11 = tensor.dim %10, %c2_4 : tensor<1x1x?xf32>
  %c0_5 = arith.constant 0 : index
  %c1_6 = arith.constant 1 : index
  %c1_7 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %12 = arith.addi %c1_7, %11 : index
  %13 = arith.cmpi slt, %c1_7, %c0_5 : index
  %14 = select %13, %12, %c1_7 : index
  %15 = arith.addi %c3, %11 : index
  %16 = arith.cmpi slt, %c3, %c0_5 : index
  %17 = select %16, %15, %c3 : index
  %18 = arith.subi %17, %14 : index
  %c1_8 = arith.constant 1 : index
  %19 = arith.addi %18, %c1_8 : index
  %20 = arith.subi %19, %c1_6 : index
  %21 = linalg.init_tensor [%c1_1, %c1_3, %20] : tensor<?x?x?xf32>
  %22 = tensor.insert_slice %10 into %21[%c0_5, %c0_5, %c0_5] [%c1_1, %c1_3, %20] [%c1_6, %c1_6, %c1_6] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %23 = tensor.cast %22 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %24 = torch_c.from_builtin_tensor %23 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %24 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68aa720) {
  %11 = "tensor.cast"(%10) : (tensor<1x1x?xf32>) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.init_tensor'(0x688a8e0) {
  %10 = "linalg.init_tensor"(%9) {static_sizes = [1, 1, -1]} : (index) -> tensor<1x1x?xf32>


  * Pattern (anonymous namespace)::ReplaceStaticShapeDims : 'linalg.init_tensor -> ()' {
Trying to match "(anonymous namespace)::ReplaceStaticShapeDims"
"(anonymous namespace)::ReplaceStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x68fe690) {
  %12 = "tensor.insert_slice"(%8, %11, %2, %2, %2, %0, %0, %9, %0, %0, %0) {operand_segment_sizes = dense<[1, 1, 3, 3, 3]> : vector<5xi32>, static_offsets = [-9223372036854775808, -9223372036854775808, -9223372036854775808], static_sizes = [-1, -1, -1], static_strides = [-9223372036854775808, -9223372036854775808, -9223372036854775808]} : (tensor<1x?x?xf32>, tensor<?x?x?xf32>, index, index, index, index, index, index, index, index, index) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::InsertSliceOpConstantArgumentFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpConstantArgumentFolder"
    ** Insert  : 'tensor.insert_slice'(0x68fc610)
    ** Replace : 'tensor.insert_slice'(0x68fe690)
"(anonymous namespace)::InsertSliceOpConstantArgumentFolder" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %6 = tensor.dim %5, %c2 : tensor<1x?x?xf32>
  %7 = linalg.init_tensor [1, 1, %6] : tensor<1x1x?xf32>
  %8 = tensor.cast %7 : tensor<1x1x?xf32> to tensor<?x?x?xf32>
  %9 = tensor.insert_slice %5 into %8[0, 0, 0] [1, 1, %6] [1, 1, 1] : tensor<1x?x?xf32> into tensor<?x?x?xf32>
  %10 = tensor.cast %9 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c1_2 = arith.constant 1 : index
  %c1_3 = arith.constant 1 : index
  %c2_4 = arith.constant 2 : index
  %11 = tensor.dim %10, %c2_4 : tensor<1x1x?xf32>
  %c0_5 = arith.constant 0 : index
  %c1_6 = arith.constant 1 : index
  %c1_7 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %12 = arith.addi %c1_7, %11 : index
  %13 = arith.cmpi slt, %c1_7, %c0_5 : index
  %14 = select %13, %12, %c1_7 : index
  %15 = arith.addi %c3, %11 : index
  %16 = arith.cmpi slt, %c3, %c0_5 : index
  %17 = select %16, %15, %c3 : index
  %18 = arith.subi %17, %14 : index
  %c1_8 = arith.constant 1 : index
  %19 = arith.addi %18, %c1_8 : index
  %20 = arith.subi %19, %c1_6 : index
  %21 = linalg.init_tensor [%c1_1, %c1_3, %20] : tensor<?x?x?xf32>
  %22 = tensor.insert_slice %10 into %21[%c0_5, %c0_5, %c0_5] [%c1_1, %c1_3, %20] [%c1_6, %c1_6, %c1_6] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %23 = tensor.cast %22 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %24 = torch_c.from_builtin_tensor %23 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %24 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x68fc610) {
  %12 = "tensor.insert_slice"(%8, %11, %9) {operand_segment_sizes = dense<[1, 1, 0, 1, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, 1, -1], static_strides = [1, 1, 1]} : (tensor<1x?x?xf32>, tensor<?x?x?xf32>, index) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::InsertSliceOpConstantArgumentFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpConstantArgumentFolder"
"(anonymous namespace)::InsertSliceOpConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpCastFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpCastFolder"
    ** Insert  : 'tensor.insert_slice'(0x68ec840)
    ** Insert  : 'tensor.cast'(0x68af140)
    ** Replace : 'tensor.insert_slice'(0x68fc610)
"(anonymous namespace)::InsertSliceOpCastFolder" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %6 = tensor.dim %5, %c2 : tensor<1x?x?xf32>
  %7 = linalg.init_tensor [1, 1, %6] : tensor<1x1x?xf32>
  %8 = tensor.cast %7 : tensor<1x1x?xf32> to tensor<?x?x?xf32>
  %9 = tensor.insert_slice %5 into %7[0, 0, 0] [1, 1, %6] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x1x?xf32>
  %10 = tensor.cast %9 : tensor<1x1x?xf32> to tensor<?x?x?xf32>
  %11 = tensor.cast %10 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c1_2 = arith.constant 1 : index
  %c1_3 = arith.constant 1 : index
  %c2_4 = arith.constant 2 : index
  %12 = tensor.dim %11, %c2_4 : tensor<1x1x?xf32>
  %c0_5 = arith.constant 0 : index
  %c1_6 = arith.constant 1 : index
  %c1_7 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %13 = arith.addi %c1_7, %12 : index
  %14 = arith.cmpi slt, %c1_7, %c0_5 : index
  %15 = select %14, %13, %c1_7 : index
  %16 = arith.addi %c3, %12 : index
  %17 = arith.cmpi slt, %c3, %c0_5 : index
  %18 = select %17, %16, %c3 : index
  %19 = arith.subi %18, %15 : index
  %c1_8 = arith.constant 1 : index
  %20 = arith.addi %19, %c1_8 : index
  %21 = arith.subi %20, %c1_6 : index
  %22 = linalg.init_tensor [%c1_1, %c1_3, %21] : tensor<?x?x?xf32>
  %23 = tensor.insert_slice %11 into %22[%c0_5, %c0_5, %c0_5] [%c1_1, %c1_3, %21] [%c1_6, %c1_6, %c1_6] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %24 = tensor.cast %23 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %25 = torch_c.from_builtin_tensor %24 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %25 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68aa720) {
  %11 = "tensor.cast"(%10) : (tensor<1x1x?xf32>) -> tensor<?x?x?xf32>

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68af140) {
  %12 = "tensor.cast"(%11) : (tensor<1x1x?xf32>) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x68ec840) {
  %11 = "tensor.insert_slice"(%8, %10, %9) {operand_segment_sizes = dense<[1, 1, 0, 1, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, 1, -1], static_strides = [1, 1, 1]} : (tensor<1x?x?xf32>, tensor<1x1x?xf32>, index) -> tensor<1x1x?xf32>


  * Pattern (anonymous namespace)::InsertSliceOpConstantArgumentFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpConstantArgumentFolder"
"(anonymous namespace)::InsertSliceOpConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpCastFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpCastFolder"
"(anonymous namespace)::InsertSliceOpCastFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpSourceCastInserter : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpSourceCastInserter"
    ** Insert  : 'tensor.cast'(0x68aa720)
    ** Insert  : 'tensor.insert_slice'(0x68fc610)
    ** Replace : 'tensor.insert_slice'(0x68ec840)
"(anonymous namespace)::InsertSliceOpSourceCastInserter" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %6 = tensor.dim %5, %c2 : tensor<1x?x?xf32>
  %7 = linalg.init_tensor [1, 1, %6] : tensor<1x1x?xf32>
  %8 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %9 = tensor.insert_slice %8 into %7[0, 0, 0] [1, 1, %6] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %10 = tensor.cast %9 : tensor<1x1x?xf32> to tensor<?x?x?xf32>
  %11 = tensor.cast %10 : tensor<?x?x?xf32> to tensor<1x1x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c1_2 = arith.constant 1 : index
  %c1_3 = arith.constant 1 : index
  %c2_4 = arith.constant 2 : index
  %12 = tensor.dim %11, %c2_4 : tensor<1x1x?xf32>
  %c0_5 = arith.constant 0 : index
  %c1_6 = arith.constant 1 : index
  %c1_7 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %13 = arith.addi %c1_7, %12 : index
  %14 = arith.cmpi slt, %c1_7, %c0_5 : index
  %15 = select %14, %13, %c1_7 : index
  %16 = arith.addi %c3, %12 : index
  %17 = arith.cmpi slt, %c3, %c0_5 : index
  %18 = select %17, %16, %c3 : index
  %19 = arith.subi %18, %15 : index
  %c1_8 = arith.constant 1 : index
  %20 = arith.addi %19, %c1_8 : index
  %21 = arith.subi %20, %c1_6 : index
  %22 = linalg.init_tensor [%c1_1, %c1_3, %21] : tensor<?x?x?xf32>
  %23 = tensor.insert_slice %11 into %22[%c0_5, %c0_5, %c0_5] [%c1_1, %c1_3, %21] [%c1_6, %c1_6, %c1_6] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %24 = tensor.cast %23 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %25 = torch_c.from_builtin_tensor %24 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %25 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68af140) {
  %13 = "tensor.cast"(%12) : (tensor<1x1x?xf32>) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x68fc610) {
  %12 = "tensor.insert_slice"(%11, %10, %9) {operand_segment_sizes = dense<[1, 1, 0, 1, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, 1, -1], static_strides = [1, 1, 1]} : (tensor<1x1x?xf32>, tensor<1x1x?xf32>, index) -> tensor<1x1x?xf32>


  * Pattern (anonymous namespace)::InsertSliceOpConstantArgumentFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpConstantArgumentFolder"
"(anonymous namespace)::InsertSliceOpConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpCastFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpCastFolder"
"(anonymous namespace)::InsertSliceOpCastFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpSourceCastInserter : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpSourceCastInserter"
"(anonymous namespace)::InsertSliceOpSourceCastInserter" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68aa720) {
  %11 = "tensor.cast"(%8) : (tensor<1x?x?xf32>) -> tensor<1x1x?xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68fe860) {
  %14 = "tensor.cast"(%13) : (tensor<?x?x?xf32>) -> tensor<1x1x?xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
    ** Insert  : 'tensor.cast'(0x68b3360)
    ** Replace : 'tensor.cast'(0x68fe860)
"(anonymous namespace)::ChainedTensorCast" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %6 = tensor.dim %5, %c2 : tensor<1x?x?xf32>
  %7 = linalg.init_tensor [1, 1, %6] : tensor<1x1x?xf32>
  %8 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %9 = tensor.insert_slice %8 into %7[0, 0, 0] [1, 1, %6] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %10 = tensor.cast %9 : tensor<1x1x?xf32> to tensor<?x?x?xf32>
  %11 = tensor.cast %9 : tensor<1x1x?xf32> to tensor<1x1x?xf32>
  %c0_0 = arith.constant 0 : index
  %c1_1 = arith.constant 1 : index
  %c1_2 = arith.constant 1 : index
  %c1_3 = arith.constant 1 : index
  %c2_4 = arith.constant 2 : index
  %12 = tensor.dim %11, %c2_4 : tensor<1x1x?xf32>
  %c0_5 = arith.constant 0 : index
  %c1_6 = arith.constant 1 : index
  %c1_7 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %13 = arith.addi %c1_7, %12 : index
  %14 = arith.cmpi slt, %c1_7, %c0_5 : index
  %15 = select %14, %13, %c1_7 : index
  %16 = arith.addi %c3, %12 : index
  %17 = arith.cmpi slt, %c3, %c0_5 : index
  %18 = select %17, %16, %c3 : index
  %19 = arith.subi %18, %15 : index
  %c1_8 = arith.constant 1 : index
  %20 = arith.addi %19, %c1_8 : index
  %21 = arith.subi %20, %c1_6 : index
  %22 = linalg.init_tensor [%c1_1, %c1_3, %21] : tensor<?x?x?xf32>
  %23 = tensor.insert_slice %11 into %22[%c0_5, %c0_5, %c0_5] [%c1_1, %c1_3, %21] [%c1_6, %c1_6, %c1_6] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %24 = tensor.cast %23 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %25 = torch_c.from_builtin_tensor %24 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %25 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68af140) {
  %13 = "tensor.cast"(%12) : (tensor<1x1x?xf32>) -> tensor<?x?x?xf32>

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68b3360) {
  %13 = "tensor.cast"(%12) : (tensor<1x1x?xf32>) -> tensor<1x1x?xf32>

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x68fc610) {
  %12 = "tensor.insert_slice"(%11, %10, %9) {operand_segment_sizes = dense<[1, 1, 0, 1, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, 1, -1], static_strides = [1, 1, 1]} : (tensor<1x1x?xf32>, tensor<1x1x?xf32>, index) -> tensor<1x1x?xf32>


  * Pattern (anonymous namespace)::InsertSliceOpConstantArgumentFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpConstantArgumentFolder"
"(anonymous namespace)::InsertSliceOpConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpCastFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpCastFolder"
"(anonymous namespace)::InsertSliceOpCastFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpSourceCastInserter : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpSourceCastInserter"
"(anonymous namespace)::InsertSliceOpSourceCastInserter" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68fe8f0) {
  %13 = "arith.constant"() {value = 0 : index} : () -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68eea90) {
  %13 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68fea00) {
  %13 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68eeaf0) {
  %13 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68feb10) {
  %13 = "arith.constant"() {value = 2 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f49a0) {
  %0 = "arith.constant"() {value = 2 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68feb70) {
  %13 = "tensor.dim"(%12, %0) : (tensor<1x1x?xf32>, index) -> index


  * Pattern (anonymous namespace)::FoldInitTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::FoldInitTensorWithDimOp"
"(anonymous namespace)::FoldInitTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTensorLoadFolder : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTensorLoadFolder"
"(anonymous namespace)::DimOfTensorLoadFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfCastOp"
"(anonymous namespace)::DimOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x6901950) {
  %14 = "arith.constant"() {value = 0 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68edf60) {
  %0 = "arith.constant"() {value = 0 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x69019b0) {
  %14 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ebc70) {
  %0 = "arith.constant"() {value = 1 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x6901a10) {
  %14 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x6901a70) {
  %14 = "arith.constant"() {value = 3 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f5fa0) {
  %0 = "arith.constant"() {value = 3 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x6901ad0) {
  %15 = "arith.addi"(%1, %14) : (index, index) -> index


  * Pattern (anonymous namespace)::AddIAddConstant : 'arith.addi -> (arith.addi, arith.constant)' {
Trying to match "(anonymous namespace)::AddIAddConstant"
    ** Failure : castedOp1 is not ::mlir::arith::AddIOp type
"(anonymous namespace)::AddIAddConstant" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddISubConstantRHS : 'arith.addi -> (arith.addi, arith.constant)' {
Trying to match "(anonymous namespace)::AddISubConstantRHS"
    ** Failure : castedOp1 is not ::mlir::arith::SubIOp type
"(anonymous namespace)::AddISubConstantRHS" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddISubConstantLHS : 'arith.addi -> (arith.constant, arith.subi)' {
Trying to match "(anonymous namespace)::AddISubConstantLHS"
    ** Failure : castedOp1 is not ::mlir::arith::SubIOp type
"(anonymous namespace)::AddISubConstantLHS" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x6901b80) {
  %16 = "arith.cmpi"(%1, %2) {predicate = 2 : i64} : (index, index) -> i1

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x6901a70) {
  %0 = "arith.constant"() {value = false} : () -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.select'(0x6901c30) {
  %17 = "std.select"(%0, %16, %2) : (i1, index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x6901ad0) {
  %16 = "arith.addi"(%15, %2) : (index, index) -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x6901a70) {
  %0 = "arith.constant"() {value = false} : () -> i1

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x6901d00) {
  %15 = "arith.addi"(%0, %14) : (index, index) -> index


  * Pattern (anonymous namespace)::AddIAddConstant : 'arith.addi -> (arith.addi, arith.constant)' {
Trying to match "(anonymous namespace)::AddIAddConstant"
    ** Failure : castedOp1 is not ::mlir::arith::AddIOp type
"(anonymous namespace)::AddIAddConstant" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddISubConstantRHS : 'arith.addi -> (arith.addi, arith.constant)' {
Trying to match "(anonymous namespace)::AddISubConstantRHS"
    ** Failure : castedOp1 is not ::mlir::arith::SubIOp type
"(anonymous namespace)::AddISubConstantRHS" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddISubConstantLHS : 'arith.addi -> (arith.constant, arith.subi)' {
Trying to match "(anonymous namespace)::AddISubConstantLHS"
    ** Failure : castedOp1 is not ::mlir::arith::SubIOp type
"(anonymous namespace)::AddISubConstantLHS" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x6901db0) {
  %16 = "arith.cmpi"(%0, %2) {predicate = 2 : i64} : (index, index) -> i1

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x6901a70) {
  %0 = "arith.constant"() {value = false} : () -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.select'(0x6901e60) {
  %17 = "std.select"(%0, %16, %1) : (i1, index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x6901d00) {
  %16 = "arith.addi"(%15, %1) : (index, index) -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68feb70) {
  %15 = "tensor.dim"(%14, %4) : (tensor<1x1x?xf32>, index) -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x6901a70) {
  %0 = "arith.constant"() {value = false} : () -> i1

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.subi'(0x6901f30) {
  %14 = "arith.subi"(%0, %1) : (index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f5fa0) {
  %1 = "arith.constant"() {value = 3 : index} : () -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f49a0) {
  %0 = "arith.constant"() {value = 2 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x6901fe0) {
  %13 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ebc70) {
  %0 = "arith.constant"() {value = 1 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x6902040) {
  %13 = "arith.addi"(%1, %0) : (index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x6901a70) {
  %0 = "arith.constant"() {value = 3 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.subi'(0x69001e0) {
  %14 = "arith.subi"(%0, %1) : (index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x6901a70) {
  %1 = "arith.constant"() {value = 3 : index} : () -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f49a0) {
  %0 = "arith.constant"() {value = 2 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.init_tensor'(0x6900340) {
  %13 = "linalg.init_tensor"(%1, %1, %0) {static_sizes = [-1, -1, -1]} : (index, index, index) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::ReplaceStaticShapeDims : 'linalg.init_tensor -> ()' {
Trying to match "(anonymous namespace)::ReplaceStaticShapeDims"
    ** Insert  : 'linalg.init_tensor'(0x68664f0)
    ** Insert  : 'tensor.cast'(0x68b3360)
    ** Replace : 'linalg.init_tensor'(0x6900340)
"(anonymous namespace)::ReplaceStaticShapeDims" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %6 = tensor.dim %5, %c2 : tensor<1x?x?xf32>
  %7 = linalg.init_tensor [1, 1, %6] : tensor<1x1x?xf32>
  %8 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %9 = tensor.insert_slice %8 into %7[0, 0, 0] [1, 1, %6] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %10 = linalg.init_tensor [1, 1, 2] : tensor<1x1x2xf32>
  %11 = tensor.cast %10 : tensor<1x1x2xf32> to tensor<?x?x?xf32>
  %12 = tensor.insert_slice %9 into %11[%c0, %c0, %c0] [%c1, %c1, %c2] [%c1, %c1, %c1] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %13 = tensor.cast %12 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %14 = torch_c.from_builtin_tensor %13 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %14 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68b3360) {
  %14 = "tensor.cast"(%13) : (tensor<1x1x2xf32>) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.init_tensor'(0x68664f0) {
  %13 = "linalg.init_tensor"() {static_sizes = [1, 1, 2]} : () -> tensor<1x1x2xf32>


  * Pattern (anonymous namespace)::ReplaceStaticShapeDims : 'linalg.init_tensor -> ()' {
Trying to match "(anonymous namespace)::ReplaceStaticShapeDims"
"(anonymous namespace)::ReplaceStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x6900560) {
  %15 = "tensor.insert_slice"(%12, %14, %2, %2, %2, %1, %1, %0, %1, %1, %1) {operand_segment_sizes = dense<[1, 1, 3, 3, 3]> : vector<5xi32>, static_offsets = [-9223372036854775808, -9223372036854775808, -9223372036854775808], static_sizes = [-1, -1, -1], static_strides = [-9223372036854775808, -9223372036854775808, -9223372036854775808]} : (tensor<1x1x?xf32>, tensor<?x?x?xf32>, index, index, index, index, index, index, index, index, index) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::InsertSliceOpConstantArgumentFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpConstantArgumentFolder"
    ** Insert  : 'tensor.insert_slice'(0x68fc560)
    ** Replace : 'tensor.insert_slice'(0x6900560)
"(anonymous namespace)::InsertSliceOpConstantArgumentFolder" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %6 = tensor.dim %5, %c2 : tensor<1x?x?xf32>
  %7 = linalg.init_tensor [1, 1, %6] : tensor<1x1x?xf32>
  %8 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %9 = tensor.insert_slice %8 into %7[0, 0, 0] [1, 1, %6] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %10 = linalg.init_tensor [1, 1, 2] : tensor<1x1x2xf32>
  %11 = tensor.cast %10 : tensor<1x1x2xf32> to tensor<?x?x?xf32>
  %12 = tensor.insert_slice %9 into %11[0, 0, 0] [1, 1, 2] [1, 1, 1] : tensor<1x1x?xf32> into tensor<?x?x?xf32>
  %13 = tensor.cast %12 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %14 = torch_c.from_builtin_tensor %13 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %14 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x68fc560) {
  %15 = "tensor.insert_slice"(%12, %14) {operand_segment_sizes = dense<[1, 1, 0, 0, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, 1, 2], static_strides = [1, 1, 1]} : (tensor<1x1x?xf32>, tensor<?x?x?xf32>) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::InsertSliceOpConstantArgumentFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpConstantArgumentFolder"
"(anonymous namespace)::InsertSliceOpConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpCastFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpCastFolder"
    ** Insert  : 'tensor.insert_slice'(0x68fe540)
    ** Insert  : 'tensor.cast'(0x68af140)
    ** Replace : 'tensor.insert_slice'(0x68fc560)
"(anonymous namespace)::InsertSliceOpCastFolder" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %6 = tensor.dim %5, %c2 : tensor<1x?x?xf32>
  %7 = linalg.init_tensor [1, 1, %6] : tensor<1x1x?xf32>
  %8 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %9 = tensor.insert_slice %8 into %7[0, 0, 0] [1, 1, %6] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %10 = linalg.init_tensor [1, 1, 2] : tensor<1x1x2xf32>
  %11 = tensor.cast %10 : tensor<1x1x2xf32> to tensor<?x?x?xf32>
  %12 = tensor.insert_slice %9 into %10[0, 0, 0] [1, 1, 2] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x2xf32>
  %13 = tensor.cast %12 : tensor<1x1x2xf32> to tensor<?x?x?xf32>
  %14 = tensor.cast %13 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %15 = torch_c.from_builtin_tensor %14 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %15 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68b3360) {
  %14 = "tensor.cast"(%13) : (tensor<1x1x2xf32>) -> tensor<?x?x?xf32>

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68af140) {
  %15 = "tensor.cast"(%14) : (tensor<1x1x2xf32>) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x68fe540) {
  %14 = "tensor.insert_slice"(%12, %13) {operand_segment_sizes = dense<[1, 1, 0, 0, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, 1, 2], static_strides = [1, 1, 1]} : (tensor<1x1x?xf32>, tensor<1x1x2xf32>) -> tensor<1x1x2xf32>


  * Pattern (anonymous namespace)::InsertSliceOpConstantArgumentFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpConstantArgumentFolder"
"(anonymous namespace)::InsertSliceOpConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpCastFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpCastFolder"
"(anonymous namespace)::InsertSliceOpCastFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpSourceCastInserter : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpSourceCastInserter"
    ** Insert  : 'tensor.cast'(0x68b3360)
    ** Insert  : 'tensor.insert_slice'(0x68fc560)
    ** Replace : 'tensor.insert_slice'(0x68fe540)
"(anonymous namespace)::InsertSliceOpSourceCastInserter" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %6 = tensor.dim %5, %c2 : tensor<1x?x?xf32>
  %7 = linalg.init_tensor [1, 1, %6] : tensor<1x1x?xf32>
  %8 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %9 = tensor.insert_slice %8 into %7[0, 0, 0] [1, 1, %6] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %10 = linalg.init_tensor [1, 1, 2] : tensor<1x1x2xf32>
  %11 = tensor.cast %9 : tensor<1x1x?xf32> to tensor<1x1x2xf32>
  %12 = tensor.insert_slice %11 into %10[0, 0, 0] [1, 1, 2] [1, 1, 1] : tensor<1x1x2xf32> into tensor<1x1x2xf32>
  %13 = tensor.cast %12 : tensor<1x1x2xf32> to tensor<?x?x?xf32>
  %14 = tensor.cast %13 : tensor<?x?x?xf32> to tensor<1x1x2xf32>
  %15 = torch_c.from_builtin_tensor %14 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %15 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68af140) {
  %16 = "tensor.cast"(%15) : (tensor<1x1x2xf32>) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x68fc560) {
  %15 = "tensor.insert_slice"(%14, %13) {operand_segment_sizes = dense<[1, 1, 0, 0, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, 1, 2], static_strides = [1, 1, 1]} : (tensor<1x1x2xf32>, tensor<1x1x2xf32>) -> tensor<1x1x2xf32>

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68af140) {
  %15 = "tensor.cast"(%14) : (tensor<1x1x2xf32>) -> tensor<?x?x?xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.init_tensor'(0x68664f0) {
  %13 = "linalg.init_tensor"() {static_sizes = [1, 1, 2]} : () -> tensor<1x1x2xf32>

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68b3360) {
  %13 = "tensor.cast"(%12) : (tensor<1x1x?xf32>) -> tensor<1x1x2xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x6900730) {
  %15 = "tensor.cast"(%14) : (tensor<?x?x?xf32>) -> tensor<1x1x2xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
    ** Insert  : 'tensor.cast'(0x68fe860)
    ** Replace : 'tensor.cast'(0x6900730)
"(anonymous namespace)::ChainedTensorCast" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %6 = tensor.dim %5, %c2 : tensor<1x?x?xf32>
  %7 = linalg.init_tensor [1, 1, %6] : tensor<1x1x?xf32>
  %8 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %9 = tensor.insert_slice %8 into %7[0, 0, 0] [1, 1, %6] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %10 = tensor.cast %9 : tensor<1x1x?xf32> to tensor<1x1x2xf32>
  %11 = tensor.cast %10 : tensor<1x1x2xf32> to tensor<?x?x?xf32>
  %12 = tensor.cast %10 : tensor<1x1x2xf32> to tensor<1x1x2xf32>
  %13 = torch_c.from_builtin_tensor %12 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %13 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68af140) {
  %14 = "tensor.cast"(%13) : (tensor<1x1x2xf32>) -> tensor<?x?x?xf32>

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68fe860) {
  %14 = "tensor.cast"(%13) : (tensor<1x1x2xf32>) -> tensor<1x1x2xf32>

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68b3360) {
  %13 = "tensor.cast"(%12) : (tensor<1x1x?xf32>) -> tensor<1x1x2xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x69008b0) {
  %14 = "torch_c.from_builtin_tensor"(%13) : (tensor<1x1x2xf32>) -> !torch.vtensor<[1,1,2],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.return'(0x68a5110) {
  "std.return"(%14) : (!torch.vtensor<[1,1,2],f32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
** Erase   : 'arith.constant'(0x68edf60)

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f49a0) {
  %0 = "arith.constant"() {value = 2 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ebc70) {
  %1 = "arith.constant"() {value = 1 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor'(0x6900820) {
  %2 = "torch_c.to_builtin_tensor"(%arg0) : (!torch.vtensor<[?,?,?],f32>) -> tensor<?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68f4a00) {
  %3 = "tensor.dim"(%2, %1) : (tensor<?x?x?xf32>, index) -> index


  * Pattern (anonymous namespace)::FoldInitTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::FoldInitTensorWithDimOp"
"(anonymous namespace)::FoldInitTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTensorLoadFolder : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTensorLoadFolder"
"(anonymous namespace)::DimOfTensorLoadFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfCastOp"
"(anonymous namespace)::DimOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68f6000) {
  %4 = "tensor.dim"(%2, %0) : (tensor<?x?x?xf32>, index) -> index


  * Pattern (anonymous namespace)::FoldInitTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::FoldInitTensorWithDimOp"
"(anonymous namespace)::FoldInitTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTensorLoadFolder : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTensorLoadFolder"
"(anonymous namespace)::DimOfTensorLoadFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfCastOp"
"(anonymous namespace)::DimOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.init_tensor'(0x68fe3c0) {
  %5 = "linalg.init_tensor"(%3, %4) {static_sizes = [1, -1, -1]} : (index, index) -> tensor<1x?x?xf32>


  * Pattern (anonymous namespace)::ReplaceStaticShapeDims : 'linalg.init_tensor -> ()' {
Trying to match "(anonymous namespace)::ReplaceStaticShapeDims"
"(anonymous namespace)::ReplaceStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68ff140) {
  %6 = "tensor.cast"(%2) : (tensor<?x?x?xf32>) -> tensor<1x?x?xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x6873070) {
  %7 = "tensor.insert_slice"(%6, %5, %3, %4) {operand_segment_sizes = dense<[1, 1, 0, 2, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, -1, -1], static_strides = [1, 1, 1]} : (tensor<1x?x?xf32>, tensor<1x?x?xf32>, index, index) -> tensor<1x?x?xf32>


  * Pattern (anonymous namespace)::InsertSliceOpConstantArgumentFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpConstantArgumentFolder"
"(anonymous namespace)::InsertSliceOpConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpCastFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpCastFolder"
"(anonymous namespace)::InsertSliceOpCastFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpSourceCastInserter : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpSourceCastInserter"
"(anonymous namespace)::InsertSliceOpSourceCastInserter" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68fc280) {
  %8 = "tensor.dim"(%7, %0) : (tensor<1x?x?xf32>, index) -> index


  * Pattern (anonymous namespace)::FoldInitTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::FoldInitTensorWithDimOp"
"(anonymous namespace)::FoldInitTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopInsOutsFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfTiledLoopResultFolder<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfTensorLoadFolder : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfTensorLoadFolder"
"(anonymous namespace)::DimOfTensorLoadFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfCastOp"
"(anonymous namespace)::DimOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.init_tensor'(0x688a8e0) {
  %9 = "linalg.init_tensor"(%8) {static_sizes = [1, 1, -1]} : (index) -> tensor<1x1x?xf32>


  * Pattern (anonymous namespace)::ReplaceStaticShapeDims : 'linalg.init_tensor -> ()' {
Trying to match "(anonymous namespace)::ReplaceStaticShapeDims"
"(anonymous namespace)::ReplaceStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68aa720) {
  %10 = "tensor.cast"(%7) : (tensor<1x?x?xf32>) -> tensor<1x1x?xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x68fc610) {
  %11 = "tensor.insert_slice"(%10, %9, %8) {operand_segment_sizes = dense<[1, 1, 0, 1, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, 1, -1], static_strides = [1, 1, 1]} : (tensor<1x1x?xf32>, tensor<1x1x?xf32>, index) -> tensor<1x1x?xf32>


  * Pattern (anonymous namespace)::InsertSliceOpConstantArgumentFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpConstantArgumentFolder"
"(anonymous namespace)::InsertSliceOpConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpCastFolder : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpCastFolder"
"(anonymous namespace)::InsertSliceOpCastFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpSourceCastInserter : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpSourceCastInserter"
"(anonymous namespace)::InsertSliceOpSourceCastInserter" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68b3360) {
  %12 = "tensor.cast"(%11) : (tensor<1x1x?xf32>) -> tensor<1x1x2xf32>


  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x69008b0) {
  %13 = "torch_c.from_builtin_tensor"(%12) : (tensor<1x1x2xf32>) -> !torch.vtensor<[1,1,2],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.return'(0x68a5110) {
  "std.return"(%13) : (!torch.vtensor<[1,1,2],f32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer //----- //
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %6 = tensor.dim %5, %c2 : tensor<1x?x?xf32>
  %7 = linalg.init_tensor [1, 1, %6] : tensor<1x1x?xf32>
  %8 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %9 = tensor.insert_slice %8 into %7[0, 0, 0] [1, 1, %6] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %10 = tensor.cast %9 : tensor<1x1x?xf32> to tensor<1x1x2xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %11 : !torch.vtensor<[1,1,2],f32>
}


//===-------------------------------------------===//
Processing operation : 'std.return'(0x68a5110) {
  "std.return"(%13) : (!torch.vtensor<[1,1,2],f32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x69008b0) {
  %13 = "torch_c.from_builtin_tensor"(%12) : (tensor<1x1x2xf32>) -> !torch.vtensor<[1,1,2],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68b3360) {
  %12 = "tensor.cast"(%11) : (tensor<1x1x?xf32>) -> tensor<1x1x2xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x68fc610) {
  %11 = "tensor.insert_slice"(%10, %9, %8) {operand_segment_sizes = dense<[1, 1, 0, 1, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, 1, -1], static_strides = [1, 1, 1]} : (tensor<1x1x?xf32>, tensor<1x1x?xf32>, index) -> tensor<1x1x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68aa720) {
  %10 = "tensor.cast"(%7) : (tensor<1x?x?xf32>) -> tensor<1x1x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.init_tensor'(0x688a8e0) {
  %9 = "linalg.init_tensor"(%8) {static_sizes = [1, 1, -1]} : (index) -> tensor<1x1x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68fc280) {
  %8 = "tensor.dim"(%7, %0) : (tensor<1x?x?xf32>, index) -> index


  * Pattern (anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp>"
    ** Insert  : 'arith.constant'(0x6901a70)
    ** Insert  : 'arith.constant'(0x69007c0)
    ** Insert  : 'arith.constant'(0x68d8350)
    ** Insert  : 'tensor.dim'(0x68fc560)
    ** Insert  : 'arith.constant'(0x686cd80)
    ** Insert  : 'tensor.dim'(0x68fe540)
    ** Replace : 'tensor.dim'(0x68fc280)
"(anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp>" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %c0 = arith.constant 0 : index
  %c1_0 = arith.constant 1 : index
  %c1_1 = arith.constant 1 : index
  %6 = tensor.dim %3, %c1_1 : tensor<1x?x?xf32>
  %c2_2 = arith.constant 2 : index
  %7 = tensor.dim %3, %c2_2 : tensor<1x?x?xf32>
  %8 = linalg.init_tensor [1, 1, %7] : tensor<1x1x?xf32>
  %9 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %10 = tensor.insert_slice %9 into %8[0, 0, 0] [1, 1, %7] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %11 = tensor.cast %10 : tensor<1x1x?xf32> to tensor<1x1x2xf32>
  %12 = torch_c.from_builtin_tensor %11 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %12 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.init_tensor'(0x688a8e0) {
  %14 = "linalg.init_tensor"(%13) {static_sizes = [1, 1, -1]} : (index) -> tensor<1x1x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x68fc610) {
  %16 = "tensor.insert_slice"(%15, %14, %13) {operand_segment_sizes = dense<[1, 1, 0, 1, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, 1, -1], static_strides = [1, 1, 1]} : (tensor<1x1x?xf32>, tensor<1x1x?xf32>, index) -> tensor<1x1x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68fe540) {
  %13 = "tensor.dim"(%5, %12) : (tensor<1x?x?xf32>, index) -> index


  * Pattern (anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp>"
    ** Insert  : 'arith.constant'(0x686b710)
    ** Replace : 'tensor.dim'(0x68fe540)
"(anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp>" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %c0 = arith.constant 0 : index
  %c1_0 = arith.constant 1 : index
  %c1_1 = arith.constant 1 : index
  %6 = tensor.dim %3, %c1_1 : tensor<1x?x?xf32>
  %c2_2 = arith.constant 2 : index
  %c1_3 = arith.constant 1 : index
  %7 = linalg.init_tensor [1, 1, %2] : tensor<1x1x?xf32>
  %8 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %9 = tensor.insert_slice %8 into %7[0, 0, 0] [1, 1, %2] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %10 = tensor.cast %9 : tensor<1x1x?xf32> to tensor<1x1x2xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %11 : !torch.vtensor<[1,1,2],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x68fc610) {
  %16 = "tensor.insert_slice"(%15, %14, %4) {operand_segment_sizes = dense<[1, 1, 0, 1, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, 1, -1], static_strides = [1, 1, 1]} : (tensor<1x1x?xf32>, tensor<1x1x?xf32>, index) -> tensor<1x1x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.init_tensor'(0x688a8e0) {
  %14 = "linalg.init_tensor"(%4) {static_sizes = [1, 1, -1]} : (index) -> tensor<1x1x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x686b710) {
  %13 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x686cd80) {
  %12 = "arith.constant"() {value = 2 : index} : () -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68fc560) {
  %11 = "tensor.dim"(%5, %10) : (tensor<1x?x?xf32>, index) -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68d8350) {
  %10 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x69007c0) {
  %9 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x6901a70) {
  %8 = "arith.constant"() {value = 0 : index} : () -> index

} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x6873070) {
  %7 = "tensor.insert_slice"(%6, %5, %3, %4) {operand_segment_sizes = dense<[1, 1, 0, 2, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, -1, -1], static_strides = [1, 1, 1]} : (tensor<1x?x?xf32>, tensor<1x?x?xf32>, index, index) -> tensor<1x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68ff140) {
  %6 = "tensor.cast"(%2) : (tensor<?x?x?xf32>) -> tensor<1x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.init_tensor'(0x68fe3c0) {
  %5 = "linalg.init_tensor"(%3, %4) {static_sizes = [1, -1, -1]} : (index, index) -> tensor<1x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68f6000) {
  %4 = "tensor.dim"(%2, %0) : (tensor<?x?x?xf32>, index) -> index


  * Pattern (anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68f4a00) {
  %3 = "tensor.dim"(%2, %1) : (tensor<?x?x?xf32>, index) -> index


  * Pattern (anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor'(0x6900820) {
  %2 = "torch_c.to_builtin_tensor"(%arg0) : (!torch.vtensor<[?,?,?],f32>) -> tensor<?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ebc70) {
  %1 = "arith.constant"() {value = 1 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68f4a00) {
  %3 = "tensor.dim"(%2, %0) : (tensor<?x?x?xf32>, index) -> index


  * Pattern (anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x6901a70) {
  %0 = "arith.constant"() {value = 1 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68f49a0) {
  %1 = "arith.constant"() {value = 2 : index} : () -> index

} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68f6000) {
  %4 = "tensor.dim"(%2, %0) : (tensor<?x?x?xf32>, index) -> index


  * Pattern (anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ebc70) {
  %0 = "arith.constant"() {value = 2 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'std.return'(0x68a5110) {
  "std.return"(%12) : (!torch.vtensor<[1,1,2],f32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x69008b0) {
  %12 = "torch_c.from_builtin_tensor"(%11) : (tensor<1x1x2xf32>) -> !torch.vtensor<[1,1,2],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68b3360) {
  %11 = "tensor.cast"(%10) : (tensor<1x1x?xf32>) -> tensor<1x1x2xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x68fc610) {
  %10 = "tensor.insert_slice"(%9, %8, %4) {operand_segment_sizes = dense<[1, 1, 0, 1, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, 1, -1], static_strides = [1, 1, 1]} : (tensor<1x1x?xf32>, tensor<1x1x?xf32>, index) -> tensor<1x1x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68aa720) {
  %9 = "tensor.cast"(%7) : (tensor<1x?x?xf32>) -> tensor<1x1x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.init_tensor'(0x688a8e0) {
  %8 = "linalg.init_tensor"(%4) {static_sizes = [1, 1, -1]} : (index) -> tensor<1x1x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x6873070) {
  %7 = "tensor.insert_slice"(%6, %5, %3, %4) {operand_segment_sizes = dense<[1, 1, 0, 2, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, -1, -1], static_strides = [1, 1, 1]} : (tensor<1x?x?xf32>, tensor<1x?x?xf32>, index, index) -> tensor<1x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x68ff140) {
  %6 = "tensor.cast"(%2) : (tensor<?x?x?xf32>) -> tensor<1x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.init_tensor'(0x68fe3c0) {
  %5 = "linalg.init_tensor"(%3, %4) {static_sizes = [1, -1, -1]} : (index, index) -> tensor<1x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68f6000) {
  %4 = "tensor.dim"(%2, %0) : (tensor<?x?x?xf32>, index) -> index


  * Pattern (anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x68f4a00) {
  %3 = "tensor.dim"(%2, %1) : (tensor<?x?x?xf32>, index) -> index


  * Pattern (anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfReifyRankedShapedTypeOpInterface<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp> : 'tensor.dim -> ()' {
Trying to match "(anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp>"
"(anonymous namespace)::DimOfShapedTypeOpInterface<mlir::tensor::DimOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor'(0x6900820) {
  %2 = "torch_c.to_builtin_tensor"(%arg0) : (!torch.vtensor<[?,?,?],f32>) -> tensor<?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x6901a70) {
  %1 = "arith.constant"() {value = 1 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x68ebc70) {
  %0 = "arith.constant"() {value = 2 : index} : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After ResolveShapedTypeResultDims //----- //
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %6 = linalg.init_tensor [1, 1, %2] : tensor<1x1x?xf32>
  %7 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %8 = tensor.insert_slice %7 into %6[0, 0, 0] [1, 1, %2] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %9 = tensor.cast %8 : tensor<1x1x?xf32> to tensor<1x1x2xf32>
  %10 = torch_c.from_builtin_tensor %9 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %10 : !torch.vtensor<[1,1,2],f32>
}

// -----// IR Dump After CSE //----- //
func @forward(%arg0: !torch.vtensor<[?,?,?],f32>, %arg1: !torch.vtensor<[?,?],f32>) -> !torch.vtensor<[1,1,2],f32> {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %6 = linalg.init_tensor [1, 1, %2] : tensor<1x1x?xf32>
  %7 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %8 = tensor.insert_slice %7 into %6[0, 0, 0] [1, 1, %2] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %9 = tensor.cast %8 : tensor<1x1x?xf32> to tensor<1x1x2xf32>
  %10 = torch_c.from_builtin_tensor %9 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  return %10 : !torch.vtensor<[1,1,2],f32>
}


//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x6867970) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'builtin.func'(0x686d8b0) {
  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'builtin.func -> ()' {
Trying to match "(anonymous namespace)::FunctionLikeSignatureConversion"
"(anonymous namespace)::FunctionLikeSignatureConversion" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'builtin.func'(0x686d8b0) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
module attributes {torch.debug_module_name = "MyModule"}  {
  func @forward(%arg0: tensor<?x?x?xf32>, %arg1: tensor<?x?xf32>) -> tensor<1x1x2xf32> {
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %0 = torch_c.to_builtin_tensor <<UNKNOWN SSA VALUE>> : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
    %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
    %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
    %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
    %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
    %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
    %6 = linalg.init_tensor [1, 1, %2] : tensor<1x1x?xf32>
    %7 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
    %8 = tensor.insert_slice %7 into %6[0, 0, 0] [1, 1, %2] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
    %9 = tensor.cast %8 : tensor<1x1x?xf32> to tensor<1x1x2xf32>
    %10 = torch_c.from_builtin_tensor %9 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
    return %10 : !torch.vtensor<[1,1,2],f32>
  }
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68ebc70) {
  %0 = "arith.constant"() {value = 2 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x6901a70) {
  %1 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x6900820) {
  %2 = "torch_c.to_builtin_tensor"(<<UNKNOWN SSA VALUE>>) : (!torch.vtensor<[?,?,?],f32>) -> tensor<?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f4a00) {
  %3 = "tensor.dim"(%2, %1) : (tensor<?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f6000) {
  %4 = "tensor.dim"(%2, %0) : (tensor<?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.init_tensor'(0x68fe3c0) {
  %5 = "linalg.init_tensor"(%3, %4) {static_sizes = [1, -1, -1]} : (index, index) -> tensor<1x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x68ff140) {
  %6 = "tensor.cast"(%2) : (tensor<?x?x?xf32>) -> tensor<1x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.insert_slice'(0x6873070) {
  %7 = "tensor.insert_slice"(%6, %5, %3, %4) {operand_segment_sizes = dense<[1, 1, 0, 2, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, -1, -1], static_strides = [1, 1, 1]} : (tensor<1x?x?xf32>, tensor<1x?x?xf32>, index, index) -> tensor<1x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.init_tensor'(0x688a8e0) {
  %8 = "linalg.init_tensor"(%4) {static_sizes = [1, 1, -1]} : (index) -> tensor<1x1x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x68aa720) {
  %9 = "tensor.cast"(%7) : (tensor<1x?x?xf32>) -> tensor<1x1x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.insert_slice'(0x68fc610) {
  %10 = "tensor.insert_slice"(%9, %8, %4) {operand_segment_sizes = dense<[1, 1, 0, 1, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, 1, -1], static_strides = [1, 1, 1]} : (tensor<1x1x?xf32>, tensor<1x1x?xf32>, index) -> tensor<1x1x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x68b3360) {
  %11 = "tensor.cast"(%10) : (tensor<1x1x?xf32>) -> tensor<1x1x2xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x69008b0) {
  %12 = "torch_c.from_builtin_tensor"(%11) : (tensor<1x1x2xf32>) -> !torch.vtensor<[1,1,2],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.return'(0x68a5110) {
  "std.return"(%12) : (!torch.vtensor<[1,1,2],f32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'std.return -> ()' {
Trying to match "(anonymous namespace)::ReturnOpTypeConversion"
"(anonymous namespace)::ReturnOpTypeConversion" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'std.return'(0x68a5110) {
      "std.return"(%13) : (tensor<1x1x2xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: tensor<?x?x?xf32>, %arg1: tensor<?x?xf32>) -> tensor<1x1x2xf32> {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %0 = torch_c.to_builtin_tensor <<UNKNOWN SSA VALUE>> : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %1 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
  %2 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
  %3 = linalg.init_tensor [1, %1, %2] : tensor<1x?x?xf32>
  %4 = tensor.cast %0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %5 = tensor.insert_slice %4 into %3[0, 0, 0] [1, %1, %2] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %6 = linalg.init_tensor [1, 1, %2] : tensor<1x1x?xf32>
  %7 = tensor.cast %5 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %8 = tensor.insert_slice %7 into %6[0, 0, 0] [1, 1, %2] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %9 = tensor.cast %8 : tensor<1x1x?xf32> to tensor<1x1x2xf32>
  %10 = torch_c.from_builtin_tensor %9 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  %11 = builtin.unrealized_conversion_cast %10 : !torch.vtensor<[1,1,2],f32> to tensor<1x1x2xf32>
  return %11 : tensor<1x1x2xf32>
}


} -> SUCCESS
//===-------------------------------------------===//
** Insert  : 'torch_c.to_builtin_tensor'(0x68fe860)
** Insert  : 'torch_c.from_builtin_tensor'(0x68af140)
// -----// IR Dump After FuncBackendTypeConversion //----- //
module attributes {torch.debug_module_name = "MyModule"}  {
  func @forward(%arg0: tensor<?x?x?xf32>, %arg1: tensor<?x?xf32>) -> tensor<1x1x2xf32> {
    %0 = torch_c.from_builtin_tensor %arg0 : tensor<?x?x?xf32> -> !torch.vtensor<[?,?,?],f32>
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
    %2 = tensor.dim %1, %c1 : tensor<?x?x?xf32>
    %3 = tensor.dim %1, %c2 : tensor<?x?x?xf32>
    %4 = linalg.init_tensor [1, %2, %3] : tensor<1x?x?xf32>
    %5 = tensor.cast %1 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
    %6 = tensor.insert_slice %5 into %4[0, 0, 0] [1, %2, %3] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
    %7 = linalg.init_tensor [1, 1, %3] : tensor<1x1x?xf32>
    %8 = tensor.cast %6 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
    %9 = tensor.insert_slice %8 into %7[0, 0, 0] [1, 1, %3] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
    %10 = tensor.cast %9 : tensor<1x1x?xf32> to tensor<1x1x2xf32>
    %11 = torch_c.from_builtin_tensor %10 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
    %12 = torch_c.to_builtin_tensor %11 : !torch.vtensor<[1,1,2],f32> -> tensor<1x1x2xf32>
    return %12 : tensor<1x1x2xf32>
  }
}



//===-------------------------------------------===//
Legalizing operation : 'builtin.func'(0x686d8b0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x68af140) {
  %0 = "torch_c.from_builtin_tensor"(%arg0) : (tensor<?x?x?xf32>) -> !torch.vtensor<[?,?,?],f32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch_c.from_builtin_tensor -> ()' {
Trying to match "(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::FromBuiltinTensorOp>"
    ** Replace : 'torch_c.from_builtin_tensor'(0x68af140)
"(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::FromBuiltinTensorOp>" result 1
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: tensor<?x?x?xf32>, %arg1: tensor<?x?xf32>) -> tensor<1x1x2xf32> {
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<?x?x?xf32> -> !torch.vtensor<[?,?,?],f32>
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %2 = tensor.dim %1, %c1 : tensor<?x?x?xf32>
  %3 = tensor.dim %1, %c2 : tensor<?x?x?xf32>
  %4 = linalg.init_tensor [1, %2, %3] : tensor<1x?x?xf32>
  %5 = tensor.cast %1 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %6 = tensor.insert_slice %5 into %4[0, 0, 0] [1, %2, %3] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %7 = linalg.init_tensor [1, 1, %3] : tensor<1x1x?xf32>
  %8 = tensor.cast %6 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %9 = tensor.insert_slice %8 into %7[0, 0, 0] [1, 1, %3] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %10 = tensor.cast %9 : tensor<1x1x?xf32> to tensor<1x1x2xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  %12 = torch_c.to_builtin_tensor %11 : !torch.vtensor<[1,1,2],f32> -> tensor<1x1x2xf32>
  return %12 : tensor<1x1x2xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68ebc70) {
  %1 = "arith.constant"() {value = 2 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x6901a70) {
  %2 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x6900820) {
  %3 = "torch_c.to_builtin_tensor"(%0) : (!torch.vtensor<[?,?,?],f32>) -> tensor<?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch_c.to_builtin_tensor -> ()' {
Trying to match "(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::ToBuiltinTensorOp>"
    ** Replace : 'torch_c.to_builtin_tensor'(0x6900820)
"(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::ToBuiltinTensorOp>" result 1
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: tensor<?x?x?xf32>, %arg1: tensor<?x?xf32>) -> tensor<1x1x2xf32> {
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<?x?x?xf32> -> !torch.vtensor<[?,?,?],f32>
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %2 = tensor.dim %1, %c1 : tensor<?x?x?xf32>
  %3 = tensor.dim %1, %c2 : tensor<?x?x?xf32>
  %4 = linalg.init_tensor [1, %2, %3] : tensor<1x?x?xf32>
  %5 = tensor.cast %1 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %6 = tensor.insert_slice %5 into %4[0, 0, 0] [1, %2, %3] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %7 = linalg.init_tensor [1, 1, %3] : tensor<1x1x?xf32>
  %8 = tensor.cast %6 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %9 = tensor.insert_slice %8 into %7[0, 0, 0] [1, 1, %3] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %10 = tensor.cast %9 : tensor<1x1x?xf32> to tensor<1x1x2xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  %12 = torch_c.to_builtin_tensor %11 : !torch.vtensor<[1,1,2],f32> -> tensor<1x1x2xf32>
  return %12 : tensor<1x1x2xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f4a00) {
  %4 = "tensor.dim"(%3, %2) : (tensor<?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f6000) {
  %5 = "tensor.dim"(%3, %1) : (tensor<?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.init_tensor'(0x68fe3c0) {
  %6 = "linalg.init_tensor"(%4, %5) {static_sizes = [1, -1, -1]} : (index, index) -> tensor<1x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x68ff140) {
  %7 = "tensor.cast"(%3) : (tensor<?x?x?xf32>) -> tensor<1x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.insert_slice'(0x6873070) {
  %8 = "tensor.insert_slice"(%7, %6, %4, %5) {operand_segment_sizes = dense<[1, 1, 0, 2, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, -1, -1], static_strides = [1, 1, 1]} : (tensor<1x?x?xf32>, tensor<1x?x?xf32>, index, index) -> tensor<1x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.init_tensor'(0x688a8e0) {
  %9 = "linalg.init_tensor"(%5) {static_sizes = [1, 1, -1]} : (index) -> tensor<1x1x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x68aa720) {
  %10 = "tensor.cast"(%8) : (tensor<1x?x?xf32>) -> tensor<1x1x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.insert_slice'(0x68fc610) {
  %11 = "tensor.insert_slice"(%10, %9, %5) {operand_segment_sizes = dense<[1, 1, 0, 1, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, 1, -1], static_strides = [1, 1, 1]} : (tensor<1x1x?xf32>, tensor<1x1x?xf32>, index) -> tensor<1x1x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x68b3360) {
  %12 = "tensor.cast"(%11) : (tensor<1x1x?xf32>) -> tensor<1x1x2xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x69008b0) {
  %13 = "torch_c.from_builtin_tensor"(%12) : (tensor<1x1x2xf32>) -> !torch.vtensor<[1,1,2],f32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch_c.from_builtin_tensor -> ()' {
Trying to match "(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::FromBuiltinTensorOp>"
    ** Replace : 'torch_c.from_builtin_tensor'(0x69008b0)
"(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::FromBuiltinTensorOp>" result 1
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: tensor<?x?x?xf32>, %arg1: tensor<?x?xf32>) -> tensor<1x1x2xf32> {
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<?x?x?xf32> -> !torch.vtensor<[?,?,?],f32>
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %2 = tensor.dim %1, %c1 : tensor<?x?x?xf32>
  %3 = tensor.dim %1, %c2 : tensor<?x?x?xf32>
  %4 = linalg.init_tensor [1, %2, %3] : tensor<1x?x?xf32>
  %5 = tensor.cast %1 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %6 = tensor.insert_slice %5 into %4[0, 0, 0] [1, %2, %3] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %7 = linalg.init_tensor [1, 1, %3] : tensor<1x1x?xf32>
  %8 = tensor.cast %6 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %9 = tensor.insert_slice %8 into %7[0, 0, 0] [1, 1, %3] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %10 = tensor.cast %9 : tensor<1x1x?xf32> to tensor<1x1x2xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  %12 = torch_c.to_builtin_tensor %11 : !torch.vtensor<[1,1,2],f32> -> tensor<1x1x2xf32>
  return %12 : tensor<1x1x2xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x68fe860) {
  %14 = "torch_c.to_builtin_tensor"(%13) : (!torch.vtensor<[1,1,2],f32>) -> tensor<1x1x2xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch_c.to_builtin_tensor -> ()' {
Trying to match "(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::ToBuiltinTensorOp>"
    ** Replace : 'torch_c.to_builtin_tensor'(0x68fe860)
"(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::ToBuiltinTensorOp>" result 1
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func @forward(%arg0: tensor<?x?x?xf32>, %arg1: tensor<?x?xf32>) -> tensor<1x1x2xf32> {
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<?x?x?xf32> -> !torch.vtensor<[?,?,?],f32>
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[?,?,?],f32> -> tensor<?x?x?xf32>
  %2 = tensor.dim %1, %c1 : tensor<?x?x?xf32>
  %3 = tensor.dim %1, %c2 : tensor<?x?x?xf32>
  %4 = linalg.init_tensor [1, %2, %3] : tensor<1x?x?xf32>
  %5 = tensor.cast %1 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %6 = tensor.insert_slice %5 into %4[0, 0, 0] [1, %2, %3] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %7 = linalg.init_tensor [1, 1, %3] : tensor<1x1x?xf32>
  %8 = tensor.cast %6 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %9 = tensor.insert_slice %8 into %7[0, 0, 0] [1, 1, %3] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %10 = tensor.cast %9 : tensor<1x1x?xf32> to tensor<1x1x2xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<1x1x2xf32> -> !torch.vtensor<[1,1,2],f32>
  %12 = torch_c.to_builtin_tensor %11 : !torch.vtensor<[1,1,2],f32> -> tensor<1x1x2xf32>
  return %12 : tensor<1x1x2xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.return'(0x68a5110) {
  "std.return"(%14) : (tensor<1x1x2xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After FinalizingBackendTypeConversion //----- //
func @forward(%arg0: tensor<?x?x?xf32>, %arg1: tensor<?x?xf32>) -> tensor<1x1x2xf32> {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32>
  %1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32>
  %2 = linalg.init_tensor [1, %0, %1] : tensor<1x?x?xf32>
  %3 = tensor.cast %arg0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
  %4 = tensor.insert_slice %3 into %2[0, 0, 0] [1, %0, %1] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
  %5 = linalg.init_tensor [1, 1, %1] : tensor<1x1x?xf32>
  %6 = tensor.cast %4 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
  %7 = tensor.insert_slice %6 into %5[0, 0, 0] [1, 1, %1] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
  %8 = tensor.cast %7 : tensor<1x1x?xf32> to tensor<1x1x2xf32>
  return %8 : tensor<1x1x2xf32>
}


//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x6867970) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'builtin.func'(0x686d8b0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x68ebc70) {
  %0 = "arith.constant"() {value = 2 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x6901a70) {
  %1 = "arith.constant"() {value = 1 : index} : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f4a00) {
  %2 = "tensor.dim"(%arg0, %1) : (tensor<?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x68f6000) {
  %3 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.init_tensor'(0x68fe3c0) {
  %4 = "linalg.init_tensor"(%2, %3) {static_sizes = [1, -1, -1]} : (index, index) -> tensor<1x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x68ff140) {
  %5 = "tensor.cast"(%arg0) : (tensor<?x?x?xf32>) -> tensor<1x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.insert_slice'(0x6873070) {
  %6 = "tensor.insert_slice"(%5, %4, %2, %3) {operand_segment_sizes = dense<[1, 1, 0, 2, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, -1, -1], static_strides = [1, 1, 1]} : (tensor<1x?x?xf32>, tensor<1x?x?xf32>, index, index) -> tensor<1x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.init_tensor'(0x688a8e0) {
  %7 = "linalg.init_tensor"(%3) {static_sizes = [1, 1, -1]} : (index) -> tensor<1x1x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x68aa720) {
  %8 = "tensor.cast"(%6) : (tensor<1x?x?xf32>) -> tensor<1x1x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.insert_slice'(0x68fc610) {
  %9 = "tensor.insert_slice"(%8, %7, %3) {operand_segment_sizes = dense<[1, 1, 0, 1, 0]> : vector<5xi32>, static_offsets = [0, 0, 0], static_sizes = [1, 1, -1], static_strides = [1, 1, 1]} : (tensor<1x1x?xf32>, tensor<1x1x?xf32>, index) -> tensor<1x1x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x68b3360) {
  %10 = "tensor.cast"(%9) : (tensor<1x1x?xf32>) -> tensor<1x1x2xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'std.return'(0x68a5110) {
  "std.return"(%10) : (tensor<1x1x2xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After VerifyLinalgOnTensorsBackendContract //----- //
module attributes {torch.debug_module_name = "MyModule"}  {
  func @forward(%arg0: tensor<?x?x?xf32>, %arg1: tensor<?x?xf32>) -> tensor<1x1x2xf32> {
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32>
    %1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32>
    %2 = linalg.init_tensor [1, %0, %1] : tensor<1x?x?xf32>
    %3 = tensor.cast %arg0 : tensor<?x?x?xf32> to tensor<1x?x?xf32>
    %4 = tensor.insert_slice %3 into %2[0, 0, 0] [1, %0, %1] [1, 1, 1] : tensor<1x?x?xf32> into tensor<1x?x?xf32>
    %5 = linalg.init_tensor [1, 1, %1] : tensor<1x1x?xf32>
    %6 = tensor.cast %4 : tensor<1x?x?xf32> to tensor<1x1x?xf32>
    %7 = tensor.insert_slice %6 into %5[0, 0, 0] [1, 1, %1] [1, 1, 1] : tensor<1x1x?xf32> into tensor<1x1x?xf32>
    %8 = tensor.cast %7 : tensor<1x1x?xf32> to tensor<1x1x2xf32>
    return %8 : tensor<1x1x2xf32>
  }
}


