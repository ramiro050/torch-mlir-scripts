graph(%self : __torch__.MyModule,
      %x.1 : Tensor):
  %6 : int = prim::Constant[value=6]() # torch-mlir-scripts/torch_linalg_mix.py:37:31
  %4 : int = prim::Constant[value=3]() # torch-mlir-scripts/torch_linalg_mix.py:37:25
  %z.1 : Tensor = aten::tanh(%x.1) # torch-mlir-scripts/torch_linalg_mix.py:36:12
  %5 : int[] = prim::ListConstruct(%4, %4)
  %t.1 : Tensor = ^init_tensor()(%5, %6) # torch-mlir-scripts/torch_linalg_mix.py:37:12
  %11 : Tensor = ^matmul()(%z.1, %z.1, %t.1) # torch-mlir-scripts/torch_linalg_mix.py:38:15
  return (%11)


Torch-MLIR
----------
module attributes {torch.debug_module_name = "MyModule"}  {
  func private @__torch__.MyModule.forward(%arg0: !torch.nn.Module<"__torch__.MyModule">, %arg1: !torch.tensor {torch.type_bound = !torch.vtensor<[3,3],f32>}) -> !torch.tensor {
    %int6 = torch.constant.int 6
    %int3 = torch.constant.int 3
    %1 = torch.aten.tanh %arg1 : !torch.tensor -> !torch.tensor
    %2 = torch.prim.ListConstruct %int3, %int3 : (!torch.int, !torch.int) -> !torch.list<!torch.int>
    %3 = torch.linalg.init_tensor %2, %int6 : !torch.list<!torch.int>, !torch.int -> !torch.tensor
    %4 = torch.linalg.matmul %1, %1, %3 : !torch.tensor, !torch.tensor, !torch.tensor -> !torch.tensor
    return %4 : !torch.tensor
  }
  torch.class_type @__torch__.MyModule  {
    torch.attr private "training" : !torch.bool
    torch.attr private "_is_full_backward_hook" : !torch.optional<!torch.bool>
    torch.method "forward", @__torch__.MyModule.forward
  }
  %true = torch.constant.bool true
  %none = torch.constant.none
  %0 = torch.nn_module  {
    torch.slot "training", %true : !torch.bool
    torch.slot "_is_full_backward_hook", %none : !torch.none
  } : !torch.nn.Module<"__torch__.MyModule">
}

Linalg-MLIR
-----------
#map = affine_map<(d0, d1) -> (d0, d1)>
module attributes {torch.debug_module_name = "MyModule"}  {
  func @forward(%arg0: tensor<3x3xf32>) -> tensor<3x3xf32> {
    %c3_i64 = arith.constant 3 : i64
    %0 = linalg.init_tensor [3, 3] : tensor<3x3xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<3x3xf32>) outs(%0 : tensor<3x3xf32>) {
    ^bb0(%arg1: f32, %arg2: f32):  // no predecessors
      %7 = math.tanh %arg1 : f32
      linalg.yield %7 : f32
    } -> tensor<3x3xf32>
    %2 = arith.index_cast %c3_i64 : i64 to index
    %3 = arith.index_cast %c3_i64 : i64 to index
    %4 = linalg.init_tensor [%2, %3] : tensor<?x?xf32>
    %5 = tensor.cast %4 : tensor<?x?xf32> to tensor<3x3xf32>
    %6 = linalg.matmul ins(%1, %1 : tensor<3x3xf32>, tensor<3x3xf32>) outs(%5 : tensor<3x3xf32>) -> tensor<3x3xf32>
    return %6 : tensor<3x3xf32>
  }
}


Running Compiled Graph
----------------------
Output from compiled MLIR:
[[ 0.8585298   0.8952646   0.76727146]
 [ 0.9589676   1.0424526   0.        ]
 [-0.9131405   0.          0.18392277]]
